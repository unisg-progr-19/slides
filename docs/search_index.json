[
["index.html", "7784 Skills: Programming in R 1 Introduction 1.1 Using left_join to join data sets", " 7784 Skills: Programming in R Kirill Müller, Christoph Sax University of St. Gallen, HS 2019 1 Introduction Intro (Sept. 19): Slides Canvas Page GitHub Organization 1.1 Using left_join to join data sets For those who are interested in using left_join() for combining data sets, see the corresponding Section in R for Data Science. "],
["visualization-and-reporting.html", "2 Visualization and Reporting", " 2 Visualization and Reporting Visualization and Reporting (Sept. 26): Slides "],
["data-tranformation-i.html", "3 Data Tranformation I", " 3 Data Tranformation I Data Tranformation I (Oct. 3): Slides "],
["data-tranformation-ii.html", "4 Data Tranformation II", " 4 Data Tranformation II Data Tranformation II (Oct. 10): Slides "],
["basics-of-r-base.html", "5 Basics of R base 5.1 The Main Data Structures 5.2 Introduction to Functions 5.3 Higher-Order Functions", " 5 Basics of R base Download as R script Intro Slides We have spent the first lessons of this course with dplyr and ggplot2, two of the great innovations in R universe of the last few years. If you were learning R 10 years ago, R would have looked quite differently. In this lesson, we cover the basics of R base - R (mostly) as it is before you load any packages. 5.1 The Main Data Structures R has four main data structures. In dplyr, we always work on data frames, or tibbles, the equivalent to an Excel sheet - a tabular collection of data. When performing operations, e.g., by using filter(), or mutate() we are performing operations on the columns of a data frame. These columns are vectors, and they are the heart of R. Related to them, matrices are basically two dimensional vectors that are useful in some mathematical or statistical applications. Finally, lists are the most versatile data type, and they are used in many circumstances. We will cover these data structures in the following order: vectors, matrices, lists and data frames, or tibbles (which we already know well). 5.1.1 Vectors, the R Workhorse The vector type is the most basic data structure in R. It’s hard to imagine R code that doesn’t involve vectors. If you use mutate to change columns of your data frame, you are effectively operating on vectors. The elements of a vector must all have the same class, or data type. You can have a vector consisting of three character strings (of class character) or three integer elements (of class integer), but not a vector with one integer element and two character string elements. 5.1.1.1 Vector Classes In many programming languages, vector variables are considered different from scalars, which are single-number variables. However, in R, numbers are actually considered one-element vectors, and there is really no such thing as a scalar. As we have seen previously, all elements of an R vector (or data frame column) must have the same class, which can be integer, numeric (also called double), character (string), logical (boolean). Here are some examples: x_log &lt;- c(TRUE, FALSE) # same as c(T, F) x_int &lt;- c(1L, 2L, 3L) # use 1L to enforce integer, rather than numeric x_num &lt;- c(1, 2, 6.3) # also called &#39;double&#39; x_chr &lt;- c(&quot;Hello World&quot;) If you need to check the class of a variable x, you can use, e.g.: class(x_log) ## [1] &quot;logical&quot; There is a certain order in the list above: logical is the least flexible class, while character is the most flexible. If you combine vectors of different classes, the more flexible class will win: class(c(x_log, x_num)) ## [1] &quot;numeric&quot; class(c(x_int, x_chr)) ## [1] &quot;character&quot; You can change the class of a vector with the following coercion functions. as.logical(c(1, 0)) ## [1] TRUE FALSE as.integer(c(1, 0)) ## [1] 1 0 as.numeric(c(&quot;1&quot;, &quot;2&quot;)) ## [1] 1 2 as.character(c(TRUE, FALSE)) ## [1] &quot;TRUE&quot; &quot;FALSE&quot; These functions will always work if you coerce towards greater flexibility. If you want to go the other way, it may give you NAs and some warnings: as.numeric(c(&quot;hi&quot;, &quot;number&quot;, &quot;1&quot;)) ## Warning: NAs introduced by coercion ## [1] NA NA 1 5.1.1.2 Recycling When applying an operation to two vectors that requires them to be the same length, R automatically recycles, or repeats, the shorter one, until it is long enough to match the longer one. Here is an example: c(1, 2) + c(6, 0, 9, 20, 22, 11) ## [1] 7 2 10 22 23 13 The shorter vector was recycled, so the operation was taken to be as follows: c(1, 2, 1, 2, 1, 2) + c(6, 0, 9, 20, 22, 11) ## [1] 7 2 10 22 23 13 The most common recycling operation involves a vector of length 1: c(6, 0, 9, 20, 22, 11) + 3 ## [1] 9 3 12 23 25 14 In dplyr, only a vector of length 1 is allowed to recycle, the other cases will result in an error. # tibble(a = c(1, 2), b = c(6, 0, 9, 20, 22, 11)) 5.1.1.3 Arithmetic Operators In R, every operator, including + in the following example, is actually a function. 2 + 3 ## [1] 5 The + here is a function with two arguments. A more functional way of writing it is the following: &quot;+&quot;(2, 3) ## [1] 5 Remember that scalars are actually one-element vectors. So, we can add vectors, and the + operation will be applied element-wise. x &lt;- c(1, 2, 4) x + c(5, 0, -1) ## [1] 6 2 3 The same is true, e.g., for multiplication, which is done element by element as well. (We will have a look at matrix multiplication in the next section.) x * c(5, 0, -1) ## [1] 5 0 -4 5.1.1.4 Comparison Operators Similar to arithmetic operators, comparison operators are applied element wise. The following expression will return a single TRUE, as we are comparing two vectors of length 1: 2 &gt; 1 ## [1] TRUE The comparison operator for ‘is equal to’ is ==, not =: 1 + 1 == 2 ## [1] TRUE Here is how they work on longer vectors: x &lt;- c(1, 2, 4, 2) y &lt;- c(2, 2, 4, 5) x == y ## [1] FALSE TRUE TRUE FALSE The usual recycling rules apply as well: x == 2 ## [1] FALSE TRUE FALSE TRUE Here are the other comparison operators: x &lt; y: less than x &lt;= y: less or equal than x &gt;= y: greater or equal than x != y: not equal Logical vectors can be combined by &amp; (AND) or | (OR): a &lt;- x &gt;= 2 b &lt;- x &lt; 4 a &amp; b ## [1] FALSE TRUE FALSE TRUE x &gt;= 4 | x &lt; 2 ## [1] TRUE FALSE TRUE FALSE 5.1.1.5 Indexing One of the most frequently used operations in R base is that of indexing vectors, in which we form a subvector by picking elements. You can use both integer values or logical vectors for indexing. 5.1.1.5.1 Indexing Using Integers We can extract values from a vector, using an integer index: y &lt;- c(1.2, 3.9, 0.4, 0.12) y[c(1, 3)] ## [1] 1.2 0.4 y[2:3] ## [1] 3.9 0.4 v &lt;- 3:4 y[v] ## [1] 0.40 0.12 Note that duplicates are allowed: y[c(1, 1, 3)] ## [1] 1.2 1.2 0.4 Negative subscripts mean that we want to exclude the elements: z &lt;- c(5, 12, 13) z[-1] ## [1] 12 13 z[-1:-2] ## [1] 13 5.1.1.5.2 Logical Indexing Logical indexing is perhaps even more important. Building on the example from above, we could also select element 1 and 3 in the following way: y &lt;- c(1.2, 3.9, 0.4, 0.12) y[c(FALSE, TRUE, FALSE, TRUE)] ## [1] 3.90 0.12 Logical indexing picks the TRUEs but not the FALSEs. This is the main building block for filtering. Suppose you have: y[y &gt; 1] ## [1] 1.2 3.9 This will return all elements that are bigger than 1. How is this done? First, R had evaluated the comparison, y &gt; 1, which led to logical vector: y &gt; 1 ## [1] TRUE TRUE FALSE FALSE Second, using logical indexing, this vector was then used to pick those elements that evaluated to TRUE. So y[y &gt; 1] is actually the same as: y[c(TRUE, TRUE, FALSE, TRUE)] ## [1] 1.20 3.90 0.12 We have already seen that you can assign to individual elements of a vector, using integer indices: y[c(2, 4)] &lt;- 5 Of course, you can do the same with logical indices: y[c(FALSE, TRUE, FALSE, TRUE)] &lt;- 5 This is a very powerful tool. For example, if you want to truncate all negative numbers in a vector to 0, you can use: z &lt;- c(-3, 1.2, 2, -22) z[z &lt; 0] &lt;- 0 Or use it with the %in% operator you encountered before: z[z %in% y] ## [1] 1.2 5.1.1.6 Exercises Create a vector called v1 containing the numbers 2, 5, 8, 12 and 16. Extract the values at positions 2 and 5 from v1. Use x:y notation to make a second vector called v2 containing the numbers 5 to 9. Subtract v2 from v1 and look at the result. Generate a vector with 1000 standard-normally distributed random numbers (use rnorm()). Store the result as v3. Extract the numbers that are bigger than 2. 5.1.2 Matrices An R matrix corresponds to the mathematical concept of the same name: a rectangular array of numbers (most of the time), or some other type. Here is some sample matrix code: m &lt;- matrix(c(1, 4, 2, 2), nrow = 2, ncol = 2) m ## [,1] [,2] ## [1,] 1 2 ## [2,] 4 2 The main use of matrices is for matrix algebra. You can do all kind of matrix algebra operations, right out of R base: m %*% m # matrix multiplication ## [,1] [,2] ## [1,] 9 6 ## [2,] 12 12 m * m # elementwise multiplication ## [,1] [,2] ## [1,] 1 4 ## [2,] 16 4 m %*% solve(m) # inverse of a matrix ## [,1] [,2] ## [1,] 1 0 ## [2,] 0 1 m / m # elementwise division ## [,1] [,2] ## [1,] 1 1 ## [2,] 1 1 m + m ## [,1] [,2] ## [1,] 2 4 ## [2,] 8 4 m - m ## [,1] [,2] ## [1,] 0 0 ## [2,] 0 0 t(m) # matrix transpose ## [,1] [,2] ## [1,] 1 4 ## [2,] 2 2 qr(m) # QR decomposition ## $qr ## [,1] [,2] ## [1,] -4.1231056 -2.425356 ## [2,] 0.9701425 -1.455214 ## ## $rank ## [1] 2 ## ## $qraux ## [1] 1.242536 1.455214 ## ## $pivot ## [1] 1 2 ## ## attr(,&quot;class&quot;) ## [1] &quot;qr&quot; det(m) # determinant ## [1] -6 eigen(m) # eigenvalues/eigenvectors ## eigen() decomposition ## $values ## [1] 4.372281 -1.372281 ## ## $vectors ## [,1] [,2] ## [1,] -0.5101065 -0.6445673 ## [2,] -0.8601113 0.7645475 diag(m) # diagonal ## [1] 1 2 Matrices are indexed by double subscripting: m[1, 2] ## [1] 2 m[2, 2] ## [1] 2 You can extract submatrices from a matrix, much as you extract subvectors from vectors: m[1, ] # row 1 ## [1] 1 2 m[1, , drop = FALSE] # keeps being a matrix ## [,1] [,2] ## [1,] 1 2 m[, 2] # column 2 ## [1] 2 2 5.1.2.1 Exercises Create a 10 x 10 matrix that contains a sequence of numbers (use the : notation). Use the transpose function on the matrix Extract the 2. column of the matrix Extract the 5. row of the matrix Extract the 5. and the 6. row of the matrix Compare the classes of the results in 3. and 4. to each other Modify 3., so that it returns the same class as 4. 5.1.3 Lists Like an R vector, an R list is a container for values, but its contents can be items of different data types, or different length. Here’s an example: x &lt;- list(u = c(2, 3, 4), v = &quot;abc&quot;) x ## $u ## [1] 2 3 4 ## ## $v ## [1] &quot;abc&quot; x$u ## [1] 2 3 4 The expression x$u refers to the u component in the list x. x[[&#39;u&#39;]] ## [1] 2 3 4 x[[1]] ## [1] 2 3 4 We can also refer to list components by their numerical indices. However, note that in this case, we use double brackets instead of single ones. We can also use single brackets rather than double brackets to get a subset of the list. x[&#39;u&#39;] ## $u ## [1] 2 3 4 x[1] ## $u ## [1] 2 3 4 x[1:2] ## $u ## [1] 2 3 4 ## ## $v ## [1] &quot;abc&quot; Note that x[[1]] returns the component (a numeric vector), while x[1] returns a subset of the list (a list of length 1): class(x[1]) ## [1] &quot;list&quot; class(x[[1]]) ## [1] &quot;numeric&quot; Hadley Wickham’s visualization helps a lot here: https://twitter.com/hadleywickham/status/643381054758363136 Lists are not restricted to containing vectors. In fact, they can contain anything, for example, a data frames: ll &lt;- list(mtcars = mtcars, u = c(2, 3, 4)) 5.1.3.1 Exercises Generate two random vectors of length 10, a, and b. Combine them in a list, call it l1. Compare the classes of l1[2] and l1[[2]]. Can you explain the difference? 5.1.4 Data Frames As we saw in many places, a typical data set contains data of different classes. Instead of a matrix, we use a data frame, or tibble. Technically, a data frame in R is a list, with each component of the list being a vector corresponding to a column in our data. You can create a data frame using tibble from tidyverse: library(tidyverse) d &lt;- tibble(kids = c(&quot;Jack&quot;, &quot;Jill&quot;), ages = c(12, 10)) d ## # A tibble: 2 x 2 ## kids ages ## &lt;chr&gt; &lt;dbl&gt; ## 1 Jack 12 ## 2 Jill 10 Here, I am using the tibble() function from tidyverse, rather than the R base equivalent, data.frame(). The reason for this is that data.frame() has some undesirable features, such as converting character vectors into factors. Because data frames are technically a list, we can access its vectors the same way as we access components of a list. d$ages ## [1] 12 10 d[[&#39;ages&#39;]] ## [1] 12 10 d[[2]] # usually not recommended ## [1] 12 10 Typically, though, data frames are created by reading in a data set from a file or a database, as we saw in the previous days of the workshop. Contrary to lists, data frames require their columns to be of the same length. If they are not, values will be recycled. That is why the following works: tibble(kids = c(&quot;Jack&quot;, &quot;Jill&quot;), ages = c(12, 10), type = &quot;kid&quot;) ## # A tibble: 2 x 3 ## kids ages type ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Jack 12 kid ## 2 Jill 10 kid 5.2 Introduction to Functions As in most programming languages, the heart of R programming consists of writing functions. A function is a group of instructions that takes inputs, uses them to compute other values, and returns a result. Let’s write a function that divides all elements of a vector by 2: half &lt;- function(x) { x / 2 } This is a function named half, whose purpose is to divides every element of a vector by 2. It’s a pretty pointless function, as the operation itself is so simple. Arguments to a function are enclosed by parentheses (()); the body of the function is enclosed by braces ({}). Let’s see how it works: half(c(3, 2, 1)) ## [1] 1.5 1.0 0.5 half(AirPassengers) ## Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov ## 1949 56.0 59.0 66.0 64.5 60.5 67.5 74.0 74.0 68.0 59.5 52.0 ## 1950 57.5 63.0 70.5 67.5 62.5 74.5 85.0 85.0 79.0 66.5 57.0 ## 1951 72.5 75.0 89.0 81.5 86.0 89.0 99.5 99.5 92.0 81.0 73.0 ## 1952 85.5 90.0 96.5 90.5 91.5 109.0 115.0 121.0 104.5 95.5 86.0 ## 1953 98.0 98.0 118.0 117.5 114.5 121.5 132.0 136.0 118.5 105.5 90.0 ## 1954 102.0 94.0 117.5 113.5 117.0 132.0 151.0 146.5 129.5 114.5 101.5 ## 1955 121.0 116.5 133.5 134.5 135.0 157.5 182.0 173.5 156.0 137.0 118.5 ## 1956 142.0 138.5 158.5 156.5 159.0 187.0 206.5 202.5 177.5 153.0 135.5 ## 1957 157.5 150.5 178.0 174.0 177.5 211.0 232.5 233.5 202.0 173.5 152.5 ## 1958 170.0 159.0 181.0 174.0 181.5 217.5 245.5 252.5 202.0 179.5 155.0 ## 1959 180.0 171.0 203.0 198.0 210.0 236.0 274.0 279.5 231.5 203.5 181.0 ## 1960 208.5 195.5 209.5 230.5 236.0 267.5 311.0 303.0 254.0 230.5 195.0 ## Dec ## 1949 59.0 ## 1950 70.0 ## 1951 83.0 ## 1952 97.0 ## 1953 100.5 ## 1954 114.5 ## 1955 139.0 ## 1956 153.0 ## 1957 168.0 ## 1958 168.5 ## 1959 202.5 ## 1960 216.0 here we are saving the output in a variable inside the function: half &lt;- function(x) { z &lt;- x / 2 z } An R functions will return the last value computed if there is no explicit return() call. We could have been more explicit, but it is usually not necessary: half &lt;- function(x) { z &lt;- x / 2 return(z) } Let’s make it a bit more complex, by adding an additional argument: fraction &lt;- function(x, denominator){ x / denominator } fraction(c(2, 3, 4), 4) ## [1] 0.50 0.75 1.00 if you have more than one argument, it is a good practice to name the arguments fraction(x = c(2, 3, 4), denominator = 4) ## [1] 0.50 0.75 1.00 that way, they become independent of the order, which is a very useful if the number of argument becomes large. fraction(denominator = 4, x = c(2, 3, 4)) ## [1] 0.50 0.75 1.00 5.2.1 Variable Scope A variable that is visible only within a function body is said to be local to that function. In square(), x is a local variable. In fraction(), x and p are local variables. They disappear after the function returns: fraction(x = c(2, 3, 4), denominator = 4) ## [1] 0.50 0.75 1.00 # denominator # Error: object &#39;denominator&#39; not found x ## $u ## [1] 2 3 4 ## ## $v ## [1] &quot;abc&quot; Note that the x is not the x we used in the fraction function, but rather the x defined earlier. x here is called a global variable, while the x in the function is a local variable. Global variables are shown in RStudio in the environment pane. A global variable can be written to from within a function by using R’s superassignment operator, &lt;&lt;-, but this is rarely recommended. 5.2.2 Default Arguments A nice feature of R functions is that you can set defaults to arguments. Let’s modify the fraction function from above: fraction &lt;- function(x, denominator = 2) { x / denominator } Here denominator will be initialized to 2 if the user does not specify p in the call. So we can use fraction() the same way as half(): fraction(c(2, 2)) ## [1] 1 1 half(c(2, 2)) ## [1] 1 1 or use its extended capabilities: fraction(c(2, 2), 3) ## [1] 0.6666667 0.6666667 5.2.3 Exercises Write a function add_constant that adds a constant to a vector, and set the default value of the constant to 10. Apply it to the AirPassengers series. You can use your function within dplyr. Using the mpg dataset, use add_constant within mutate to add a constant value (100) to the number of cyl. (evil) We saw that \"+\" is actually a function. In R, it is easily possible to change the working of such a fundamental function (this is, in fact, what ggplot does). In order to do so, let’s write a new function with the same name, \"+\", and two arguments, a, and b. But instead of summing the values, let’s subtract them (or figure out something more evil). Verify the result of 1 + 1. Cool, isn’t it? (rm(\"+\") will restore sanity.) 5.3 Higher-Order Functions Because functions in R are objects like any other objects, it is easy to write functions that return functions or take functions as arguments. We saw some examples above. The most prominent of these functions are higher-order functions, which are very central to R. They are called apply or lapply, or similar, and there are many of them. To avoid too much confusion, we will restrict ourself to map, the tidyverse equivalent of lapply, 5.3.1 map The map function ‘maps’ a function to each component of a list. Because lists are such a useful container for objects in R, very often, you want to map a function to each component of a list. Here is an example: ll &lt;- list(a = c(2, 3, 4), b = c(1, 2, 3), c = c(5, 2, 1)) This is a list with three vectors, of which we want to calculate the means. Here’s an expression that does what we want (we will cover loops later on): z &lt;- NULL for (vi in ll){ z &lt;- c(z, mean(vi)) } We loop through each component of the list, calculate the mean and add it to an output vector, z. With the map function, this can be written much more concise: library(tidyverse) map(ll, mean) ## $a ## [1] 3 ## ## $b ## [1] 2 ## ## $c ## [1] 2.666667 So map returns a list of the same length as the first argument (a list), each element of which is the result of applying the second argument (a function) to the corresponding component of the list argument. If the input list is named (as in the example), so will be the output. Additional arguments to the mean() function can be included as well: map(ll, mean, na.rm = TRUE) ## $a ## [1] 3 ## ## $b ## [1] 2 ## ## $c ## [1] 2.666667 If you want to convert the list to a vector (as in the loop example), use unlist on the result: unlist(map(ll, mean)) ## a b c ## 3.000000 2.000000 2.666667 Of course, you can use map with your own functions. Here we want to pick the second element of each vector in the list: pick_second &lt;- function(x){ x[2] } map(ll, pick_second) ## $a ## [1] 3 ## ## $b ## [1] 2 ## ## $c ## [1] 2 In cases like this, we may want to use the possibility of having anonymous functions, i.e. functions without a name. So we just substitute pick_second by its definition: map(ll, function(x) x[2]) ## $a ## [1] 3 ## ## $b ## [1] 2 ## ## $c ## [1] 2 For simple one line functions like this, it may be justified to omit the curly braces. ### Exercises Use map to calculate the mean of each variable in the swiss dataset. Convert the resulting list to a vector. Use map to coerce the variables in the swiss dataset to character. Using map, generate a list containing 10 random vectors of random length between 1 and 10. Use the help to see what the colSums() function does. Using apply, try writing your own version, colSums2(). 5.3.2 Loops In many programming languages, one of first things you learn are loops. There is a reason that we didn’t cover them until now, and also only for completeness. Loops in R are slow, and they are – most of the time – unnecessary. This is because many operations in R are vectorized anyway, so there is no need to loop over each element. Also the group_by() operation in dplyr offers a much more elegant way of applying a function repeatedly to a group of data. Third, there are higher order functions like map (or the tidyverse equivalent: purrr::map) that save you from loops most of the time. Anyway, here is the loop: for (i in 1:10) { print(i) } ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 ## [1] 6 ## [1] 7 ## [1] 8 ## [1] 9 ## [1] 10 There will be one iteration of the loop for each component of the vector 1:10, with i taking on the values of those components – in the first iteration, i = (1:10)[1]; in the second iteration, i = (1:10)[2]; and so on. And we are not restricted to integer vectors, but can loop over any vector, even over lists: z &lt;- NULL for (i in list(swiss, mtcars)) { z &lt;- c(z, colnames(i)) } z ## [1] &quot;Fertility&quot; &quot;Agriculture&quot; &quot;Examination&quot; ## [4] &quot;Education&quot; &quot;Catholic&quot; &quot;Infant.Mortality&quot; ## [7] &quot;mpg&quot; &quot;cyl&quot; &quot;disp&quot; ## [10] &quot;hp&quot; &quot;drat&quot; &quot;wt&quot; ## [13] &quot;qsec&quot; &quot;vs&quot; &quot;am&quot; ## [16] &quot;gear&quot; &quot;carb&quot; But we saw a much clearer way of doing this above: unlist(map(list(swiss, mtcars), colnames)) ## [1] &quot;Fertility&quot; &quot;Agriculture&quot; &quot;Examination&quot; ## [4] &quot;Education&quot; &quot;Catholic&quot; &quot;Infant.Mortality&quot; ## [7] &quot;mpg&quot; &quot;cyl&quot; &quot;disp&quot; ## [10] &quot;hp&quot; &quot;drat&quot; &quot;wt&quot; ## [13] &quot;qsec&quot; &quot;vs&quot; &quot;am&quot; ## [16] &quot;gear&quot; &quot;carb&quot; "],
["math-and-statistics.html", "6 Math and Statistics 6.1 Math 6.2 Linear Regression 6.3 Numerical Optimization (optional) 6.4 Extended Exercises", " 6 Math and Statistics Download as R script Intro Slides R calls itself a software environment for statistical computing. Math and statistics are at the heart of R. This Lesson provides an overview of the many build-in functions for math and statistics. We will also discuss some basic statistical modeling techniques. 6.1 Math 6.1.1 Basic Math Functions R includes an extensive set of built-in math functions. Here is a partial list: log(): Natural logarithm exp(): Exponential function, base e sqrt(): Square root abs(): Absolute value sin(), cos(), and so on: Trigonometric functions min() and max(): Minimum value and maximum value within a vector which.min() and which.max(): Index of the minimal element and maximal element of a vector pmin() and pmax(): Element-wise minima and maxima of several vectors sum() and prod(): Sum and product of the elements of a vector cumsum() and cumprod(): Cumulative sum and product of the elements of a vector round(), floor(), and ceiling(): Round to the closest integer, to the closest integer below, and to the closest integer above Most of these functions are self explaining. Some remarks: The functions cumsum() and cumprod() return cumulative sums and products. x &lt;- c(12, 5, 13) cumsum(x) ## [1] 12 17 30 cumprod(x) ## [1] 12 60 780 In x, the sum of the first element is 12, the sum of the first two elements is 17, and the sum of the first three elements is 30. The function cumprod() works the same way as cumsum(), but with the product instead of the sum. We briefly mentioned the difference between min() and pmin() in the last chapter. The former simply combines all its arguments into one long vector and returns the minimum value in that vector. In contrast, if pmin() is applied to two or more vectors, it returns a vector of the pair-wise minima, hence the name pmin. Here’s an example: x &lt;- c(1, 5, 6, 2, 3, 1) y &lt;- c(3, 5, 2, 3, 2, 2) min(x) ## [1] 1 pmin(x, y) ## [1] 1 5 2 2 2 1 In the first case, min() computed the smallest value in c(1, 5, 6, 2, 3, 1). But the call to pmin() computed the smaller of 1 and 3, yielding 1; then the smaller of 5 and 5, which is 5; then the minimum of 6 and 2, giving 2, and so on. 6.1.2 Linear Algebra Operations on Vectors and Matrices (optional) Multiplying a vector by a scalar works directly, as you saw earlier. Here’s another example: y &lt;- c(1, 3, 4, 10) 2 * y ## [1] 2 6 8 20 For matrix multiplication in the mathematical sense, the operator to use is %*%, not * . For instance, here we compute the matrix product: Here’s the code: a &lt;- matrix(c(1, 3, 2, 4), nrow = 2, ncol = 2) x &lt;- matrix(c(1, 0, -1, 1), nrow = 2, ncol = 2) b &lt;- a %*% x; b ## [,1] [,2] ## [1,] 1 1 ## [2,] 3 1 The function solve() will solve systems of linear equations and find matrix inverses. For example, to solve this equation for x: a %*% x = b Here’s the code: solve(a, b) ## [,1] [,2] ## [1,] 1 -1 ## [2,] 0 1 And for the inverse: solve(a) ## [,1] [,2] ## [1,] -2.0 1.0 ## [2,] 1.5 -0.5 In that second call to solve(), the lack of a second argument signifies that we simply wish to compute the inverse of the matrix. Here are a few other linear algebra functions: t(): Matrix transpose qr(): QR decomposition chol(): Cholesky decomposition det(): Determinant eigen(): Eigenvalues/eigenvectors diag(): Extracts the diagonal of a square matrix (useful for obtaining variances from a covariance matrix and for constructing a diagonal matrix). 6.1.3 Statistical Distributions (optional) R has functions available for most of the standard statistical distributions. Prefix the name as follows: d for the density function p for the cumulative distribution function q for quantile function r for random number generation The rest of the name indicates the distribution. Let’s start with a very simple distribution, the uniform distribution, which describes an experiment where each value on a continuous scale is equally likely. The runif(n) function returns a vector of n uniformly distributed random numbers. We will visualize the outcome, using ggplot: library(tidyverse) tibble(uniform = runif(100000)) %&gt;% ggplot(mapping = aes(x = uniform)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. These functions also have arguments specific to the given distribution. The uniform distribution has a min and max argument that allows you to control the range on the random numbers. The density function of a uniform distribution looks as follows. Values below 0 and above 1 have 0 probability, while all values between 0 and 1 are equally likely: x &lt;- seq(-1, 2, 0.1) # evaluation from -1 to 2 tibble(x, density_at_x = dunif(x)) %&gt;% ggplot(mapping = aes(x = x, y = density_at_x)) + geom_point() The cumulative distribution function describes the probability that a realization occurs below a certain value. As before, the probability of a realization below 0 is 0. The cumulative distribution function then increases linearly up to one; the probability is 1 (it is certain) that a realization is smaller than or equal to 1: tibble(x, cumulative_distribution_at_x = punif(x)) %&gt;% ggplot(mapping = aes(x = x, y = cumulative_distribution_at_x)) + geom_point() The quantile function is the inverse function of the cumulative distribution function. It returns the value at which the probability of a realization is equal to the one specified in the argument. It is useful to calculate critical values. For the uniform distribution, this is quite boring: qunif(0.025) ## [1] 0.025 Performing the same calculations for the normal distribution is left as an exercise. 6.1.4 Set Operations (optional) R includes some handy set operations, including these: union(x, y): Union of the sets x and y intersect(x, y): Intersection of the sets x and y setdiff(x, y): Set difference between x and y, consisting of all elements of x that are not in y setequal(x, y): Test for equality between x and y c %in% y: Membership, testing whether c is an element of the set y Here are some examples of using these functions: x &lt;- c(1, 2, 5) y &lt;- c(5, 1, 8, 9) union(x, y) ## [1] 1 2 5 8 9 intersect(x, y) ## [1] 1 5 setdiff(x, y) ## [1] 2 setdiff(y, x) ## [1] 8 9 2 %in% x ## [1] TRUE 2 %in% y ## [1] FALSE x %in% y ## [1] TRUE FALSE TRUE The set operators are frequently used with character vectors, for example: intersect(c(&quot;Sara&quot;, &quot;Leo&quot;, &quot;Max&quot;), c(&quot;Mia&quot;, &quot;Leo&quot;)) ## [1] &quot;Leo&quot; 6.1.5 Exercises Create a vector v containing 10 uniformly distributed random numbers. Calculate log(v) and store it as logv Calculate exp(logv) and compare it to v. What do you observe? In the vector c(1, 2, 5, -2, -1, NA), what is the minimum? Hint: You may have a look at the help, the relevant function has an argument that we discussed in the previous lesson. Plot the density function and the cumulative distribution function for a normal distribution. Run an experiment with 10000 draws from a normal distribution and plot its histogram. Look up the critical value where the probability of a realization being lower than that value is 0.025. Consider the vectors: c(2, 5, 1) and c(2, 1, 7, 3). Which elements are in the first but not in the second vector? Which elements are in the second but not in the first vector? Which elements are in both vectors? 6.2 Linear Regression Let’s perform a simple linerar regression, using two vector in the anscombe data set: lm(anscombe$y1 ~ anscombe$x1) ## ## Call: ## lm(formula = anscombe$y1 ~ anscombe$x1) ## ## Coefficients: ## (Intercept) anscombe$x1 ## 3.0001 0.5001 If a data argument is provided, the formula is evaluated within the data frame, similar to the working of mutate(). lm(y1 ~ x1, data = anscombe) ## ## Call: ## lm(formula = y1 ~ x1, data = anscombe) ## ## Coefficients: ## (Intercept) x1 ## 3.0001 0.5001 As we have seen, the summary() function gives a more detailed overview of a regression, so we will usually wrap the function around: summary(lm(y1 ~ x1, data = anscombe)) ## ## Call: ## lm(formula = y1 ~ x1, data = anscombe) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.92127 -0.45577 -0.04136 0.70941 1.83882 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.0001 1.1247 2.667 0.02573 * ## x1 0.5001 0.1179 4.241 0.00217 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.237 on 9 degrees of freedom ## Multiple R-squared: 0.6665, Adjusted R-squared: 0.6295 ## F-statistic: 17.99 on 1 and 9 DF, p-value: 0.00217 Let’s try a more complex example, with more than a single independent variable. In the last lesson, we have investigated the swiss data set. Let’s use it to perform linear regression. To start with, we may want to regress the Fertility variable on all other variables. The formula interface offers a convenient shortcut for this task, the ., which stands for ‘all variables in the data set’: summary(lm(Fertility ~ ., data = swiss)) ## ## Call: ## lm(formula = Fertility ~ ., data = swiss) ## ## Residuals: ## Min 1Q Median 3Q Max ## -15.2743 -5.2617 0.5032 4.1198 15.3213 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 66.91518 10.70604 6.250 1.91e-07 *** ## Agriculture -0.17211 0.07030 -2.448 0.01873 * ## Examination -0.25801 0.25388 -1.016 0.31546 ## Education -0.87094 0.18303 -4.758 2.43e-05 *** ## Catholic 0.10412 0.03526 2.953 0.00519 ** ## Infant.Mortality 1.07705 0.38172 2.822 0.00734 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 7.165 on 41 degrees of freedom ## Multiple R-squared: 0.7067, Adjusted R-squared: 0.671 ## F-statistic: 19.76 on 5 and 41 DF, p-value: 5.594e-10 This is a standard Ordinary Least Square (OLS) regression summary, as it exists in many statistical software applications: According to the output, Education and Agriculture has a significant negative relationship on Fertility, while Catholic and Infant.Mortality has a positive impact. The coefficient on Examination is not significantly different from 0. Note also that the R2 is quite high (0.71), meaning the 5 variables are explaining 71% of the variation in Fertility. If we want to be more specific about which variable we want to include, we can use the + operator in the formula interface. summary(lm(Fertility ~ Education + Catholic + Infant.Mortality, data = swiss)) ## ## Call: ## lm(formula = Fertility ~ Education + Catholic + Infant.Mortality, ## data = swiss) ## ## Residuals: ## Min 1Q Median 3Q Max ## -14.4781 -5.4403 -0.5143 4.1568 15.1187 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 48.67707 7.91908 6.147 2.24e-07 *** ## Education -0.75925 0.11680 -6.501 6.83e-08 *** ## Catholic 0.09607 0.02722 3.530 0.00101 ** ## Infant.Mortality 1.29615 0.38699 3.349 0.00169 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 7.505 on 43 degrees of freedom ## Multiple R-squared: 0.6625, Adjusted R-squared: 0.639 ## F-statistic: 28.14 on 3 and 43 DF, p-value: 3.15e-10 The + here combines variables in the formula. If you want to use the + in the usual way – to add elements of two vectors – you can use the I() function. The following will add Eduction and Catholic element-wise and use the sum as a single regressor variable. summary(lm(Fertility ~ I(Education + Catholic), data = swiss)) ## ## Call: ## lm(formula = Fertility ~ I(Education + Catholic), data = swiss) ## ## Residuals: ## Min 1Q Median 3Q Max ## -39.237 -5.044 0.913 7.511 17.974 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 65.20494 2.83427 23.006 &lt;2e-16 *** ## I(Education + Catholic) 0.09473 0.04278 2.214 0.0319 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 11.99 on 45 degrees of freedom ## Multiple R-squared: 0.09825, Adjusted R-squared: 0.07821 ## F-statistic: 4.903 on 1 and 45 DF, p-value: 0.03192 By default, the model contains an intercept. If you want to turn it off, you can add a 0 at the beginning: summary(lm(Fertility ~ 0 + Education + Catholic, data = swiss)) ## ## Call: ## lm(formula = Fertility ~ 0 + Education + Catholic, data = swiss) ## ## Residuals: ## Min 1Q Median 3Q Max ## -100.272 1.952 22.146 46.403 68.845 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## Education 2.0020 0.4536 4.413 6.29e-05 *** ## Catholic 0.6889 0.1131 6.092 2.28e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 40.06 on 45 degrees of freedom ## Multiple R-squared: 0.6972, Adjusted R-squared: 0.6837 ## F-statistic: 51.8 on 2 and 45 DF, p-value: 2.125e-12 There is a bunch of helper functions for linear models: m &lt;- lm(Fertility ~ Education, data = swiss) coef(m) # the OLS coefficients ## (Intercept) Education ## 79.6100585 -0.8623503 confint(m, level = 0.95) # confidence intervals of the coefficients ## 2.5 % 97.5 % ## (Intercept) 75.372189 83.8479277 ## Education -1.154083 -0.5706181 fitted(m) # fitted values of the regression ## Courtelary Delemont Franches-Mnt Moutier Neuveville ## 69.26186 71.84891 75.29831 73.57361 66.67480 ## Porrentruy Broye Glane Gruyere Sarine ## 73.57361 73.57361 72.71126 73.57361 68.39950 ## Veveyse Aigle Aubonne Avenches Cossonay ## 74.43596 69.26186 73.57361 69.26186 75.29831 ## Echallens Grandson Lausanne La Vallee Lavaux ## 77.88536 72.71126 55.46425 62.36305 71.84891 ## Morges Moudon Nyone Orbe Oron ## 70.98656 77.02301 69.26186 74.43596 78.74771 ## Payerne Paysd&#39;enhaut Rolle Vevey Yverdon ## 72.71126 77.02301 70.98656 63.22540 72.71126 ## Conthey Entremont Herens Martigwy Monthey ## 77.88536 74.43596 77.88536 74.43596 77.02301 ## St Maurice Sierre Sion Boudry La Chauxdfnd ## 71.84891 77.02301 68.39950 69.26186 70.12421 ## Le Locle Neuchatel Val de Ruz ValdeTravers V. De Geneve ## 68.39950 52.01485 73.57361 73.57361 33.90549 ## Rive Droite Rive Gauche ## 54.60190 54.60190 resid(m) # residuals ## Courtelary Delemont Franches-Mnt Moutier Neuveville ## 10.9381450 11.2510941 17.2016929 12.2263935 10.2251959 ## Porrentruy Broye Glane Gruyere Sarine ## 2.5263935 10.2263935 19.6887438 8.8263935 14.5004953 ## Veveyse Aigle Aubonne Avenches Cossonay ## 12.6640432 -5.1618550 -6.6736065 -0.3618550 -13.5983071 ## Echallens Grandson Lausanne La Vallee Lavaux ## -9.5853579 -1.0112562 0.2357497 -8.0630527 -6.7489059 ## Morges Moudon Nyone Orbe Oron ## -5.4865556 -12.0230077 -12.6618550 -17.0359568 -6.2477082 ## Payerne Paysd&#39;enhaut Rolle Vevey Yverdon ## 1.4887438 -5.0230077 -10.4865556 -4.9254030 -7.3112562 ## Conthey Entremont Herens Martigwy Monthey ## -2.3853579 -5.1359568 -0.5853579 -3.9359568 2.3769923 ## St Maurice Sierre Sion Boudry La Chauxdfnd ## -6.8489059 15.1769923 10.9004953 1.1381450 -4.4242053 ## Le Locle Neuchatel Val de Ruz ValdeTravers V. De Geneve ## 4.3004953 12.3851508 4.0263935 -5.9736065 1.0945070 ## Rive Droite Rive Gauche ## -9.9019000 -11.8019000 6.2.1 Example: Bootstrapping standard errors (optional) We want to use our simple linear model to explore another, very powerful statistical technique: the bootstrap. The bootstrap can be used to assess the variability of almost any statistical estimation, whether we understand it or not. Fortunately, we understand linear models, and the accuracy of the coefficients can be easily assessed by looking at the analytically derived standard errors: m &lt;- lm(Fertility ~ Education, data = swiss) summary(m) ## ## Call: ## lm(formula = Fertility ~ Education, data = swiss) ## ## Residuals: ## Min 1Q Median 3Q Max ## -17.036 -6.711 -1.011 9.526 19.689 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 79.6101 2.1041 37.836 &lt; 2e-16 *** ## Education -0.8624 0.1448 -5.954 3.66e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.446 on 45 degrees of freedom ## Multiple R-squared: 0.4406, Adjusted R-squared: 0.4282 ## F-statistic: 35.45 on 1 and 45 DF, p-value: 3.659e-07 A standard error of 0.1448 tells us that there is a 68.3% (pnorm(1) - pnorm(-1)) probability that the true coefficient is between -0.8624 - 0.1448 and -0.8624 + 0.1448. What if we wouldn’t have these standard errors? How much confidence should we have in our estimate of -0.8624? Let’s figuring out by using the bootstrap. First, we create a simple function, boot.fn(), that takes the swiss data set as an argument, plus and index argument, which is used to pick rows in the data. The function returns the coefficients of the model the linear regression model. To keep it simple, we just focus on the second coefficient: boot.fn &lt;- function(data, index){ unname(coef(lm(Fertility ~ Education, data = data[index, ]))[2]) } To verify that it works we are running with the dataset in the original order. boot.fn(swiss, 1:nrow(swiss)) ## [1] -0.8623503 The idea of the bootstrap is to re-estimate our model with mutated data, were the mutated data is a random sample of the original data. An estimation on a mutated dataset can be generated as such: boot.fn(swiss, sample(1:nrow(swiss), replace = TRUE)) ## [1] -0.6325681 The replace = TRUE argument ensures that we are not ending up with the original dataset. If replace is TRUE, some rows will appear multiple times in the mutated dataset. Each time we run this function, we get a different result. The final step of the bootstrap is now to this many times, and analyze the results: res &lt;- numeric(1000) for (i in 1:1000){ res[i] &lt;- boot.fn(swiss, sample(1:nrow(swiss), replace = TRUE)) } Happily, both the mean and the standard deviation is very close to what we got from our the analytic result: mean(res) ## [1] -0.864459 sd(res) ## [1] 0.1371371 The boot function from the boot package gives some more powerful tools to perform bootstrap analysis. For example, it can run simulations on several cores, which may speed up the simulation. library(boot) system.time(boot(swiss, boot.fn, R = 1000, parallel = &quot;multicore&quot;, ncpus = 4)) ## user system elapsed ## 1.527 0.499 0.873 system.time(boot(swiss, boot.fn, R = 1000)) ## user system elapsed ## 1.328 0.023 1.375 6.2.2 Factors in regression models A factor is simply as an integer vector with a bit of extra information. That extra information consists of a record of the distinct values in that vector, called levels: x &lt;- c(&quot;female&quot;, &quot;male&quot;, &quot;male&quot;, &quot;female&quot;) xf &lt;- factor(x) class(xf) ## [1] &quot;factor&quot; To see whats’s in xf, let’s ‘unclass’ it: unclass(xf) ## [1] 1 2 2 1 ## attr(,&quot;levels&quot;) ## [1] &quot;female&quot; &quot;male&quot; The core of xf is not c(\"female\", \"male\", \"male\", \"female\"), but an integer vector c(1, 2, 2, 1). The level attribute maps the integer to their original meaning: 1 means \"female\" and 2 means \"male\". From a memory perspective, this is appealing, since you just have to save an integer value, rather than the whole string. However, today, character vectors are cached in a hash table, and are already memory efficient. So, the two ways of storing data are equivalent from a memory perspective. That’s why we want to get rid of factors whenever we can! In a linear regression, however, factors are cool. To see how they work, let’s add a new variable, Conf, to our dataset, which is \"protestant\" if the share of Catholics is below 50, and \"catholic\" otherwise. We then transform it to a factor. (We can omit the last step, as R will convert all character variable to factors when used in a regression.) library(tidyverse) swiss_plus &lt;- swiss %&gt;% as_tibble() %&gt;% mutate(Conf = if_else(Catholic &gt;= 50, &quot;catholic&quot;, &quot;protestant&quot;)) %&gt;% mutate(Conf = as.factor(Conf)) We have now a categorical variable in our dataset. The usual way to deal with this variable would be to build a dummy variable, which contains 0 for catholic and 1 for protestant locations. Thanks to factors, R will do this automatically for you: summary(lm(Fertility ~ Education + Conf, data = swiss_plus)) ## ## Call: ## lm(formula = Fertility ~ Education + Conf, data = swiss_plus) ## ## Residuals: ## Min 1Q Median 3Q Max ## -17.739 -5.832 -1.953 6.251 15.466 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 83.7551 2.3994 34.907 &lt; 2e-16 *** ## Education -0.8006 0.1355 -5.909 4.59e-07 *** ## Confprotestant -7.8173 2.6512 -2.949 0.0051 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 8.729 on 44 degrees of freedom ## Multiple R-squared: 0.5329, Adjusted R-squared: 0.5117 ## F-statistic: 25.1 on 2 and 44 DF, p-value: 5.332e-08 6.2.3 Exercises Plot the bivariate relationship between Agriculture and Fertility. Perform a bivariate linear regression. Plot both the data and the regression line in a single graph. Also add Education to the regression. How has the coefficient of Agriculture changed? Do you have an explanation for it? Add a dummy variable DCATH that contains 0 if the share of Catholics is below 50, 1 otherwise. Verify that including a factor really leads to the same result. 6.3 Numerical Optimization (optional) R offers a large collection of tools for numerical optimization. The built-in standard function is optim(), which is sufficient in most situations. The syntax of optim() is different from the functions we met so far, in that the second argument of the function is a function itself! Let’s see how it works: Let’s define a simple quadratic function as our objective function: quadratic_function &lt;- function(x){ x^2 } To see how it looks, let’s evaluate it for a sequence of numbers: x &lt;- seq(from = -5, to = 5, by = 0.1) tibble(x, quadratic_function_at_x = quadratic_function(x)) %&gt;% ggplot(mapping = aes(x = x, y = quadratic_function_at_x)) + geom_point() Suppose you want to find the minimum value of this function. Visual inspection already told us that the minimum is at x = 0, where the function evaluates to 0. If you know calculus, you could have derived this result analytically. Often, however, the problem is too complicated, or there is no analytical solution at all. So you want to use numerical optimization instead. A simple way to perform numerical optimization is to evaluate the function over a relevant range of values, and simply pick the lowest. In our example, picking the minimum of y would have given you the correct result. x[which.min(y)] ## [1] -4.9 This was grid-search minimization, which works fine for small problems like this. A problem with grid search is that is quite inefficient. If you know the function is increasing at x = 1 what is the point of checking for a minimum at x = 1.1? The optim function includes several methods that handle this problem in a more efficient way. Usually, for numerical optimization, you need to give the computer a hint where to start, by providing an initial value for each parameter. 0, would be an obvious starting point, but we don’t want to make it too boring, so let’s try something different. op &lt;- optim(4, quadratic_function, method = &quot;Brent&quot;, lower = -100, upper = 100) Here, we were using the “Brent” method, which is suited for the single parameter optimization that we have here. If you have more than one parameter, the default “Nelder-Mead” usually works fine. As we have seen before, complex functions in R often return a complex object. optim returns a list with 5 components, where the first, par, is the one we are mostly interested in: op$par ## [1] -3.552714e-15 For a more complex example, let’s find the solution to the linear regression problem numerically. This is the result we want to replicate: m &lt;- lm(Fertility ~ Education, data = swiss) coef(m) ## (Intercept) Education ## 79.6100585 -0.8623503 The OLS estimator is minimizing the sum of squared residuals. Let us write down our objective function: sum_of_squared_residuals &lt;- function(b){ b0 &lt;- b[1] b1 &lt;- b[2] fitted.values &lt;- b0 + b1 * swiss$Education residuals &lt;- swiss$Fertility - fitted.values squared_residuals &lt;- residuals^2 sum(squared_residuals) } The argument b is a vector of length 2, containing the coefficient for the intercept and for Education. Using the two coefficients and the data, we calculate fitted.values, the value predicted by our model. The difference between the actual values (swiss$Fertility) are the residuals. In order to calculate the sum of squared residuals, we square the residual vector element- wise and sum up the elements, using the sum() function. For every two coefficients, the function returns the sum of squared residuals. E.g., an intercept of 0 and a slope of 1 lead to a pretty high sum of squared residuals: sum_of_squared_residuals(c(0, 1)) ## [1] 183282.9 Using the optim function, we can easily find the coefficients that minimize the function (the initial values are chosen arbitrarily): optim(c(0, 0), sum_of_squared_residuals) ## $par ## [1] 79.6077096 -0.8618092 ## ## $value ## [1] 4015.238 ## ## $counts ## function gradient ## 97 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL Indeed: At a value of 4015.238, The OLS estimator c(79.6077096, -0.8618092) is minimizing the sum of squared residuals! 6.4 Extended Exercises It can be shown analytically that the OLS estimator can be calculated as follows, using matrix algebra: b = (X’X)^(-1)X’y See, e.g., here (the formula is on page 4): http://www.stat.purdue.edu/~jennings/stat514/stat512notes/topic3.pdf where X is the data matrix, possibly including an intercept, and y is a column vector containing the left-hand variable. Let’s consider our simple linear model from above: m &lt;- lm(Fertility ~ Education, data = swiss) coef(m) ## (Intercept) Education ## 79.6100585 -0.8623503 Can you replicate the result of coef(m), using pure matrix algebra? Hints: Construct X first, adding a vector of 1s as an intercept. Make sure X is of class “matrix”, not “data.frame”. It should look like this: # Intercept Education # [1,] 1 12 # [2,] 1 9 # [3,] 1 5 # [4,] 1 7 # [5,] 1 15 This is matrix algebra, so you will need to use matrix multiplication, not the usual element-wise multiplication. Also remember the inverse and the transpose function from above. "]
]
