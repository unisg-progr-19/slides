[
["index.html", "7784 Skills: Programming in R 1 Introduction", " 7784 Skills: Programming in R Kirill Müller, Christoph Sax University of St. Gallen, HS 2019 1 Introduction Intro (Sept. 19): Slides Canvas Page GitHub Organization "],
["visualization-and-reporting.html", "2 Visualization and Reporting", " 2 Visualization and Reporting Visualization and Reporting (Sept. 26): Slides "],
["data-tranformation-i.html", "3 Data Tranformation I", " 3 Data Tranformation I Data Tranformation I (Oct. 3): Slides "],
["data-tranformation-ii.html", "4 Data Tranformation II", " 4 Data Tranformation II Data Tranformation II (Oct. 10): Slides "],
["basics-of-r-base.html", "5 Basics of R base 5.1 The Main Data Structures 5.2 Introduction to Functions 5.3 Higher-Order Functions", " 5 Basics of R base Download as R script Intro Slides We have spent the first lessons of this course with dplyr and ggplot2, two of the great innovations in R universe of the last few years. If you were learning R 10 years ago, R would have looked quite differently. In this lesson, we cover the basics of R base - R (mostly) as it is before you load any packages. 5.1 The Main Data Structures R has four main data structures. In dplyr, we always work on data frames, or tibbles, the equivalent to an Excel sheet - a tabular collection of data. When performing operations, e.g., by using filter(), or mutate() we are performing operations on the columns of a data frame. These columns are vectors, and they are the heart of R. Related to them, matrices are basically two dimensional vectors that are useful in some mathematical or statistical applications. Finally, lists are the most versatile data type, and they are used in many circumstances. We will cover these data structures in the following order: vectors, matrices, lists and data frames, or tibbles (which we already know well). 5.1.1 Vectors, the R Workhorse The vector type is the most basic data structure in R. It’s hard to imagine R code that doesn’t involve vectors. If you use mutate to change columns of your data frame, you are effectively operating on vectors. The elements of a vector must all have the same class, or data type. You can have a vector consisting of three character strings (of class character) or three integer elements (of class integer), but not a vector with one integer element and two character string elements. 5.1.1.1 Vector Classes In many programming languages, vector variables are considered different from scalars, which are single-number variables. However, in R, numbers are actually considered one-element vectors, and there is really no such thing as a scalar. As we have seen previously, all elements of an R vector (or data frame column) must have the same class, which can be integer, numeric (also called double), character (string), logical (boolean). Here are some examples: x_log &lt;- c(TRUE, FALSE) # same as c(T, F) x_int &lt;- c(1L, 2L, 3L) # use 1L to enforce integer, rather than numeric x_num &lt;- c(1, 2, 6.3) # also called &#39;double&#39; x_chr &lt;- c(&quot;Hello World&quot;) If you need to check the class of a variable x, you can use, e.g.: class(x_log) ## [1] &quot;logical&quot; There is a certain order in the list above: logical is the least flexible class, while character is the most flexible. If you combine vectors of different classes, the more flexible class will win: class(c(x_log, x_num)) ## [1] &quot;numeric&quot; class(c(x_int, x_chr)) ## [1] &quot;character&quot; You can change the class of a vector with the following coercion functions. as.logical(c(1, 0)) ## [1] TRUE FALSE as.integer(c(1, 0)) ## [1] 1 0 as.numeric(c(&quot;1&quot;, &quot;2&quot;)) ## [1] 1 2 as.character(c(TRUE, FALSE)) ## [1] &quot;TRUE&quot; &quot;FALSE&quot; These functions will always work if you coerce towards greater flexibility. If you want to go the other way, it may give you NAs and some warnings: as.numeric(c(&quot;hi&quot;, &quot;number&quot;, &quot;1&quot;)) ## Warning: NAs introduced by coercion ## [1] NA NA 1 5.1.1.2 Recycling When applying an operation to two vectors that requires them to be the same length, R automatically recycles, or repeats, the shorter one, until it is long enough to match the longer one. Here is an example: c(1, 2) + c(6, 0, 9, 20, 22, 11) ## [1] 7 2 10 22 23 13 The shorter vector was recycled, so the operation was taken to be as follows: c(1, 2, 1, 2, 1, 2) + c(6, 0, 9, 20, 22, 11) ## [1] 7 2 10 22 23 13 The most common recycling operation involves a vector of length 1: c(6, 0, 9, 20, 22, 11) + 3 ## [1] 9 3 12 23 25 14 In dplyr, only a vector of length 1 is allowed to recycle, the other cases will result in an error. # tibble(a = c(1, 2), b = c(6, 0, 9, 20, 22, 11)) 5.1.1.3 Arithmetic Operators In R, every operator, including + in the following example, is actually a function. 2 + 3 ## [1] 5 The + here is a function with two arguments. A more functional way of writing it is the following: &quot;+&quot;(2, 3) ## [1] 5 Remember that scalars are actually one-element vectors. So, we can add vectors, and the + operation will be applied element-wise. x &lt;- c(1, 2, 4) x + c(5, 0, -1) ## [1] 6 2 3 The same is true, e.g., for multiplication, which is done element by element as well. (We will have a look at matrix multiplication in the next section.) x * c(5, 0, -1) ## [1] 5 0 -4 5.1.1.4 Comparison Operators Similar to arithmetic operators, comparison operators are applied element wise. The following expression will return a single TRUE, as we are comparing two vectors of length 1: 2 &gt; 1 ## [1] TRUE The comparison operator for ‘is equal to’ is ==, not =: 1 + 1 == 2 ## [1] TRUE Here is how they work on longer vectors: x &lt;- c(1, 2, 4, 2) y &lt;- c(2, 2, 4, 5) x == y ## [1] FALSE TRUE TRUE FALSE The usual recycling rules apply as well: x == 2 ## [1] FALSE TRUE FALSE TRUE Here are the other comparison operators: x &lt; y: less than x &lt;= y: less or equal than x &gt;= y: greater or equal than x != y: not equal Logical vectors can be combined by &amp; (AND) or | (OR): a &lt;- x &gt;= 2 b &lt;- x &lt; 4 a &amp; b ## [1] FALSE TRUE FALSE TRUE x &gt;= 4 | x &lt; 2 ## [1] TRUE FALSE TRUE FALSE 5.1.1.5 Indexing One of the most frequently used operations in R base is that of indexing vectors, in which we form a subvector by picking elements. You can use both integer values or logical vectors for indexing. 5.1.1.5.1 Indexing Using Integers We can extract values from a vector, using an integer index: y &lt;- c(1.2, 3.9, 0.4, 0.12) y[c(1, 3)] ## [1] 1.2 0.4 y[2:3] ## [1] 3.9 0.4 v &lt;- 3:4 y[v] ## [1] 0.40 0.12 Note that duplicates are allowed: y[c(1, 1, 3)] ## [1] 1.2 1.2 0.4 Negative subscripts mean that we want to exclude the elements: z &lt;- c(5, 12, 13) z[-1] ## [1] 12 13 z[-1:-2] ## [1] 13 5.1.1.5.2 Logical Indexing Logical indexing is perhaps even more important. Building on the example from above, we could also select element 1 and 3 in the following way: y &lt;- c(1.2, 3.9, 0.4, 0.12) y[c(FALSE, TRUE, FALSE, TRUE)] ## [1] 3.90 0.12 Logical indexing picks the TRUEs but not the FALSEs. This is the main building block for filtering. Suppose you have: y[y &gt; 1] ## [1] 1.2 3.9 This will return all elements that are bigger than 1. How is this done? First, R had evaluated the comparison, y &gt; 1, which led to logical vector: y &gt; 1 ## [1] TRUE TRUE FALSE FALSE Second, using logical indexing, this vector was then used to pick those elements that evaluated to TRUE. So y[y &gt; 1] is actually the same as: y[c(TRUE, TRUE, FALSE, TRUE)] ## [1] 1.20 3.90 0.12 We have already seen that you can assign to individual elements of a vector, using integer indices: y[c(2, 4)] &lt;- 5 Of course, you can do the same with logical indices: y[c(FALSE, TRUE, FALSE, TRUE)] &lt;- 5 This is a very powerful tool. For example, if you want to truncate all negative numbers in a vector to 0, you can use: z &lt;- c(-3, 1.2, 2, -22) z[z &lt; 0] &lt;- 0 Or use it with the %in% operator you encountered before: z[z %in% y] ## [1] 1.2 5.1.1.6 Exercises Create a vector called v1 containing the numbers 2, 5, 8, 12 and 16. Extract the values at positions 2 and 5 from v1. Use x:y notation to make a second vector called v2 containing the numbers 5 to 9. Subtract v2 from v1 and look at the result. Generate a vector with 1000 standard-normally distributed random numbers (use rnorm()). Store the result as v3. Extract the numbers that are bigger than 2. 5.1.2 Matrices An R matrix corresponds to the mathematical concept of the same name: a rectangular array of numbers (most of the time), or some other type. Here is some sample matrix code: m &lt;- matrix(c(1, 4, 2, 2), nrow = 2, ncol = 2) m ## [,1] [,2] ## [1,] 1 2 ## [2,] 4 2 The main use of matrices is for matrix algebra. You can do all kind of matrix algebra operations, right out of R base: m %*% m # matrix multiplication ## [,1] [,2] ## [1,] 9 6 ## [2,] 12 12 m * m # elementwise multiplication ## [,1] [,2] ## [1,] 1 4 ## [2,] 16 4 m %*% solve(m) # inverse of a matrix ## [,1] [,2] ## [1,] 1 0 ## [2,] 0 1 m / m # elementwise division ## [,1] [,2] ## [1,] 1 1 ## [2,] 1 1 m + m ## [,1] [,2] ## [1,] 2 4 ## [2,] 8 4 m - m ## [,1] [,2] ## [1,] 0 0 ## [2,] 0 0 t(m) # matrix transpose ## [,1] [,2] ## [1,] 1 4 ## [2,] 2 2 qr(m) # QR decomposition ## $qr ## [,1] [,2] ## [1,] -4.1231056 -2.425356 ## [2,] 0.9701425 -1.455214 ## ## $rank ## [1] 2 ## ## $qraux ## [1] 1.242536 1.455214 ## ## $pivot ## [1] 1 2 ## ## attr(,&quot;class&quot;) ## [1] &quot;qr&quot; det(m) # determinant ## [1] -6 eigen(m) # eigenvalues/eigenvectors ## eigen() decomposition ## $values ## [1] 4.372281 -1.372281 ## ## $vectors ## [,1] [,2] ## [1,] -0.5101065 -0.6445673 ## [2,] -0.8601113 0.7645475 diag(m) # diagonal ## [1] 1 2 Matrices are indexed by double subscripting: m[1, 2] ## [1] 2 m[2, 2] ## [1] 2 You can extract submatrices from a matrix, much as you extract subvectors from vectors: m[1, ] # row 1 ## [1] 1 2 m[1, , drop = FALSE] # keeps being a matrix ## [,1] [,2] ## [1,] 1 2 m[, 2] # column 2 ## [1] 2 2 5.1.2.1 Exercises Create a 10 x 10 matrix that contains a sequence of numbers (use the : notation). Use the transpose function on the matrix Extract the 2. column of the matrix Extract the 5. row of the matrix Extract the 5. and the 6. row of the matrix Compare the classes of the results in 3. and 4. to each other Modify 3., so that it returns the same class as 4. 5.1.3 Lists Like an R vector, an R list is a container for values, but its contents can be items of different data types, or different length. Here’s an example: x &lt;- list(u = c(2, 3, 4), v = &quot;abc&quot;) x ## $u ## [1] 2 3 4 ## ## $v ## [1] &quot;abc&quot; x$u ## [1] 2 3 4 The expression x$u refers to the u component in the list x. x[[&#39;u&#39;]] ## [1] 2 3 4 x[[1]] ## [1] 2 3 4 We can also refer to list components by their numerical indices. However, note that in this case, we use double brackets instead of single ones. We can also use single brackets rather than double brackets to get a subset of the list. x[&#39;u&#39;] ## $u ## [1] 2 3 4 x[1] ## $u ## [1] 2 3 4 x[1:2] ## $u ## [1] 2 3 4 ## ## $v ## [1] &quot;abc&quot; Note that x[[1]] returns the component (a numeric vector), while x[1] returns a subset of the list (a list of length 1): class(x[1]) ## [1] &quot;list&quot; class(x[[1]]) ## [1] &quot;numeric&quot; Hadley Wickham’s visualization helps a lot here: https://twitter.com/hadleywickham/status/643381054758363136 Lists are not restricted to containing vectors. In fact, they can contain anything, for example, a data frames: ll &lt;- list(mtcars = mtcars, u = c(2, 3, 4)) 5.1.3.1 Exercises Generate two random vectors of length 10, a, and b. Combine them in a list, call it l1. Compare the classes of l1[2] and l1[[2]]. Can you explain the difference? 5.1.4 Data Frames As we saw in many places, a typical data set contains data of different classes. Instead of a matrix, we use a data frame, or tibble. Technically, a data frame in R is a list, with each component of the list being a vector corresponding to a column in our data. You can create a data frame using tibble from tidyverse: library(tidyverse) d &lt;- tibble(kids = c(&quot;Jack&quot;, &quot;Jill&quot;), ages = c(12, 10)) d ## # A tibble: 2 x 2 ## kids ages ## &lt;chr&gt; &lt;dbl&gt; ## 1 Jack 12 ## 2 Jill 10 Here, I am using the tibble() function from tidyverse, rather than the R base equivalent, data.frame(). The reason for this is that data.frame() has some undesirable features, such as converting character vectors into factors. Because data frames are technically a list, we can access its vectors the same way as we access components of a list. d$ages ## [1] 12 10 d[[&#39;ages&#39;]] ## [1] 12 10 d[[2]] # usually not recommended ## [1] 12 10 Typically, though, data frames are created by reading in a data set from a file or a database, as we saw in the previous days of the workshop. Contrary to lists, data frames require their columns to be of the same length. If they are not, values will be recycled. That is why the following works: tibble(kids = c(&quot;Jack&quot;, &quot;Jill&quot;), ages = c(12, 10), type = &quot;kid&quot;) ## # A tibble: 2 x 3 ## kids ages type ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Jack 12 kid ## 2 Jill 10 kid 5.2 Introduction to Functions As in most programming languages, the heart of R programming consists of writing functions. A function is a group of instructions that takes inputs, uses them to compute other values, and returns a result. Let’s write a function that divides all elements of a vector by 2: half &lt;- function(x) { x / 2 } This is a function named half, whose purpose is to divides every element of a vector by 2. It’s a pretty pointless function, as the operation itself is so simple. Arguments to a function are enclosed by parentheses (()); the body of the function is enclosed by braces ({}). Let’s see how it works: half(c(3, 2, 1)) ## [1] 1.5 1.0 0.5 half(AirPassengers) ## Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec ## 1949 56.0 59.0 66.0 64.5 60.5 67.5 74.0 74.0 68.0 59.5 52.0 59.0 ## 1950 57.5 63.0 70.5 67.5 62.5 74.5 85.0 85.0 79.0 66.5 57.0 70.0 ## 1951 72.5 75.0 89.0 81.5 86.0 89.0 99.5 99.5 92.0 81.0 73.0 83.0 ## 1952 85.5 90.0 96.5 90.5 91.5 109.0 115.0 121.0 104.5 95.5 86.0 97.0 ## 1953 98.0 98.0 118.0 117.5 114.5 121.5 132.0 136.0 118.5 105.5 90.0 100.5 ## 1954 102.0 94.0 117.5 113.5 117.0 132.0 151.0 146.5 129.5 114.5 101.5 114.5 ## 1955 121.0 116.5 133.5 134.5 135.0 157.5 182.0 173.5 156.0 137.0 118.5 139.0 ## 1956 142.0 138.5 158.5 156.5 159.0 187.0 206.5 202.5 177.5 153.0 135.5 153.0 ## 1957 157.5 150.5 178.0 174.0 177.5 211.0 232.5 233.5 202.0 173.5 152.5 168.0 ## 1958 170.0 159.0 181.0 174.0 181.5 217.5 245.5 252.5 202.0 179.5 155.0 168.5 ## 1959 180.0 171.0 203.0 198.0 210.0 236.0 274.0 279.5 231.5 203.5 181.0 202.5 ## 1960 208.5 195.5 209.5 230.5 236.0 267.5 311.0 303.0 254.0 230.5 195.0 216.0 here we are saving the output in a variable inside the function: half &lt;- function(x) { z &lt;- x / 2 z } An R functions will return the last value computed if there is no explicit return() call. We could have been more explicit, but it is usually not necessary: half &lt;- function(x) { z &lt;- x / 2 return(z) } Let’s make it a bit more complex, by adding an additional argument: fraction &lt;- function(x, denominator){ x / denominator } fraction(c(2, 3, 4), 4) ## [1] 0.50 0.75 1.00 if you have more than one argument, it is a good practice to name the arguments fraction(x = c(2, 3, 4), denominator = 4) ## [1] 0.50 0.75 1.00 that way, they become independent of the order, which is a very useful if the number of argument becomes large. fraction(denominator = 4, x = c(2, 3, 4)) ## [1] 0.50 0.75 1.00 5.2.1 Variable Scope A variable that is visible only within a function body is said to be local to that function. In square(), x is a local variable. In fraction(), x and p are local variables. They disappear after the function returns: fraction(x = c(2, 3, 4), denominator = 4) ## [1] 0.50 0.75 1.00 # denominator # Error: object &#39;denominator&#39; not found x ## $u ## [1] 2 3 4 ## ## $v ## [1] &quot;abc&quot; Note that the x is not the x we used in the fraction function, but rather the x defined earlier. x here is called a global variable, while the x in the function is a local variable. Global variables are shown in RStudio in the environment pane. A global variable can be written to from within a function by using R’s superassignment operator, &lt;&lt;-, but this is rarely recommended. 5.2.2 Default Arguments A nice feature of R functions is that you can set defaults to arguments. Let’s modify the fraction function from above: fraction &lt;- function(x, denominator = 2) { x / denominator } Here denominator will be initialized to 2 if the user does not specify p in the call. So we can use fraction() the same way as half(): fraction(c(2, 2)) ## [1] 1 1 half(c(2, 2)) ## [1] 1 1 or use its extended capabilities: fraction(c(2, 2), 3) ## [1] 0.6666667 0.6666667 5.2.3 Exercises Write a function add_constant that adds a constant to a vector, and set the default value of the constant to 10. Apply it to the AirPassengers series. You can use your function within dplyr. Using the mpg dataset, use add_constant within mutate to add a constant value (100) to the number of cyl. (evil) We saw that &quot;+&quot; is actually a function. In R, it is easily possible to change the working of such a fundamental function (this is, in fact, what ggplot does). In order to do so, let’s write a new function with the same name, &quot;+&quot;, and two arguments, a, and b. But instead of summing the values, let’s subtract them (or figure out something more evil). Verify the result of 1 + 1. Cool, isn’t it? (rm(&quot;+&quot;) will restore sanity.) 5.3 Higher-Order Functions Because functions in R are objects like any other objects, it is easy to write functions that return functions or take functions as arguments. We saw some examples above. The most prominent of these functions are higher-order functions, which are very central to R. They are called apply or lapply, or similar, and there are many of them. To avoid too much confusion, we will restrict ourself to map, the tidyverse equivalent of lapply, 5.3.1 map The map function ‘maps’ a function to each component of a list. Because lists are such a useful container for objects in R, very often, you want to map a function to each component of a list. Here is an example: ll &lt;- list(a = c(2, 3, 4), b = c(1, 2, 3), c = c(5, 2, 1)) This is a list with three vectors, of which we want to calculate the means. Here’s an expression that does what we want (we will cover loops later on): z &lt;- NULL for (vi in ll){ z &lt;- c(z, mean(vi)) } We loop through each component of the list, calculate the mean and add it to an output vector, z. With the map function, this can be written much more concise: library(tidyverse) map(ll, mean) ## $a ## [1] 3 ## ## $b ## [1] 2 ## ## $c ## [1] 2.666667 So map returns a list of the same length as the first argument (a list), each element of which is the result of applying the second argument (a function) to the corresponding component of the list argument. If the input list is named (as in the example), so will be the output. Additional arguments to the mean() function can be included as well: map(ll, mean, na.rm = TRUE) ## $a ## [1] 3 ## ## $b ## [1] 2 ## ## $c ## [1] 2.666667 If you want to convert the list to a vector (as in the loop example), use unlist on the result: unlist(map(ll, mean)) ## a b c ## 3.000000 2.000000 2.666667 Of course, you can use map with your own functions. Here we want to pick the second element of each vector in the list: pick_second &lt;- function(x){ x[2] } map(ll, pick_second) ## $a ## [1] 3 ## ## $b ## [1] 2 ## ## $c ## [1] 2 In cases like this, we may want to use the possibility of having anonymous functions, i.e. functions without a name. So we just substitute pick_second by its definition: map(ll, function(x) x[2]) ## $a ## [1] 3 ## ## $b ## [1] 2 ## ## $c ## [1] 2 For simple one line functions like this, it may be justified to omit the curly braces. ### Exercises Use map to calculate the mean of each variable in the swiss dataset. Convert the resulting list to a vector. Use map to coerce the variables in the swiss dataset to character. Using map, generate a list containing 10 random vectors of random length between 1 and 10. Use the help to see what the colSums() function does. Using apply, try writing your own version, colSums2(). 5.3.2 Loops In many programming languages, one of first things you learn are loops. There is a reason that we didn’t cover them until now, and also only for completeness. Loops in R are slow, and they are – most of the time – unnecessary. This is because many operations in R are vectorized anyway, so there is no need to loop over each element. Also the group_by() operation in dplyr offers a much more elegant way of applying a function repeatedly to a group of data. Third, there are higher order functions like map (or the tidyverse equivalent: purrr::map) that save you from loops most of the time. Anyway, here is the loop: for (i in 1:10) { print(i) } ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 ## [1] 6 ## [1] 7 ## [1] 8 ## [1] 9 ## [1] 10 There will be one iteration of the loop for each component of the vector 1:10, with i taking on the values of those components – in the first iteration, i = (1:10)[1]; in the second iteration, i = (1:10)[2]; and so on. And we are not restricted to integer vectors, but can loop over any vector, even over lists: z &lt;- NULL for (i in list(swiss, mtcars)) { z &lt;- c(z, colnames(i)) } z ## [1] &quot;Fertility&quot; &quot;Agriculture&quot; &quot;Examination&quot; &quot;Education&quot; ## [5] &quot;Catholic&quot; &quot;Infant.Mortality&quot; &quot;mpg&quot; &quot;cyl&quot; ## [9] &quot;disp&quot; &quot;hp&quot; &quot;drat&quot; &quot;wt&quot; ## [13] &quot;qsec&quot; &quot;vs&quot; &quot;am&quot; &quot;gear&quot; ## [17] &quot;carb&quot; But we saw a much clearer way of doing this above: unlist(map(list(swiss, mtcars), colnames)) ## [1] &quot;Fertility&quot; &quot;Agriculture&quot; &quot;Examination&quot; &quot;Education&quot; ## [5] &quot;Catholic&quot; &quot;Infant.Mortality&quot; &quot;mpg&quot; &quot;cyl&quot; ## [9] &quot;disp&quot; &quot;hp&quot; &quot;drat&quot; &quot;wt&quot; ## [13] &quot;qsec&quot; &quot;vs&quot; &quot;am&quot; &quot;gear&quot; ## [17] &quot;carb&quot; "],
["math-and-statistics.html", "6 Math and Statistics 6.1 Math 6.2 Linear Regression 6.3 Numerical Optimization (optional) 6.4 Extended Exercises", " 6 Math and Statistics Download as R script Intro Slides R calls itself a software environment for statistical computing. Math and statistics are at the heart of R. This Lesson provides an overview of the many build-in functions for math and statistics. We will also discuss some basic statistical modeling techniques. 6.1 Math 6.1.1 Basic Math Functions R includes an extensive set of built-in math functions. Here is a partial list: log(): Natural logarithm exp(): Exponential function, base e sqrt(): Square root abs(): Absolute value sin(), cos(), and so on: Trigonometric functions min() and max(): Minimum value and maximum value within a vector which.min() and which.max(): Index of the minimal element and maximal element of a vector pmin() and pmax(): Element-wise minima and maxima of several vectors sum() and prod(): Sum and product of the elements of a vector cumsum() and cumprod(): Cumulative sum and product of the elements of a vector round(), floor(), and ceiling(): Round to the closest integer, to the closest integer below, and to the closest integer above Most of these functions are self explaining. Some remarks: The functions cumsum() and cumprod() return cumulative sums and products. x &lt;- c(12, 5, 13) cumsum(x) ## [1] 12 17 30 cumprod(x) ## [1] 12 60 780 In x, the sum of the first element is 12, the sum of the first two elements is 17, and the sum of the first three elements is 30. The function cumprod() works the same way as cumsum(), but with the product instead of the sum. We briefly mentioned the difference between min() and pmin() in the last chapter. The former simply combines all its arguments into one long vector and returns the minimum value in that vector. In contrast, if pmin() is applied to two or more vectors, it returns a vector of the pair-wise minima, hence the name pmin. Here’s an example: x &lt;- c(1, 5, 6, 2, 3, 1) y &lt;- c(3, 5, 2, 3, 2, 2) min(x) ## [1] 1 pmin(x, y) ## [1] 1 5 2 2 2 1 In the first case, min() computed the smallest value in c(1, 5, 6, 2, 3, 1). But the call to pmin() computed the smaller of 1 and 3, yielding 1; then the smaller of 5 and 5, which is 5; then the minimum of 6 and 2, giving 2, and so on. 6.1.2 Linear Algebra Operations on Vectors and Matrices (optional) Multiplying a vector by a scalar works directly, as you saw earlier. Here’s another example: y &lt;- c(1, 3, 4, 10) 2 * y ## [1] 2 6 8 20 For matrix multiplication in the mathematical sense, the operator to use is %*%, not * . For instance, here we compute the matrix product: Here’s the code: a &lt;- matrix(c(1, 3, 2, 4), nrow = 2, ncol = 2) x &lt;- matrix(c(1, 0, -1, 1), nrow = 2, ncol = 2) b &lt;- a %*% x; b ## [,1] [,2] ## [1,] 1 1 ## [2,] 3 1 The function solve() will solve systems of linear equations and find matrix inverses. For example, to solve this equation for x: a %*% x = b Here’s the code: solve(a, b) ## [,1] [,2] ## [1,] 1 -1 ## [2,] 0 1 And for the inverse: solve(a) ## [,1] [,2] ## [1,] -2.0 1.0 ## [2,] 1.5 -0.5 In that second call to solve(), the lack of a second argument signifies that we simply wish to compute the inverse of the matrix. Here are a few other linear algebra functions: t(): Matrix transpose qr(): QR decomposition chol(): Cholesky decomposition det(): Determinant eigen(): Eigenvalues/eigenvectors diag(): Extracts the diagonal of a square matrix (useful for obtaining variances from a covariance matrix and for constructing a diagonal matrix). 6.1.3 Statistical Distributions (optional) R has functions available for most of the standard statistical distributions. Prefix the name as follows: d for the density function p for the cumulative distribution function q for quantile function r for random number generation The rest of the name indicates the distribution. Let’s start with a very simple distribution, the uniform distribution, which describes an experiment where each value on a continuous scale is equally likely. The runif(n) function returns a vector of n uniformly distributed random numbers. We will visualize the outcome, using ggplot: library(tidyverse) tibble(uniform = runif(100000)) %&gt;% ggplot(mapping = aes(x = uniform)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. These functions also have arguments specific to the given distribution. The uniform distribution has a min and max argument that allows you to control the range on the random numbers. The density function of a uniform distribution looks as follows. Values below 0 and above 1 have 0 probability, while all values between 0 and 1 are equally likely: x &lt;- seq(-1, 2, 0.1) # evaluation from -1 to 2 tibble(x, density_at_x = dunif(x)) %&gt;% ggplot(mapping = aes(x = x, y = density_at_x)) + geom_point() The cumulative distribution function describes the probability that a realization occurs below a certain value. As before, the probability of a realization below 0 is 0. The cumulative distribution function then increases linearly up to one; the probability is 1 (it is certain) that a realization is smaller than or equal to 1: tibble(x, cumulative_distribution_at_x = punif(x)) %&gt;% ggplot(mapping = aes(x = x, y = cumulative_distribution_at_x)) + geom_point() The quantile function is the inverse function of the cumulative distribution function. It returns the value at which the probability of a realization is equal to the one specified in the argument. It is useful to calculate critical values. For the uniform distribution, this is quite boring: qunif(0.025) ## [1] 0.025 Performing the same calculations for the normal distribution is left as an exercise. 6.1.4 Set Operations (optional) R includes some handy set operations, including these: union(x, y): Union of the sets x and y intersect(x, y): Intersection of the sets x and y setdiff(x, y): Set difference between x and y, consisting of all elements of x that are not in y setequal(x, y): Test for equality between x and y c %in% y: Membership, testing whether c is an element of the set y Here are some examples of using these functions: x &lt;- c(1, 2, 5) y &lt;- c(5, 1, 8, 9) union(x, y) ## [1] 1 2 5 8 9 intersect(x, y) ## [1] 1 5 setdiff(x, y) ## [1] 2 setdiff(y, x) ## [1] 8 9 2 %in% x ## [1] TRUE 2 %in% y ## [1] FALSE x %in% y ## [1] TRUE FALSE TRUE The set operators are frequently used with character vectors, for example: intersect(c(&quot;Sara&quot;, &quot;Leo&quot;, &quot;Max&quot;), c(&quot;Mia&quot;, &quot;Leo&quot;)) ## [1] &quot;Leo&quot; 6.1.5 Exercises Create a vector v containing 10 uniformly distributed random numbers. Calculate log(v) and store it as logv Calculate exp(logv) and compare it to v. What do you observe? In the vector c(1, 2, 5, -2, -1, NA), what is the minimum? Hint: You may have a look at the help, the relevant function has an argument that we discussed in the previous lesson. Plot the density function and the cumulative distribution function for a normal distribution. Run an experiment with 10000 draws from a normal distribution and plot its histogram. Look up the critical value where the probability of a realization being lower than that value is 0.025. Consider the vectors: c(2, 5, 1) and c(2, 1, 7, 3). Which elements are in the first but not in the second vector? Which elements are in the second but not in the first vector? Which elements are in both vectors? 6.2 Linear Regression Let’s perform a simple linerar regression, using two vector in the anscombe data set: lm(anscombe$y1 ~ anscombe$x1) ## ## Call: ## lm(formula = anscombe$y1 ~ anscombe$x1) ## ## Coefficients: ## (Intercept) anscombe$x1 ## 3.0001 0.5001 If a data argument is provided, the formula is evaluated within the data frame, similar to the working of mutate(). lm(y1 ~ x1, data = anscombe) ## ## Call: ## lm(formula = y1 ~ x1, data = anscombe) ## ## Coefficients: ## (Intercept) x1 ## 3.0001 0.5001 As we have seen, the summary() function gives a more detailed overview of a regression, so we will usually wrap the function around: summary(lm(y1 ~ x1, data = anscombe)) ## ## Call: ## lm(formula = y1 ~ x1, data = anscombe) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.92127 -0.45577 -0.04136 0.70941 1.83882 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.0001 1.1247 2.667 0.02573 * ## x1 0.5001 0.1179 4.241 0.00217 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.237 on 9 degrees of freedom ## Multiple R-squared: 0.6665, Adjusted R-squared: 0.6295 ## F-statistic: 17.99 on 1 and 9 DF, p-value: 0.00217 Let’s try a more complex example, with more than a single independent variable. In the last lesson, we have investigated the swiss data set. Let’s use it to perform linear regression. To start with, we may want to regress the Fertility variable on all other variables. The formula interface offers a convenient shortcut for this task, the ., which stands for ‘all variables in the data set’: summary(lm(Fertility ~ ., data = swiss)) ## ## Call: ## lm(formula = Fertility ~ ., data = swiss) ## ## Residuals: ## Min 1Q Median 3Q Max ## -15.2743 -5.2617 0.5032 4.1198 15.3213 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 66.91518 10.70604 6.250 1.91e-07 *** ## Agriculture -0.17211 0.07030 -2.448 0.01873 * ## Examination -0.25801 0.25388 -1.016 0.31546 ## Education -0.87094 0.18303 -4.758 2.43e-05 *** ## Catholic 0.10412 0.03526 2.953 0.00519 ** ## Infant.Mortality 1.07705 0.38172 2.822 0.00734 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 7.165 on 41 degrees of freedom ## Multiple R-squared: 0.7067, Adjusted R-squared: 0.671 ## F-statistic: 19.76 on 5 and 41 DF, p-value: 5.594e-10 This is a standard Ordinary Least Square (OLS) regression summary, as it exists in many statistical software applications: According to the output, Education and Agriculture has a significant negative relationship on Fertility, while Catholic and Infant.Mortality has a positive impact. The coefficient on Examination is not significantly different from 0. Note also that the R2 is quite high (0.71), meaning the 5 variables are explaining 71% of the variation in Fertility. If we want to be more specific about which variable we want to include, we can use the + operator in the formula interface. summary(lm(Fertility ~ Education + Catholic + Infant.Mortality, data = swiss)) ## ## Call: ## lm(formula = Fertility ~ Education + Catholic + Infant.Mortality, ## data = swiss) ## ## Residuals: ## Min 1Q Median 3Q Max ## -14.4781 -5.4403 -0.5143 4.1568 15.1187 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 48.67707 7.91908 6.147 2.24e-07 *** ## Education -0.75925 0.11680 -6.501 6.83e-08 *** ## Catholic 0.09607 0.02722 3.530 0.00101 ** ## Infant.Mortality 1.29615 0.38699 3.349 0.00169 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 7.505 on 43 degrees of freedom ## Multiple R-squared: 0.6625, Adjusted R-squared: 0.639 ## F-statistic: 28.14 on 3 and 43 DF, p-value: 3.15e-10 The + here combines variables in the formula. If you want to use the + in the usual way – to add elements of two vectors – you can use the I() function. The following will add Eduction and Catholic element-wise and use the sum as a single regressor variable. summary(lm(Fertility ~ I(Education + Catholic), data = swiss)) ## ## Call: ## lm(formula = Fertility ~ I(Education + Catholic), data = swiss) ## ## Residuals: ## Min 1Q Median 3Q Max ## -39.237 -5.044 0.913 7.511 17.974 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 65.20494 2.83427 23.006 &lt;2e-16 *** ## I(Education + Catholic) 0.09473 0.04278 2.214 0.0319 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 11.99 on 45 degrees of freedom ## Multiple R-squared: 0.09825, Adjusted R-squared: 0.07821 ## F-statistic: 4.903 on 1 and 45 DF, p-value: 0.03192 By default, the model contains an intercept. If you want to turn it off, you can add a 0 at the beginning: summary(lm(Fertility ~ 0 + Education + Catholic, data = swiss)) ## ## Call: ## lm(formula = Fertility ~ 0 + Education + Catholic, data = swiss) ## ## Residuals: ## Min 1Q Median 3Q Max ## -100.272 1.952 22.146 46.403 68.845 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## Education 2.0020 0.4536 4.413 6.29e-05 *** ## Catholic 0.6889 0.1131 6.092 2.28e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 40.06 on 45 degrees of freedom ## Multiple R-squared: 0.6972, Adjusted R-squared: 0.6837 ## F-statistic: 51.8 on 2 and 45 DF, p-value: 2.125e-12 There is a bunch of helper functions for linear models: m &lt;- lm(Fertility ~ Education, data = swiss) coef(m) # the OLS coefficients ## (Intercept) Education ## 79.6100585 -0.8623503 confint(m, level = 0.95) # confidence intervals of the coefficients ## 2.5 % 97.5 % ## (Intercept) 75.372189 83.8479277 ## Education -1.154083 -0.5706181 fitted(m) # fitted values of the regression ## Courtelary Delemont Franches-Mnt Moutier Neuveville Porrentruy ## 69.26186 71.84891 75.29831 73.57361 66.67480 73.57361 ## Broye Glane Gruyere Sarine Veveyse Aigle ## 73.57361 72.71126 73.57361 68.39950 74.43596 69.26186 ## Aubonne Avenches Cossonay Echallens Grandson Lausanne ## 73.57361 69.26186 75.29831 77.88536 72.71126 55.46425 ## La Vallee Lavaux Morges Moudon Nyone Orbe ## 62.36305 71.84891 70.98656 77.02301 69.26186 74.43596 ## Oron Payerne Paysd&#39;enhaut Rolle Vevey Yverdon ## 78.74771 72.71126 77.02301 70.98656 63.22540 72.71126 ## Conthey Entremont Herens Martigwy Monthey St Maurice ## 77.88536 74.43596 77.88536 74.43596 77.02301 71.84891 ## Sierre Sion Boudry La Chauxdfnd Le Locle Neuchatel ## 77.02301 68.39950 69.26186 70.12421 68.39950 52.01485 ## Val de Ruz ValdeTravers V. De Geneve Rive Droite Rive Gauche ## 73.57361 73.57361 33.90549 54.60190 54.60190 resid(m) # residuals ## Courtelary Delemont Franches-Mnt Moutier Neuveville Porrentruy ## 10.9381450 11.2510941 17.2016929 12.2263935 10.2251959 2.5263935 ## Broye Glane Gruyere Sarine Veveyse Aigle ## 10.2263935 19.6887438 8.8263935 14.5004953 12.6640432 -5.1618550 ## Aubonne Avenches Cossonay Echallens Grandson Lausanne ## -6.6736065 -0.3618550 -13.5983071 -9.5853579 -1.0112562 0.2357497 ## La Vallee Lavaux Morges Moudon Nyone Orbe ## -8.0630527 -6.7489059 -5.4865556 -12.0230077 -12.6618550 -17.0359568 ## Oron Payerne Paysd&#39;enhaut Rolle Vevey Yverdon ## -6.2477082 1.4887438 -5.0230077 -10.4865556 -4.9254030 -7.3112562 ## Conthey Entremont Herens Martigwy Monthey St Maurice ## -2.3853579 -5.1359568 -0.5853579 -3.9359568 2.3769923 -6.8489059 ## Sierre Sion Boudry La Chauxdfnd Le Locle Neuchatel ## 15.1769923 10.9004953 1.1381450 -4.4242053 4.3004953 12.3851508 ## Val de Ruz ValdeTravers V. De Geneve Rive Droite Rive Gauche ## 4.0263935 -5.9736065 1.0945070 -9.9019000 -11.8019000 6.2.1 Example: Bootstrapping standard errors (optional) We want to use our simple linear model to explore another, very powerful statistical technique: the bootstrap. The bootstrap can be used to assess the variability of almost any statistical estimation, whether we understand it or not. Fortunately, we understand linear models, and the accuracy of the coefficients can be easily assessed by looking at the analytically derived standard errors: m &lt;- lm(Fertility ~ Education, data = swiss) summary(m) ## ## Call: ## lm(formula = Fertility ~ Education, data = swiss) ## ## Residuals: ## Min 1Q Median 3Q Max ## -17.036 -6.711 -1.011 9.526 19.689 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 79.6101 2.1041 37.836 &lt; 2e-16 *** ## Education -0.8624 0.1448 -5.954 3.66e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.446 on 45 degrees of freedom ## Multiple R-squared: 0.4406, Adjusted R-squared: 0.4282 ## F-statistic: 35.45 on 1 and 45 DF, p-value: 3.659e-07 A standard error of 0.1448 tells us that there is a 68.3% (pnorm(1) - pnorm(-1)) probability that the true coefficient is between -0.8624 - 0.1448 and -0.8624 + 0.1448. What if we wouldn’t have these standard errors? How much confidence should we have in our estimate of -0.8624? Let’s figuring out by using the bootstrap. First, we create a simple function, boot.fn(), that takes the swiss data set as an argument, plus and index argument, which is used to pick rows in the data. The function returns the coefficients of the model the linear regression model. To keep it simple, we just focus on the second coefficient: boot.fn &lt;- function(data, index){ unname(coef(lm(Fertility ~ Education, data = data[index, ]))[2]) } To verify that it works we are running with the dataset in the original order. boot.fn(swiss, 1:nrow(swiss)) ## [1] -0.8623503 The idea of the bootstrap is to re-estimate our model with mutated data, were the mutated data is a random sample of the original data. An estimation on a mutated dataset can be generated as such: boot.fn(swiss, sample(1:nrow(swiss), replace = TRUE)) ## [1] -1.099617 The replace = TRUE argument ensures that we are not ending up with the original dataset. If replace is TRUE, some rows will appear multiple times in the mutated dataset. Each time we run this function, we get a different result. The final step of the bootstrap is now to this many times, and analyze the results: res &lt;- numeric(1000) for (i in 1:1000){ res[i] &lt;- boot.fn(swiss, sample(1:nrow(swiss), replace = TRUE)) } Happily, both the mean and the standard deviation is very close to what we got from our the analytic result: mean(res) ## [1] -0.8713993 sd(res) ## [1] 0.1503248 The boot function from the boot package gives some more powerful tools to perform bootstrap analysis. For example, it can run simulations on several cores, which may speed up the simulation. library(boot) system.time(boot(swiss, boot.fn, R = 1000, parallel = &quot;multicore&quot;, ncpus = 4)) ## user system elapsed ## 0.373 0.128 0.259 system.time(boot(swiss, boot.fn, R = 1000)) ## user system elapsed ## 0.585 0.006 0.591 6.2.2 Factors in regression models A factor is simply as an integer vector with a bit of extra information. That extra information consists of a record of the distinct values in that vector, called levels: x &lt;- c(&quot;female&quot;, &quot;male&quot;, &quot;male&quot;, &quot;female&quot;) xf &lt;- factor(x) class(xf) ## [1] &quot;factor&quot; To see whats’s in xf, let’s ‘unclass’ it: unclass(xf) ## [1] 1 2 2 1 ## attr(,&quot;levels&quot;) ## [1] &quot;female&quot; &quot;male&quot; The core of xf is not c(&quot;female&quot;, &quot;male&quot;, &quot;male&quot;, &quot;female&quot;), but an integer vector c(1, 2, 2, 1). The level attribute maps the integer to their original meaning: 1 means &quot;female&quot; and 2 means &quot;male&quot;. From a memory perspective, this is appealing, since you just have to save an integer value, rather than the whole string. However, today, character vectors are cached in a hash table, and are already memory efficient. So, the two ways of storing data are equivalent from a memory perspective. That’s why we want to get rid of factors whenever we can! In a linear regression, however, factors are cool. To see how they work, let’s add a new variable, Conf, to our dataset, which is &quot;protestant&quot; if the share of Catholics is below 50, and &quot;catholic&quot; otherwise. We then transform it to a factor. (We can omit the last step, as R will convert all character variable to factors when used in a regression.) library(tidyverse) swiss_plus &lt;- swiss %&gt;% as_tibble() %&gt;% mutate(Conf = if_else(Catholic &gt;= 50, &quot;catholic&quot;, &quot;protestant&quot;)) %&gt;% mutate(Conf = as.factor(Conf)) We have now a categorical variable in our dataset. The usual way to deal with this variable would be to build a dummy variable, which contains 0 for catholic and 1 for protestant locations. Thanks to factors, R will do this automatically for you: summary(lm(Fertility ~ Education + Conf, data = swiss_plus)) ## ## Call: ## lm(formula = Fertility ~ Education + Conf, data = swiss_plus) ## ## Residuals: ## Min 1Q Median 3Q Max ## -17.739 -5.832 -1.953 6.251 15.466 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 83.7551 2.3994 34.907 &lt; 2e-16 *** ## Education -0.8006 0.1355 -5.909 4.59e-07 *** ## Confprotestant -7.8173 2.6512 -2.949 0.0051 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 8.729 on 44 degrees of freedom ## Multiple R-squared: 0.5329, Adjusted R-squared: 0.5117 ## F-statistic: 25.1 on 2 and 44 DF, p-value: 5.332e-08 6.2.3 Exercises Plot the bivariate relationship between Agriculture and Fertility. Perform a bivariate linear regression. Plot both the data and the regression line in a single graph. Also add Education to the regression. How has the coefficient of Agriculture changed? Do you have an explanation for it? Add a dummy variable DCATH that contains 0 if the share of Catholics is below 50, 1 otherwise. Verify that including a factor really leads to the same result. 6.3 Numerical Optimization (optional) R offers a large collection of tools for numerical optimization. The built-in standard function is optim(), which is sufficient in most situations. The syntax of optim() is different from the functions we met so far, in that the second argument of the function is a function itself! Let’s see how it works: Let’s define a simple quadratic function as our objective function: quadratic_function &lt;- function(x){ x^2 } To see how it looks, let’s evaluate it for a sequence of numbers: x &lt;- seq(from = -5, to = 5, by = 0.1) tibble(x, quadratic_function_at_x = quadratic_function(x)) %&gt;% ggplot(mapping = aes(x = x, y = quadratic_function_at_x)) + geom_point() Suppose you want to find the minimum value of this function. Visual inspection already told us that the minimum is at x = 0, where the function evaluates to 0. If you know calculus, you could have derived this result analytically. Often, however, the problem is too complicated, or there is no analytical solution at all. So you want to use numerical optimization instead. A simple way to perform numerical optimization is to evaluate the function over a relevant range of values, and simply pick the lowest. In our example, picking the minimum of y would have given you the correct result. x[which.min(y)] ## [1] -4.9 This was grid-search minimization, which works fine for small problems like this. A problem with grid search is that is quite inefficient. If you know the function is increasing at x = 1 what is the point of checking for a minimum at x = 1.1? The optim function includes several methods that handle this problem in a more efficient way. Usually, for numerical optimization, you need to give the computer a hint where to start, by providing an initial value for each parameter. 0, would be an obvious starting point, but we don’t want to make it too boring, so let’s try something different. op &lt;- optim(4, quadratic_function, method = &quot;Brent&quot;, lower = -100, upper = 100) Here, we were using the “Brent” method, which is suited for the single parameter optimization that we have here. If you have more than one parameter, the default “Nelder-Mead” usually works fine. As we have seen before, complex functions in R often return a complex object. optim returns a list with 5 components, where the first, par, is the one we are mostly interested in: op$par ## [1] -3.552714e-15 For a more complex example, let’s find the solution to the linear regression problem numerically. This is the result we want to replicate: m &lt;- lm(Fertility ~ Education, data = swiss) coef(m) ## (Intercept) Education ## 79.6100585 -0.8623503 The OLS estimator is minimizing the sum of squared residuals. Let us write down our objective function: sum_of_squared_residuals &lt;- function(b){ b0 &lt;- b[1] b1 &lt;- b[2] fitted.values &lt;- b0 + b1 * swiss$Education residuals &lt;- swiss$Fertility - fitted.values squared_residuals &lt;- residuals^2 sum(squared_residuals) } The argument b is a vector of length 2, containing the coefficient for the intercept and for Education. Using the two coefficients and the data, we calculate fitted.values, the value predicted by our model. The difference between the actual values (swiss$Fertility) are the residuals. In order to calculate the sum of squared residuals, we square the residual vector element- wise and sum up the elements, using the sum() function. For every two coefficients, the function returns the sum of squared residuals. E.g., an intercept of 0 and a slope of 1 lead to a pretty high sum of squared residuals: sum_of_squared_residuals(c(0, 1)) ## [1] 183282.9 Using the optim function, we can easily find the coefficients that minimize the function (the initial values are chosen arbitrarily): optim(c(0, 0), sum_of_squared_residuals) ## $par ## [1] 79.6077096 -0.8618092 ## ## $value ## [1] 4015.238 ## ## $counts ## function gradient ## 97 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL Indeed: At a value of 4015.238, The OLS estimator c(79.6077096, -0.8618092) is minimizing the sum of squared residuals! 6.4 Extended Exercises It can be shown analytically that the OLS estimator can be calculated as follows, using matrix algebra: b = (X’X)^(-1)X’y See, e.g., here (the formula is on page 4): http://www.stat.purdue.edu/~jennings/stat514/stat512notes/topic3.pdf where X is the data matrix, possibly including an intercept, and y is a column vector containing the left-hand variable. Let’s consider our simple linear model from above: m &lt;- lm(Fertility ~ Education, data = swiss) coef(m) ## (Intercept) Education ## 79.6100585 -0.8623503 Can you replicate the result of coef(m), using pure matrix algebra? Hints: Construct X first, adding a vector of 1s as an intercept. Make sure X is of class “matrix”, not “data.frame”. It should look like this: # Intercept Education # [1,] 1 12 # [2,] 1 9 # [3,] 1 5 # [4,] 1 7 # [5,] 1 15 This is matrix algebra, so you will need to use matrix multiplication, not the usual element-wise multiplication. Also remember the inverse and the transpose function from above. "],
["regression-and-visualization.html", "7 Regression and Visualization 7.1 The broom package 7.2 texreg and friends 7.3 From data frame to output: kable 7.4 More on linear regression 7.5 Exercises", " 7 Regression and Visualization Download as R script Intro Slides We had a first look at linear regression in the last lesson. Today, we are build on it in two ways. First, we discuss some possibilities to turn regression output into beautiful tables in your RMarkdown report. Second, we explore extensions to the linear regression model, to make it a more powerful and statistically accurate tool to capture linear relationships in data. 7.1 The broom package In the first few lessons of this course, through dplyr and ggplot, we explored powerful tools to manipulate data frames. Because we are getting good at it, we want to use this knowledge to work with regression output (or any model output) as well. The broom package tries to help, by turning any model output into a familiar data frame. For more information on the broom package, check out the vignette broom is an attempt to bridge the gap from untidy outputs of predictions and estimations to the tidy data we want to work with. It centers around three methods, each of which take common objects produced by R statistical functions (lm(), t.test(), nls(), etc) and convert them into a data frame. broom is particularly designed to work with the dplyr package. broom provides three functions that do three distinct kinds of tidying. tidy: constructs a data frame that summarizes the model’s statistical findings. This includes coefficients and p-values for each term in a regression, per-cluster information in clustering applications, or per-test information for multtest functions. augment: add columns to the original data that was modeled. This includes predictions, residuals, and cluster assignments. glance: construct a concise one-row summary of the model. This typically contains values such as R^2, adjusted R^2, and residual standard error that are computed once for the entire model. To see it in action, consider our simple linear regression from last lesson: m &lt;- lm(Fertility ~ Education, data = swiss) We saw that the summary() function prints an overview of the regression: summary(m) ## ## Call: ## lm(formula = Fertility ~ Education, data = swiss) ## ## Residuals: ## Min 1Q Median 3Q Max ## -17.036 -6.711 -1.011 9.526 19.689 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 79.6101 2.1041 37.836 &lt; 2e-16 *** ## Education -0.8624 0.1448 -5.954 3.66e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.446 on 45 degrees of freedom ## Multiple R-squared: 0.4406, Adjusted R-squared: 0.4282 ## F-statistic: 35.45 on 1 and 45 DF, p-value: 3.659e-07 But this is tedious to work with. It is much easier to have the regression output in a data frame. To extract the information on the coefficients, use tidy(): library(broom) tidy(m) ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 79.6 2.10 37.8 9.30e-36 ## 2 Education -0.862 0.145 -5.95 3.66e- 7 augment() returns statistical information on the observations, such as resiudals and predicted values: augment(m) ## # A tibble: 47 x 10 ## .rownames Fertility Education .fitted .se.fit .resid .hat .sigma .cooksd ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Courtela… 80.2 12 69.3 1.39 10.9 0.0215 9.41 1.51e-2 ## 2 Delemont 83.1 9 71.8 1.41 11.3 0.0222 9.40 1.65e-2 ## 3 Franches… 92.5 5 75.3 1.63 17.2 0.0297 9.18 5.23e-2 ## 4 Moutier 85.8 7 73.6 1.49 12.2 0.0250 9.37 2.20e-2 ## 5 Neuvevil… 76.9 15 66.7 1.50 10.2 0.0251 9.42 1.55e-2 ## 6 Porrentr… 76.1 7 73.6 1.49 2.53 0.0250 9.54 9.41e-4 ## 7 Broye 83.8 7 73.6 1.49 10.2 0.0250 9.42 1.54e-2 ## 8 Glane 92.4 8 72.7 1.44 19.7 0.0234 9.07 5.32e-2 ## 9 Gruyere 82.4 7 73.6 1.49 8.83 0.0250 9.46 1.15e-2 ## 10 Sarine 82.9 13 68.4 1.41 14.5 0.0222 9.29 2.74e-2 ## # … with 37 more rows, and 1 more variable: .std.resid &lt;dbl&gt; Finally glance() returns summary statistics that are valid for the regression model: glance(m) ## # A tibble: 1 x 11 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.441 0.428 9.45 35.4 3.66e-7 2 -171. 348. 354. ## # … with 2 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt; 7.2 texreg and friends Publications in Economics usually require regression outputs to be in a specific form. The texreg package gives you professional regression output for publications out-of-the box. (There is an alternative solution, stargazer, but its output has some flaws, especially when used in the PDF mode. E.g., it does not support the LaTeX booktab package.) For a quick demonstration, consider two competing models: m1 &lt;- lm(Fertility ~ Education, data = swiss) m2 &lt;- lm(Fertility ~ Education + Agriculture + Examination, data = swiss) m3 &lt;- lm(Fertility ~ Agriculture + Examination, data = swiss) Wrap the model in a list() and pass them to texreg() (if you want to produces a PDF) or to htmlreg() (if you want to produces a HTML document). library(texreg) ## Version: 1.36.23 ## Date: 2017-03-03 ## Author: Philip Leifeld (University of Glasgow) ## ## Please cite the JSS article in your publications -- see citation(&quot;texreg&quot;). ## ## Attaching package: &#39;texreg&#39; ## The following object is masked from &#39;package:tidyr&#39;: ## ## extract htmlreg(list(m1, m2, m3), doctype = FALSE, star.symbol = &quot;\\\\*&quot;) Statistical models Model 1 Model 2 Model 3 (Intercept) 79.61*** 99.80*** 94.61*** (2.10) (7.16) (7.83) Education -0.86*** -0.67** (0.14) (0.19) Agriculture -0.18* -0.09 (0.08) (0.09) Examination -0.80** -1.20*** (0.25) (0.24) R2 0.44 0.56 0.43 Adj. R2 0.43 0.53 0.41 Num. obs. 47 47 47 RMSE 9.45 8.60 9.62 ***p &lt; 0.001, **p &lt; 0.01, *p &lt; 0.05 Like with any table output, make sure you add the chunk option results = 'asis'. This ensures that the code produced by your function (HTML or TeX) is interpreted as it is, not not simply printed. doctype = FALSE and star.symbol = &quot;\\\\*&quot; are two fixes needed if HTML content is used within RMarkdown. There is also a version than produces a text version of the output, which is useful for quick interactive exploration of the table: screenreg(list(m1, m2, m3)) ## ## ============================================ ## Model 1 Model 2 Model 3 ## -------------------------------------------- ## (Intercept) 79.61 *** 99.80 *** 94.61 *** ## (2.10) (7.16) (7.83) ## Education -0.86 *** -0.67 ** ## (0.14) (0.19) ## Agriculture -0.18 * -0.09 ## (0.08) (0.09) ## Examination -0.80 ** -1.20 *** ## (0.25) (0.24) ## -------------------------------------------- ## R^2 0.44 0.56 0.43 ## Adj. R^2 0.43 0.53 0.41 ## Num. obs. 47 47 47 ## RMSE 9.45 8.60 9.62 ## ============================================ ## *** p &lt; 0.001, ** p &lt; 0.01, * p &lt; 0.05 You can give the models a customized name: htmlreg( list(`(1)` = m1, `(2)` = m2, `(3)` = m3), doctype = FALSE, star.symbol = &quot;\\\\*&quot; ) Statistical models (1) (2) (3) (Intercept) 79.61*** 99.80*** 94.61*** (2.10) (7.16) (7.83) Education -0.86*** -0.67** (0.14) (0.19) Agriculture -0.18* -0.09 (0.08) (0.09) Examination -0.80** -1.20*** (0.25) (0.24) R2 0.44 0.56 0.43 Adj. R2 0.43 0.53 0.41 Num. obs. 47 47 47 RMSE 9.45 8.60 9.62 ***p &lt; 0.001, **p &lt; 0.01, *p &lt; 0.05 texreg shines when it comes to LaTeX tables, which are often used in publications. The following line will produce the LaTeX table below texreg(list(`(1)` = m1, `(2)` = m2, `(3)` = m3), booktabs = TRUE, dcolumn = TRUE) However, because we are using the LaTeX packages booktabs (for nicer lines) and dcolumn (for aligned numbers), we need to load these first and adjust the YAML header as follows: --- output: pdf_document header-includes: - \\usepackage{booktabs} - \\usepackage{dcolumn} --- 7.3 From data frame to output: kable Including any kind of table in a RMarkdown document can be done with the kable package. The kable package is super simple but sufficient in most cases, so there is rearely a need to switch to more extensive package like gt or xtable. Contrary to texreg, kable is smart enough to see if we want to produce a PDF or a HTML document. To produce a simple table, just write kable() around a data frame: library(knitr) kable(iris[1:5,]) Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.0 3.6 1.4 0.2 setosa In a PDF, the output looks like the following: Customization can be done through the kableExtra package but the details differ for HTML and PDF. We won’t cover them here, but both are useful sources for documentation to style your tables according to your needs. 7.4 More on linear regression We had a quick look at the lm() function and linear regressions in R. We will deepen our knowledge by having another look at these models. This time, we put more emphasis on the econometric side of the problem. The following relies heavily on a new a new online book, Econometrics in R. This chapter covers the content of chapters 4 to 7. Contrary to the textbook, we will rely on the tools of the tidyverse, to unfiy the data analysis. We will not cover most of the statistical topics in the book. The interested reader is refered to the book, or to the underlying Econometrics textbook, Introduction to Econometrics (which is not free). 7.4.1 Relationship between class size and test score The book takes a careful look on the impact of class sizes on school test scores. If, for example, a school cuts its class sizes by hiring new teachers, that is, the school lowers the student-teacher ratios of its classes, how would this affect the performance of the students involved in a standardized test? With linear regression we can not only examine whether the student-teacher ratio does have an impact on the test results but we can also learn about the direction and the strength of this effect. The dataset is included in the AER package and can be loaded as follows: library(AER) ## Loading required package: car ## Loading required package: carData ## ## Attaching package: &#39;car&#39; ## The following object is masked from &#39;package:boot&#39;: ## ## logit ## The following object is masked from &#39;package:dplyr&#39;: ## ## recode ## The following object is masked from &#39;package:purrr&#39;: ## ## some ## Loading required package: lmtest ## Loading required package: zoo ## ## Attaching package: &#39;zoo&#39; ## The following objects are masked from &#39;package:base&#39;: ## ## as.Date, as.Date.numeric ## Loading required package: sandwich ## Loading required package: survival ## ## Attaching package: &#39;survival&#39; ## The following object is masked from &#39;package:boot&#39;: ## ## aml data(CASchools) The dataset contains data on test performance, school characteristics and student demographic backgrounds for school districts in California. We will enhance the dataset by defining two new variables, student_teacher_ratio, the student-teacher ratio, and test_score, an average of two underlying test scores: library(tidyverse) caschools &lt;- CASchools %&gt;% as_tibble() %&gt;% mutate(student_teacher_ratio = students / teachers) %&gt;% mutate(test_score = (read + math) / 2) caschools ## # A tibble: 420 x 16 ## district school county grades students teachers calworks lunch computer ## &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 75119 Sunol… Alame… KK-08 195 10.9 0.510 2.04 67 ## 2 61499 Manza… Butte KK-08 240 11.1 15.4 47.9 101 ## 3 61549 Therm… Butte KK-08 1550 82.9 55.0 76.3 169 ## 4 61457 Golde… Butte KK-08 243 14 36.5 77.0 85 ## 5 61523 Paler… Butte KK-08 1335 71.5 33.1 78.4 171 ## 6 62042 Burre… Fresno KK-08 137 6.40 12.3 87.0 25 ## 7 68536 Holt … San J… KK-08 195 10 12.9 94.6 28 ## 8 63834 Vinel… Kern KK-08 888 42.5 18.8 100 66 ## 9 62331 Orang… Fresno KK-08 379 19 32.2 93.1 35 ## 10 67306 Del P… Sacra… KK-06 2247 108 79.0 87.3 0 ## # … with 410 more rows, and 7 more variables: expenditure &lt;dbl&gt;, income &lt;dbl&gt;, ## # english &lt;dbl&gt;, read &lt;dbl&gt;, math &lt;dbl&gt;, student_teacher_ratio &lt;dbl&gt;, ## # test_score &lt;dbl&gt; It is always a good idea to start with a plot. Let us refresh our knowledge of ggplot: caschools %&gt;% ggplot(aes(x = student_teacher_ratio, y = test_score)) + geom_point() + ggtitle(&quot;Test scores are higher in small classes&quot;) The plot shows the scatterplot of all observations on the student-teacher ratio and test score. We see that the points are strongly scattered, and that the variables are negatively correlated. That is, we expect to observe lower test scores in bigger classes. Let’s use lm() to estimate a linear regression model: m &lt;- lm(test_score ~ student_teacher_ratio, data = caschools) summary(m) ## ## Call: ## lm(formula = test_score ~ student_teacher_ratio, data = caschools) ## ## Residuals: ## Min 1Q Median 3Q Max ## -47.727 -14.251 0.483 12.822 48.540 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 698.9329 9.4675 73.825 &lt; 2e-16 *** ## student_teacher_ratio -2.2798 0.4798 -4.751 2.78e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 18.58 on 418 degrees of freedom ## Multiple R-squared: 0.05124, Adjusted R-squared: 0.04897 ## F-statistic: 22.58 on 1 and 418 DF, p-value: 2.783e-06 Thus, the coefficient on the student_teacher_ratio is about -2.3, i.e., a reduced class size by one is associated with a test score increased by 2.3. geom_smooth() allows us to plot a bivariate regression line directly into a ggplot: caschools %&gt;% ggplot(aes(x = student_teacher_ratio, y = test_score)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + ggtitle(&quot;Test scores are higher in small classes&quot;, &quot;with regression line&quot;) 7.4.2 Heteroskedasticity-consistent standard errors In economic contexts, some of the assumptions of the classical regression model are usually violated, and it recommended and sometimes required to use robust standard errors. The textbook discusses the case of heteroskedasticity-consistent standard errors. The quickest way to compute heteroskedasticity-consistent standard errors is to use robust_lm, from the estimatr package. library(estimatr) m_robust &lt;- lm_robust( test_score ~ student_teacher_ratio, se_type = &quot;HC1&quot;, data = caschools ) summary(m_robust) ## ## Call: ## lm_robust(formula = test_score ~ student_teacher_ratio, data = caschools, ## se_type = &quot;HC1&quot;) ## ## Standard error type: HC1 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) CI Lower CI Upper ## (Intercept) 698.93 10.3644 67.436 9.487e-227 678.560 719.306 ## student_teacher_ratio -2.28 0.5195 -4.389 1.447e-05 -3.301 -1.259 ## DF ## (Intercept) 418 ## student_teacher_ratio 418 ## ## Multiple R-squared: 0.05124 , Adjusted R-squared: 0.04897 ## F-statistic: 19.26 on 1 and 418 DF, p-value: 1.447e-05 We can use htmlreg (or screenreg, for interactive use; or texreg) from above to get a clean overview of the two estimations (include.ci = FALSE is needed for a peculiarity of lm_robust): library(texreg) htmlreg( list( `OLS s.e.` = m, `heteroskedasticity-consistent s.e.` = m_robust ), include.ci = FALSE, star.symbol = &quot;\\\\*&quot;, doctype = FALSE, caption = &quot;Robust standard errors are slightly wider.&quot; ) Robust standard errors are slightly wider. OLS s.e. heteroskedasticity-consistent s.e. (Intercept) 698.93*** 698.93*** (9.47) (10.36) student_teacher_ratio -2.28*** -2.28*** (0.48) (0.52) R2 0.05 0.05 Adj. R2 0.05 0.05 Num. obs. 420 420 RMSE 18.58 18.58 ***p &lt; 0.001, **p &lt; 0.01, *p &lt; 0.05 Robust standard errors are often wider, making it less likely that an effect is statistically significant. Whenever possible, use heteroskedasticity-consistent standard errors. 7.4.3 Omitted variable bias The previous analysis of the relationship between test score and class size has a major flaw: we ignored other determinants of the dependent variable (test score) that correlate with the regressor (class size). This might induce an estimation bias. In our example we therefore wrongly estimate the causal effect on test scores of a unit change in the student-teacher ratio, on average. This issue is called omitted variable bias (OVB). Let’s have a look at the following relationship between the percentage of English learners (english) and class size: caschools %&gt;% ggplot(aes(x = student_teacher_ratio, y = english)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + ggtitle( &quot;Larger classes have a larger share of English learners &quot;, &quot;with regression line&quot; ) Since a high share of English learners is likely to have lowering impact on test scores, this suggests that the effect of small classes is overestimated as it captures the effect of having fewer English learners, too. Multiple regression allows us to disentangle the two effects: m_multiple &lt;- lm_robust(test_score ~ student_teacher_ratio + english, se_type = &quot;HC1&quot;, data = caschools) summary(m_robust) ## ## Call: ## lm_robust(formula = test_score ~ student_teacher_ratio, data = caschools, ## se_type = &quot;HC1&quot;) ## ## Standard error type: HC1 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) CI Lower CI Upper ## (Intercept) 698.93 10.3644 67.436 9.487e-227 678.560 719.306 ## student_teacher_ratio -2.28 0.5195 -4.389 1.447e-05 -3.301 -1.259 ## DF ## (Intercept) 418 ## student_teacher_ratio 418 ## ## Multiple R-squared: 0.05124 , Adjusted R-squared: 0.04897 ## F-statistic: 19.26 on 1 and 418 DF, p-value: 1.447e-05 htmlreg( list( m_robust, m_multiple ), include.ci = FALSE, star.symbol = &quot;\\\\*&quot;, doctype = FALSE, caption = &quot;Including the share of English learners decreases the coefficient on class size.&quot; ) Including the share of English learners decreases the coefficient on class size. Model 1 Model 2 (Intercept) 698.93*** 686.03*** (10.36) (8.73) student_teacher_ratio -2.28*** -1.10* (0.52) (0.43) english -0.65*** (0.03) R2 0.05 0.43 Adj. R2 0.05 0.42 Num. obs. 420 420 RMSE 18.58 14.46 ***p &lt; 0.001, **p &lt; 0.01, *p &lt; 0.05 We find that the negative coefficient on student_teacher_ratio is still significant but only half the size. 7.5 Exercises For this exercise, you will work with Boston, which is part of the MASS package. Load the MASS package, check the documentation to get an overview of the variables in Boston. Convert Boston from a data frame to a tibble, called boston. Plot a scatter-plot between the percent of households with low socioeconomic status, lstat, (x-axis) and the median house value of districts medv, (y-axis). Describe your observations. How appropriate is the assumption of linearity? Estimate a simple linear regression model that explains medv by lstat and a constant. Save the model to m_uni. Use heteroskedasticity-consistent standard errors. Regress the median housing value in a district, medv, on the average age of the buildings, age, the per-capita crime rate, crim, the percentage of individuals with low socioeconomic status, lstat, and a constant. Store it as m_multi. Regress medv on all available regressors. Use the formula medv ~ . as a short cut. Store the model as m_full. Use htmlreg() (or screereg(), or texreg() to produce a regression output that is ready for publication. Include m_uni, m_multi and m_full. Check whether the augmented model yields a higher adjusted R2. Why is not meaningful to compare the unadjusted R2? Can we improve the model (in terms of adjusted R2), by dropping a variable? Hint: the glance function makes it easy to directly access the adjusted R2. "],
["transformation-3.html", "8 Transformation 3", " 8 Transformation 3 Intro Slides "],
["random-forests.html", "9 Random Forests 9.1 Slang 9.2 The Boston housing data set, again 9.3 Prediction with OLS 9.4 Decision Trees 9.5 Random Forest 9.6 Classification", " 9 Random Forests Download as R script Intro Slides 9.1 Slang Econometrics ML sample data, to estimate the model training sample estimating a model training a model regression parameters weights regressor, predictor, independent variable, RHS feature estimating relationship between dependent var and regressors supervised learning clustering unsupervised learning discrete response problems classification problems 9.2 The Boston housing data set, again As in a previous exercise, we will work with Boston, which is part of the MASS package. The data set was used in an 1978 article on hedonic price estimation and the measurement for the demand for clean air. For an explanation of variable names, check out the documentation: # `?MASS::Boston` Again, we want to keep the data in a tibble, for convenience: library(tidyverse) boston &lt;- as_tibble(MASS::Boston) %&gt;% mutate(chas = as.logical(chas)) Most predictive modeling is subject to the danger of overfitting. We will discuss the concept later on. This means that a model can perform well in within the sample it was estimated, but terrible, if it is applied to new data. A popular way to control the problem is to estimate (train) the model on a part of the original data only (e.g. 70%), and use the rest to test the model afterwards. To separate the dataset, we first add an ID column: boston_with_id &lt;- boston %&gt;% mutate(id = row_number()) The sample_frac() function allow as to pick a percentage of all rows from the original dataset: boston_train_with_id &lt;- boston_with_id %&gt;% sample_frac(0.7) boston_train &lt;- boston_train_with_id %&gt;% select(-id) This is the datset we will use for estimating our models. anti_join() can be used to retrieve all rows that are not within boston_train, based on the id: boston_test &lt;- boston_with_id %&gt;% anti_join(boston_train_with_id, by = &quot;id&quot;) %&gt;% select(-id) This is the datset we will use for evaluating our models. 9.3 Prediction with OLS We can use the OLS model to perform predictions of the median house value. The OLS curve covers the predicted values of the model. Let’s recapitulate and see, how the median value of owner-occupied homes, medv, can pe predicted by the neighborhood, as measured by the percentage of lower status population, lstat. m_ols &lt;- lm(medv ~ lstat, data = boston_train) boston_test %&gt;% mutate(predict = predict(m_ols, newdata = boston_test)) %&gt;% ggplot(aes(x = lstat, y = medv)) + geom_point() + geom_point(mapping = aes(y = predict), shape = 21) + geom_segment(mapping = aes(xend = lstat, yend = predict)) 9.4 Decision Trees A decision tree is tree-like structure of decisions and their possible consequences. In a decision tree model, the prediction depends on particular value of a variable. For example: library(rpart) m_tree_1 &lt;- rpart(medv ~ lstat, data = boston_train, method = &quot;anova&quot;, maxdepth = 1) m_tree_1 ## n= 354 ## ## node), split, n, deviance, yval ## * denotes terminal node ## ## 1) root 354 29416.060 22.19887 ## 2) lstat&gt;=9.95 215 5017.037 17.15116 * ## 3) lstat&lt; 9.95 139 10447.700 30.00647 * plot(m_tree_1) text(m_tree_1) boston_test %&gt;% mutate(predict = predict(m_tree_1, newdata = boston_test)) %&gt;% ggplot(aes(x = lstat, y = medv)) + geom_point() + geom_point(mapping = aes(y = predict), shape = 21) + geom_segment(mapping = aes(xend = lstat, yend = predict), alpha = 0.2) + geom_vline(aes(xintercept = 9.95)) More interesting trees can be grown if we increase the depth of a tree. Here, we let the tree grow to a depths of 2, which allows for four possible prediction values: m_tree_2 &lt;- rpart(medv ~ lstat, data = boston_train, method = &quot;anova&quot;, maxdepth = 2) m_tree_2 ## n= 354 ## ## node), split, n, deviance, yval ## * denotes terminal node ## ## 1) root 354 29416.060 22.19887 ## 2) lstat&gt;=9.95 215 5017.037 17.15116 ## 4) lstat&gt;=17.98 77 1223.175 13.01818 * ## 5) lstat&lt; 17.98 138 1744.698 19.45725 * ## 3) lstat&lt; 9.95 139 10447.700 30.00647 ## 6) lstat&gt;=4.6 110 5085.212 27.43545 * ## 7) lstat&lt; 4.6 29 1877.350 39.75862 * plot(m_tree_2) text(m_tree_2) boston_test %&gt;% mutate(predict = predict(m_tree_2, newdata = boston_test)) %&gt;% ggplot(aes(x = lstat, y = medv)) + geom_point() + geom_point(mapping = aes(y = predict), shape = 21) + geom_segment(mapping = aes(xend = lstat, yend = predict), alpha = 0.2) + geom_vline(aes(xintercept = 4.295)) + geom_vline(aes(xintercept = 4.295)) + geom_vline(aes(xintercept = 15)) Analogous to multiple regression, we also can grow trees that depend on more than variable. Here, our prediction depends both on the average number of rooms per dwelling, rm and the percentage of lower status population lstat. m_tree_all &lt;- rpart(medv ~ ., data = boston_train, method = &quot;anova&quot;, maxdepth = 2) m_tree_all ## n= 354 ## ## node), split, n, deviance, yval ## * denotes terminal node ## ## 1) root 354 29416.0600 22.19887 ## 2) rm&lt; 6.8375 294 10828.6900 19.32483 ## 4) lstat&gt;=14.4 131 2121.3290 14.69771 * ## 5) lstat&lt; 14.4 163 3648.5010 23.04356 * ## 3) rm&gt;=6.8375 60 4259.3900 36.28167 ## 6) rm&lt; 7.445 42 1470.9160 32.09048 * ## 7) rm&gt;=7.445 18 329.2228 46.06111 * plot(m_tree_all) text(m_tree_all) boston_test %&gt;% mutate(predict = predict(m_tree_all, newdata = boston_test)) %&gt;% ggplot(aes(x = lstat, y = medv, color = rm)) + geom_point() + geom_point(mapping = aes(y = predict), shape = 21) + geom_segment(mapping = aes(xend = lstat, yend = predict), alpha = 0.2) 9.5 Random Forest Random forest takes random subsets of data and train trees on each subset. That is the reason behind the name for this method: random (subsets) forest (trees). When predicting new data from the test set each tree produces its own prediction. Then the predictions from all the trees are combined together (averaged). This is why random forest can approximate linear patterns, even when the single trees, as we saw above, only produce mean values for different ranges of values. The randomForest package offers basic random forest estimation in R: library(randomForest) ## randomForest 4.6-14 ## Type rfNews() to see new features/changes/bug fixes. ## ## Attaching package: &#39;randomForest&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## combine ## The following object is masked from &#39;package:ggplot2&#39;: ## ## margin Because random forests are based on random sampling, setting the seed is needed to make the estimation reproducible: set.seed(0) Let’s start with a simple example: m_forest &lt;- randomForest(medv ~ lstat, data = boston_train) plot(m_forest) boston_test %&gt;% mutate(predict = predict(m_forest, newdata = boston_test)) %&gt;% ggplot(aes(x = lstat, y = medv)) + geom_point() + geom_point(mapping = aes(y = predict), shape = 21) + geom_segment(mapping = aes(xend = lstat, yend = predict), alpha = 0.2) We now have a bunch of possible forecasts. Which one should you use. The Root Mean Squared Error, while not the only possible measure, is usually a good starting point. It’s main characteristics are the following: Squared - so negative values become positive (deviation is deviation) Squared - so large deviations are extra-penalized Mean - influence of all errors are summarized with one number Root - to go back to original scale after squaring. boston_test %&gt;% summarize( rmse_forest = sqrt(mean((medv - predict(m_forest, newdata = boston_test))^2)), rmse_tree_1 = sqrt(mean((medv - predict(m_tree_1, newdata = boston_test))^2)), rmse_ols = sqrt(mean((medv - predict(m_ols, newdata = boston_test))^2)) ) ## # A tibble: 1 x 3 ## rmse_forest rmse_tree_1 rmse_ols ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 5.83 7.49 6.67 9.5.1 Using more than one variable library(randomForest) m_forest_all &lt;- randomForest(medv ~ ., data = boston_train) boston_test %&gt;% mutate(predict = predict(m_forest_all, newdata = boston_test)) %&gt;% ggplot(aes(x = lstat, y = medv)) + geom_point() + geom_point(mapping = aes(y = predict), shape = 21) + geom_segment(mapping = aes(xend = lstat, yend = predict), alpha = 0.2) m_ols_all &lt;- lm(medv ~ ., data = boston_train) summary(m_ols_all) ## ## Call: ## lm(formula = medv ~ ., data = boston_train) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.8726 -2.6912 -0.5077 1.7630 25.3839 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 35.997880 5.788580 6.219 1.47e-09 *** ## crim -0.035568 0.055400 -0.642 0.521290 ## zn 0.036757 0.016954 2.168 0.030852 * ## indus -0.055376 0.067473 -0.821 0.412380 ## chasTRUE 2.785284 1.015855 2.742 0.006433 ** ## nox -15.374447 4.255582 -3.613 0.000349 *** ## rm 3.869644 0.486694 7.951 2.76e-14 *** ## age -0.003304 0.015444 -0.214 0.830717 ## dis -1.496414 0.241473 -6.197 1.66e-09 *** ## rad 0.247217 0.079219 3.121 0.001959 ** ## tax -0.011103 0.004327 -2.566 0.010726 * ## ptratio -0.955571 0.143562 -6.656 1.13e-10 *** ## black 0.009634 0.003033 3.176 0.001627 ** ## lstat -0.533832 0.058511 -9.124 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.51 on 340 degrees of freedom ## Multiple R-squared: 0.7649, Adjusted R-squared: 0.7559 ## F-statistic: 85.1 on 13 and 340 DF, p-value: &lt; 2.2e-16 boston_test %&gt;% mutate(predict = predict(m_ols_all, newdata = boston_test)) %&gt;% ggplot(aes(x = lstat, y = medv)) + geom_point() + geom_point(mapping = aes(y = predict), shape = 21) + geom_segment(mapping = aes(xend = lstat, yend = predict), alpha = 0.2) boston_test %&gt;% summarize( rmse_forest = sqrt(sum((medv - predict(m_forest, newdata = boston_test))^2)), rmse_tree_1 = sqrt(sum((medv - predict(m_tree_1, newdata = boston_test))^2)), rmse_ols = sqrt(sum((medv - predict(m_ols, newdata = boston_test))^2)), rmse_ols_all = sqrt(sum((medv - predict(m_ols_all, newdata = boston_test))^2)), rmse_forest_all = sqrt(sum((medv - predict(m_forest_all, newdata = boston_test))^2)) ) ## # A tibble: 1 x 5 ## rmse_forest rmse_tree_1 rmse_ols rmse_ols_all rmse_forest_all ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 71.8 92.3 82.2 65.8 46.5 If the interactions between features are mostly linear (as modeled by linear regression) then random forest will not beat linear regression no matter how much data it will have. However if there are interactions in the data the linear model will require for you to list them in the formula (y ~ x1 * x2) and random forest would be able to find them on its own. 9.5.2 Variable importance So we have used OLS, decision trees and random forest to explain the values of medv. But which independent variables are the most important? We have many variables, and would like to rank them by importance. How can we do that? For OLS, the importance can be obtained by the p-value. According to this, lstat and rm are the most important variables: broom::tidy(m_ols_all) %&gt;% arrange(p.value) ## # A tibble: 14 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 lstat -0.534 0.0585 -9.12 6.54e-18 ## 2 rm 3.87 0.487 7.95 2.76e-14 ## 3 ptratio -0.956 0.144 -6.66 1.13e-10 ## 4 (Intercept) 36.0 5.79 6.22 1.47e- 9 ## 5 dis -1.50 0.241 -6.20 1.66e- 9 ## 6 nox -15.4 4.26 -3.61 3.49e- 4 ## 7 black 0.00963 0.00303 3.18 1.63e- 3 ## 8 rad 0.247 0.0792 3.12 1.96e- 3 ## 9 chasTRUE 2.79 1.02 2.74 6.43e- 3 ## 10 tax -0.0111 0.00433 -2.57 1.07e- 2 ## 11 zn 0.0368 0.0170 2.17 3.09e- 2 ## 12 indus -0.0554 0.0675 -0.821 4.12e- 1 ## 13 crim -0.0356 0.0554 -0.642 5.21e- 1 ## 14 age -0.00330 0.0154 -0.214 8.31e- 1 For single trees, the values that split the data at the highest level should be more important. Again, lstat and rm are the most important variables: m_tree_all ## n= 354 ## ## node), split, n, deviance, yval ## * denotes terminal node ## ## 1) root 354 29416.0600 22.19887 ## 2) rm&lt; 6.8375 294 10828.6900 19.32483 ## 4) lstat&gt;=14.4 131 2121.3290 14.69771 * ## 5) lstat&lt; 14.4 163 3648.5010 23.04356 * ## 3) rm&gt;=6.8375 60 4259.3900 36.28167 ## 6) rm&lt; 7.445 42 1470.9160 32.09048 * ## 7) rm&gt;=7.445 18 329.2228 46.06111 * For random forest, there are several different measures of variable importance. We will focus on one of them, the decrease in accuracy. It measures the percentage of accuracy decrease if one variable is omitted. To calculate this measure of importance - we need to add importance = TRUE to our model call: m_forest_all &lt;- randomForest(medv ~ ., data = boston_train, importance = TRUE) varImpPlot() gives a convenient overview of variable importance, type = 1 selects the measure of importance, here, the mean decrease in accuracy after permutation: varImpPlot(m_forest_all, type = 1) Again lstat and rm are the most important variables, but rm seems to be substantially more important than lstat. 9.5.3 Exercises Back to the CASchools dataset on test performance, school characteristics and student demographic backgrounds for school districts in California. As before, we will enhance the dataset by defining two new variables, student_teacher_ratio, the student-teacher ratio, and test_score, an average of two underlying test scores: library(AER) data(CASchools) caschools &lt;- CASchools %&gt;% as_tibble() %&gt;% mutate(student_teacher_ratio = students / teachers) %&gt;% mutate(test_score = (read + math) / 2) %&gt;% select(-read, -math, -students, -teachers, -district, -school, -county, -grades) caschools ## # A tibble: 420 x 8 ## calworks lunch computer expenditure income english student_teacher… ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.510 2.04 67 6385. 22.7 0 17.9 ## 2 15.4 47.9 101 5099. 9.82 4.58 21.5 ## 3 55.0 76.3 169 5502. 8.98 30.0 18.7 ## 4 36.5 77.0 85 7102. 8.98 0 17.4 ## 5 33.1 78.4 171 5236. 9.08 13.9 18.7 ## 6 12.3 87.0 25 5580. 10.4 12.4 21.4 ## 7 12.9 94.6 28 5253. 6.58 68.7 19.5 ## 8 18.8 100 66 4566. 8.17 47.0 20.9 ## 9 32.2 93.1 35 5356. 7.39 30.1 19.9 ## 10 79.0 87.3 0 5036. 11.6 40.3 20.8 ## # … with 410 more rows, and 1 more variable: test_score &lt;dbl&gt; Separate the data set into a training and a test set. Make sure the training set contains 75% of the available observations. Build a decision tree to predict student_teacher_ratio, using all variables in caschools_train. Use the defaults of rpart. Draw the resulting decision tree. Store the model as m_tree. From the documentation, ?rpart, can you figure out how the depth of a tree is determined? Which one is the most important variable? Estimate an OLS model to predict student_teacher_ratio, using all variables in caschools_train. Which one is the most important variable? Store the model as m_ols. grow a random forest to predict student_teacher_ratio, using all variables in caschools_train. Use the defaults of randomForest. Store the model as m_forest. Plot the variable importance for m_forest. Which one is the most imporant? Using the test data, can you compute RMSE measures for m_ols, m_tree, and m_forest? Which performs best? 9.6 Classification This data set provides information on the fate of passengers on the fatal maiden voyage of the ocean liner Titanic, summarized according to economic status (class), sex, age and survival. Titanic dataset is classic example in machine learning, and can be found in the titanic package. # install.packages(&quot;titanic&quot;) # make sure it is installed. head(titanic::titanic_train) ## PassengerId Survived Pclass ## 1 1 0 3 ## 2 2 1 1 ## 3 3 1 3 ## 4 4 1 1 ## 5 5 0 3 ## 6 6 0 3 ## Name Sex Age SibSp Parch ## 1 Braund, Mr. Owen Harris male 22 1 0 ## 2 Cumings, Mrs. John Bradley (Florence Briggs Thayer) female 38 1 0 ## 3 Heikkinen, Miss. Laina female 26 0 0 ## 4 Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35 1 0 ## 5 Allen, Mr. William Henry male 35 0 0 ## 6 Moran, Mr. James male NA 0 0 ## Ticket Fare Cabin Embarked ## 1 A/5 21171 7.2500 S ## 2 PC 17599 71.2833 C85 C ## 3 STON/O2. 3101282 7.9250 S ## 4 113803 53.1000 C123 S ## 5 373450 8.0500 S ## 6 330877 8.4583 Q The goal of the exercise it to predict survival. The datasets come divided into training and testing set. However, the testing part does not have survival information and is not useful that way. In the following, we will limit ourself to the titanic::titanic_train. Categorical variables will be a problem in random forest, so we will convert them into factors. We also clean up the data and remove missing values. The cleaned dataset looks as follows: titanic &lt;- as_tibble(na.omit(titanic::titanic_train)) %&gt;% select(-Name, -PassengerId, -Cabin, -Ticket, -Embarked) %&gt;% mutate(Sex = as_factor(Sex)) %&gt;% mutate(Survived = as_factor(Survived)) Survived Passenger Survival Indicator Pclass Passenger Class Sex Sex Age Age SibSp Number of Siblings/Spouses Aboard Parch Number of Parents/Children Aboard Again, the usual separation in test and training data set. set.seed(0) titanic_with_id &lt;- titanic %&gt;% mutate(id = row_number()) titanic_train_with_id &lt;- titanic_with_id %&gt;% sample_frac(0.70) titanic_train &lt;- titanic_train_with_id %&gt;% select(-id) titanic_test &lt;- titanic_with_id %&gt;% anti_join(titanic_train_with_id, by = &quot;id&quot;) %&gt;% select(-id) To start, let’s build a decision tree: m_titanic_tree &lt;- rpart(Survived ~ ., data = titanic_train) plot(m_titanic_tree) text(m_titanic_tree, all = TRUE) m_titanic_tree ## n= 500 ## ## node), split, n, loss, yval, (yprob) ## * denotes terminal node ## ## 1) root 500 203 0 (0.59400000 0.40600000) ## 2) Sex=male 319 62 0 (0.80564263 0.19435737) ## 4) Pclass&gt;=1.5 248 35 0 (0.85887097 0.14112903) ## 8) Age&gt;=9.5 227 25 0 (0.88986784 0.11013216) * ## 9) Age&lt; 9.5 21 10 0 (0.52380952 0.47619048) ## 18) SibSp&gt;=2.5 11 0 0 (1.00000000 0.00000000) * ## 19) SibSp&lt; 2.5 10 0 1 (0.00000000 1.00000000) * ## 5) Pclass&lt; 1.5 71 27 0 (0.61971831 0.38028169) ## 10) Age&gt;=36.5 45 12 0 (0.73333333 0.26666667) * ## 11) Age&lt; 36.5 26 11 1 (0.42307692 0.57692308) ## 22) Fare&gt;=61.8 11 4 0 (0.63636364 0.36363636) * ## 23) Fare&lt; 61.8 15 4 1 (0.26666667 0.73333333) * ## 3) Sex=female 181 40 1 (0.22099448 0.77900552) ## 6) Pclass&gt;=2.5 65 31 0 (0.52307692 0.47692308) ## 12) Fare&gt;=20.8 15 2 0 (0.86666667 0.13333333) * ## 13) Fare&lt; 20.8 50 21 1 (0.42000000 0.58000000) ## 26) Age&gt;=17.5 36 17 0 (0.52777778 0.47222222) ## 52) Fare&lt; 15 29 12 0 (0.58620690 0.41379310) ## 104) Fare&gt;=8.29375 16 4 0 (0.75000000 0.25000000) * ## 105) Fare&lt; 8.29375 13 5 1 (0.38461538 0.61538462) * ## 53) Fare&gt;=15 7 2 1 (0.28571429 0.71428571) * ## 27) Age&lt; 17.5 14 2 1 (0.14285714 0.85714286) * ## 7) Pclass&lt; 2.5 116 6 1 (0.05172414 0.94827586) * This has a very simple interpretation: If you are on the Titanic, the best guess is that you will die. If you are male, things look bad, unless you are a child. If you are female, things look better, especially if you travel first class. So it is not very hard to guess the end of the movie Titanic. Switching to random forest, the output now includes the a confusion matrix. The confusion matrix shows, based on out-of-bag evaluation, which classification has been done correctly and which has not. 270 deaths has been predicted correctly, 151 survivals have been predicted correctly. On the other hand, 27 dying passengers have been incorrectly predicted to survive, while 52 surviving passengers have been incorrectly predicted to to die. set.seed(0) m_titanic_forest &lt;- randomForest(Survived ~ ., data = titanic_train, importance = TRUE) m_titanic_forest ## ## Call: ## randomForest(formula = Survived ~ ., data = titanic_train, importance = TRUE) ## Type of random forest: classification ## Number of trees: 500 ## No. of variables tried at each split: 2 ## ## OOB estimate of error rate: 15.8% ## Confusion matrix: ## 0 1 class.error ## 0 270 27 0.09090909 ## 1 52 151 0.25615764 The out-of-bag forecast error are usually a good way to quickly assess the forecast accuracy of a random forest model. However, if we want to run the model on our test data, compute the confusion table as follows: tibble( predicted = predict(m_titanic_forest, newdata = titanic_test), actual = titanic_test$Survived ) %&gt;% group_by(predicted, actual) %&gt;% summarize(count = n()) ## # A tibble: 4 x 3 ## # Groups: predicted [2] ## predicted actual count ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; ## 1 0 0 107 ## 2 0 1 26 ## 3 1 0 20 ## 4 1 1 61 The corresponding out-of-bag confusion table as is as follows: tibble( predicted = predict(m_titanic_forest), actual = titanic_train$Survived ) %&gt;% group_by(predicted, actual) %&gt;% summarize(count = n()) ## # A tibble: 4 x 3 ## # Groups: predicted [2] ## predicted actual count ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; ## 1 0 0 270 ## 2 0 1 52 ## 3 1 0 27 ## 4 1 1 151 Finally, which variables are the important one? varImpPlot(m_titanic_forest, type = 1) "],
["time-series.html", "10 Time Series 10.1 Time Series Objects 10.2 Economic Time Series 10.3 Financial Time Series 10.4 Automatic ARIMA Forecasting with the ‘forecast’ package 10.5 Seasonal Adjustment 10.6 Forecasting daily time series 10.7 Exercises 10.8 From ts to tibble: The tsbox package 10.9 Exercises", " 10 Time Series Download as R script Intro Slides We will cover the basic handling of time series in R. We will meet quarterly and monthly time series as well as daily time series, and check out ways to forecast them. We will also cover the tsbox package, which allows you to convert between various time series classes. 10.1 Time Series Objects R offers a build-in object class that facilitates working with time series data. Like matrices, time series objects (or ts objects) are basically a vector with an additional attribute. In order to create a ts object, use the ts function: simple_ts &lt;- ts(c(4, 2, 1, 2, 3, 4, 4, 3, 5), start = c(2015, 4), frequency = 4) class(simple_ts) ## [1] &quot;ts&quot; We will cover the tsbox package in a second. Among other, it gives you a convenient way to plot all kind of time series: library(tsbox) ts_plot(simple_ts) The first argument is the data, a vector. The start argument describes the beginning of the time series. It is usually a vector of length 2; first the year, then the higher frequency period – the quarter, the month, the day. The frequency argument describes the frequency. It is 4 in our case, indicating quarterly data, but may be any other integer value. There are many build-in example time series R. A popular is AirPassengers, which shows monthly totals of international airline passengers, from 1949 to 1960. AirPassengers ## Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec ## 1949 112 118 132 129 121 135 148 148 136 119 104 118 ## 1950 115 126 141 135 125 149 170 170 158 133 114 140 ## 1951 145 150 178 163 172 178 199 199 184 162 146 166 ## 1952 171 180 193 181 183 218 230 242 209 191 172 194 ## 1953 196 196 236 235 229 243 264 272 237 211 180 201 ## 1954 204 188 235 227 234 264 302 293 259 229 203 229 ## 1955 242 233 267 269 270 315 364 347 312 274 237 278 ## 1956 284 277 317 313 318 374 413 405 355 306 271 306 ## 1957 315 301 356 348 355 422 465 467 404 347 305 336 ## 1958 340 318 362 348 363 435 491 505 404 359 310 337 ## 1959 360 342 406 396 420 472 548 559 463 407 362 405 ## 1960 417 391 419 461 472 535 622 606 508 461 390 432 ts_plot(AirPassengers) 10.2 Economic Time Series More often than constructing your own time series, you will get time series from other sources, e.g., from spreadsheets, from databases, or from the Internet. There is a vast number of on-line data sources for time series, as well as a plethora of R packages to access them. For economic data, the FRED database is one of the best free on-line data source. However, it needs (free) registration and an API key, so we will use a different package in the following. A partial list with packages can be found [here](https://cengel.github.io/gearup2016/SULdataAccess.html(). Another good website for economic data is dbnomics. # install.packages(&quot;rdbnomics&quot;) library(tidyverse) library(rdbnomics) ## Visit &lt;https://db.nomics.world&gt;. swiss_gdp &lt;- rdb(ids = c( &quot;Eurostat/namq_10_gdp/Q.CLV05_MEUR.NSA.B1G.CH&quot; )) %&gt;% select(period, value) %&gt;% ts_ts() ## [time]: &#39;period&#39; ts_plot(swiss_gdp) 10.3 Financial Time Series For financial data, a popular package is quantmod, which allows you to access multiple data sources. Let’s get Apple stock data; Apple’s ticker symbol is AAPL. We use the quantmod function getSymbols, and pass a string as a first argument to identify the desired ticker symbol, pass 'yahoo' to src for Yahoo! Finance. Always use auto.assign = TRUE in getSymbols(). The default behavior is confusing and should be avoided. library(quantmod) ## Loading required package: xts ## Registered S3 method overwritten by &#39;xts&#39;: ## method from ## as.zoo.xts zoo ## ## Attaching package: &#39;xts&#39; ## The following objects are masked from &#39;package:dplyr&#39;: ## ## first, last ## Loading required package: TTR ## Registered S3 method overwritten by &#39;quantmod&#39;: ## method from ## as.zoo.data.frame zoo ## Version 0.4-0 included new data defaults. See ?getSymbols. apple &lt;- getSymbols(&quot;AAPL&quot;, src = &quot;yahoo&quot;, auto.assign = FALSE) %&gt;% ts_tbl() %&gt;% filter(id == &quot;AAPL.Close&quot;) ## &#39;getSymbols&#39; currently uses auto.assign=TRUE by default, but will ## use auto.assign=FALSE in 0.5-0. You will still be able to use ## &#39;loadSymbols&#39; to automatically load data. getOption(&quot;getSymbols.env&quot;) ## and getOption(&quot;getSymbols.auto.assign&quot;) will still be checked for ## alternate defaults. ## ## This message is shown once per session and may be disabled by setting ## options(&quot;getSymbols.warning4.0&quot;=FALSE). See ?getSymbols for details. tsbox::ts_tbl converts the output of getSymbols to a tibble data frame, which is much more useful. We will cover it later on. We use filter to focus on closing prices. ts_plot(apple) This data covers business days only, so it is not regular. ts objects are not suitable to store data like this. Usually, data frames are a good alternative, but there are many other time series classes that deal with daily time series in R. We will meet some of them later on. 10.4 Automatic ARIMA Forecasting with the ‘forecast’ package The forecast package offers many useful functions for working with regular time-series, such as swiss_gdp. We will just cover a single function that allows you to automatically build an ARIMA model. For more information on ARIMA based time series forecasting in R, please refer to the free online book, Forecasting: principles and practice, by Rob Hyndman: https://otexts.com/fpp2/ library(forecast) ## Registered S3 methods overwritten by &#39;forecast&#39;: ## method from ## fitted.fracdiff fracdiff ## residuals.fracdiff fracdiff If you don’t know much about a time series, but want to produce a simple forecast without external indicators, the auto.arima function is your friend. Let’s say we want to know what will happen to AirPassengers in 1961. m_arima &lt;- auto.arima(AirPassengers) m_arima ## Series: AirPassengers ## ARIMA(2,1,1)(0,1,0)[12] ## ## Coefficients: ## ar1 ar2 ma1 ## 0.5960 0.2143 -0.9819 ## s.e. 0.0888 0.0880 0.0292 ## ## sigma^2 estimated as 132.3: log likelihood=-504.92 ## AIC=1017.85 AICc=1018.17 BIC=1029.35 broom::tidy(m_arima) ## # A tibble: 3 x 3 ## term estimate std.error ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 ar1 0.596 0.0888 ## 2 ar2 0.214 0.0880 ## 3 ma1 -0.982 0.0292 As the ‘auto’ indicates, the function does everything on its own. First, it made the series stationary, by taking first differences and seasonal differences. Second, on the stationary series, it evaluates what it finds to be the best ARMA model. AR and MA models are just special cases of ARMA models. With these details you could also have fed the built-in arima function, to verify the result of the estimation: arima(AirPassengers, order = c(2, 1, 1), seasonal = c(0, 1, 0)) ## ## Call: ## arima(x = AirPassengers, order = c(2, 1, 1), seasonal = c(0, 1, 0)) ## ## Coefficients: ## ar1 ar2 ma1 ## 0.5960 0.2143 -0.9819 ## s.e. 0.0888 0.0880 0.0292 ## ## sigma^2 estimated as 129.3: log likelihood = -504.92, aic = 1017.85 In order to produce a forecast, the forecast package has a function forecast(), which can be applied on the output of auto.arima(). The h argument indicates the number of periods to be forecasted: fct_arima &lt;- forecast(m_arima, h = 12) plot(fct_arima) In order to access the point forecast, you can access the mean component of the forecast() output: fct_arima$mean ## Jan Feb Mar Apr May Jun Jul Aug ## 1961 445.6349 420.3950 449.1983 491.8399 503.3944 566.8624 654.2601 638.5974 ## Sep Oct Nov Dec ## 1961 540.8837 494.1266 423.3327 465.5075 10.5 Seasonal Adjustment Many time series exhibit a regular seasonal pattern over the year. US unemployment, for example, is usually higher from January to March, and again in June and July. Similarly, retail sales tend to peak with the Christmas season. To model the underlying structure of these series, any regular (seasonal) patterns are estimated and removed from the data. For example, to see if the economy is moving out of a recession during certain months, one wants the labor market data to be free from such seasonal effects. Seasonal adjustment decomposes a time series into a trend, a seasonal and an irregular component and removes the seasonal component from the data. The seasonal packages gives access to X-13ARIMA-SEATS, the seasonal adjustment software from the U.S. Census bureau. It is the standard software for seasonal adjustment of regular time series and takes care of seasonal effects, Easter effects and trading day adjustment. Basic usage is very simple and offers a good seasonal adjustment in most cases: library(seasonal) ## ## Attaching package: &#39;seasonal&#39; ## The following object is masked from &#39;package:randomForest&#39;: ## ## outlier ## The following object is masked from &#39;package:tibble&#39;: ## ## view m_seas &lt;- seas(AirPassengers) plot(m_seas) ts_plot(final(m_seas)) 10.6 Forecasting daily time series While auto.arima() and seas() work well with regular data, they are not suitable for daily data. An easy to use forecasting tool for daily (or even hourly) data is Facebook’s R package prophet. Note that such a forecast is purely based on historical information, and may have close to zero value for for financial data. Let’s try it nevertheless. # install.packages(&quot;prophet&quot;) library(prophet) ## Loading required package: Rcpp ## Loading required package: rlang ## ## Attaching package: &#39;rlang&#39; ## The following objects are masked from &#39;package:purrr&#39;: ## ## %@%, as_function, flatten, flatten_chr, flatten_dbl, flatten_int, ## flatten_lgl, flatten_raw, invoke, list_along, modify, prepend, ## splice We call the prophet function to fit the model. The first argument is the historical data frame. It must have columns ds, for the time stamp, and y, the value of the series. df &lt;- apple %&gt;% select(ds = time, y = value) m_prophet &lt;- prophet(df) ## Disabling daily seasonality. Run prophet with daily.seasonality=TRUE to override this. prophet checks for daily, weekly, and yearly seasonality, as well as for holiday effects. Because our data is daily, there is no daily seasonality. This would only appear, e.g., in hourly data. Predictions are made on a data frame with a column ds containing the dates for which predictions are to be made. make_future_dataframe() takes the model object and a number of periods to forecast and produces a suitable data frame. df_future &lt;- make_future_dataframe(m_prophet, periods = 365) tail(df_future) ## ds ## 3624 2020-12-12 ## 3625 2020-12-13 ## 3626 2020-12-14 ## 3627 2020-12-15 ## 3628 2020-12-16 ## 3629 2020-12-17 As with most modeling procedures in R, we use the generic predict function to get our forecast. The forecast object is a data frame with a column yhat containing the forecast. It has additional columns for uncertainty intervals and seasonal components. fct_prophet &lt;- predict(m_prophet, df_future) %&gt;% as_tibble() fct_prophet %&gt;% select(ds, yhat) %&gt;% tail() ## # A tibble: 6 x 2 ## ds yhat ## &lt;dttm&gt; &lt;dbl&gt; ## 1 2020-12-12 00:00:00 255. ## 2 2020-12-13 00:00:00 255. ## 3 2020-12-14 00:00:00 256. ## 4 2020-12-15 00:00:00 256. ## 5 2020-12-16 00:00:00 256. ## 6 2020-12-17 00:00:00 256. You can use the generic plot function to plot the forecast, by passing in the model and the forecast data frame. plot(m_prophet, fct_prophet) You can use `prophet_plot_components() to see the forecast broken down into trend, weekly seasonality, and yearly seasonality: prophet_plot_components(m_prophet, fct_prophet) 10.7 Exercises Estimate an automated ARIMA model of Swiss GDP. Inspect the model: What is the degree of seasonal and non-seasonal differentiation, what are the seasonal and non-seasonal AR and MA orders? Re-estimate 1, using R base arima() function Seasonally adjust Swiss GDP. Compare the ARIMA model with the one from auto.arima. Download your stock title of choice. Use prophet to forecast the series. 10.8 From ts to tibble: The tsbox package The R ecosystem knows a vast number of time series classes: ts, xts, zoo, tsibble, tibbletime or timeSeries. The plethora of standards causes confusion. As different packages rely on different classes, it is hard to use them in the same analysis. tsbox provides a set of tools that make it easy to switch between these classes. It also allows the user to treat time series as plain data frames, facilitating the use with tools that assume rectangular data. It has it’s own website. tsbox is built around a set of converters, which convert time series stored as ts, xts, data.frame, data.table, tibble, zoo, tsibble, tibbletime or timeSeries to each other. Because this works reliably, we can easily write functions that work for all classes. So whether we want to smooth, scale, differentiate, chain, forecast, regularize or seasonally adjust a time series, we can use the same commands to whatever time series class at hand. And, most conveniently, we get a time series plot function that works for all classes and frequencies. 10.8.1 Convert everything to everything tsbox can convert time series stored as ts, xts, data.frame, data.table, tibble, zoo, tsibble, tibbletime or timeSeries to each other: library(tsbox) x_ts &lt;- ts_c(fdeaths, mdeaths) x_xts &lt;- ts_xts(x_ts) x_df &lt;- ts_df(x_xts) x_dt &lt;- ts_df(x_df) x_tbl &lt;- ts_tbl(x_dt) # and more exotic stuff x_zoo &lt;- ts_zoo(x_tbl) x_tsibble &lt;- ts_tsibble(x_zoo) ## Loading required namespace: tsibble all.equal(ts_ts(x_tsibble), x_ts) ## [1] TRUE 10.8.2 Use same functions for all time series classes tsbox provides a basic toolkit for handling time series. These functions start with ts_, so you use them with auto complete (press Tab). These functions work with any ts-boxable time series, ts, xts, data.frame, data.table tibble, zoo, tsibble or timeSeries and return the class of their inputs. For example, the ts_scale function performs normalization - it subtracts the mean and divides by the standard deviation of series. Like almost all ts- functions, it can be used with on any ts-boxable object, with single or multiple time series. Because ts_scale normalizes time series, it is useful to make different time series comparable. All of the following operations perform the same task, but return the same object class as the input: x_scale &lt;- ts_scale(x_ts) ts_scale(x_xts) ## fdeaths mdeaths ## 1974-01-01 1.89360883 1.473056003 ## 1974-02-01 0.71399633 0.847408011 ## 1974-03-01 1.48185729 0.879729310 ## 1974-04-01 0.64722581 0.879729310 ## 1974-05-01 -0.21522673 -0.009106398 ## 1974-06-01 -0.86067508 -0.570111792 ## 1974-07-01 -0.66592773 -0.498543203 ## 1974-08-01 -0.93300981 -0.842534165 ## 1974-09-01 -0.96639507 -0.662458359 ## 1974-10-01 0.11862586 -0.009106398 ## 1974-11-01 0.09636902 0.288711281 ## 1974-12-01 0.58601950 0.808160720 ## 1975-01-01 1.49854992 1.401487414 ## 1975-02-01 1.06454155 1.479981995 ## 1975-03-01 1.24816048 1.516920622 ## 1975-04-01 0.57489108 0.778148086 ## 1975-05-01 -0.52125828 -0.214577509 ## 1975-06-01 -0.68262036 -0.480073889 ## 1975-07-01 -0.77721193 -0.715557635 ## 1975-08-01 -0.82728982 -0.837916837 ## 1975-09-01 -1.21122031 -1.022609971 ## 1975-10-01 -0.67149194 -0.343862703 ## 1975-11-01 -0.16514884 0.113252804 ## 1975-12-01 1.17026154 1.316066839 ## 1976-01-01 1.14800470 1.209868287 ## 1976-02-01 3.22901921 2.895193135 ## 1976-03-01 1.86578778 1.817046965 ## 1976-04-01 -0.15958463 -0.039119032 ## 1976-05-01 -0.63254247 -0.708631643 ## 1976-06-01 -0.78277614 -0.775582904 ## 1976-07-01 -1.02760138 -0.884090120 ## 1976-08-01 -1.28355504 -1.214229097 ## 1976-09-01 -1.13332137 -1.147277836 ## 1976-10-01 -0.64367089 -0.664767023 ## 1976-11-01 -0.08168569 -0.066823002 ## 1976-12-01 1.13131207 1.299906190 ## 1977-01-01 1.67660464 1.717774406 ## 1977-02-01 0.55263424 0.318723915 ## 1977-03-01 0.56932687 0.521886363 ## 1977-04-01 0.45804267 0.704270833 ## 1977-05-01 -0.32651093 -0.577037785 ## 1977-06-01 -0.93857402 -0.770965575 ## 1977-07-01 -0.83285403 -0.944115389 ## 1977-08-01 -1.18339926 -1.114956538 ## 1977-09-01 -0.96639507 -1.239624403 ## 1977-10-01 -0.97752349 -0.731718284 ## 1977-11-01 -0.83285403 -0.616285076 ## 1977-12-01 0.43022162 0.367205863 ## 1978-01-01 1.30936679 1.207559623 ## 1978-02-01 1.62652675 1.819355629 ## 1978-03-01 0.98107840 1.029792481 ## 1978-04-01 -0.08168569 -0.168404226 ## 1978-05-01 -0.17071305 -0.360023352 ## 1978-06-01 -0.63810668 -0.713248971 ## 1978-07-01 -0.72156983 -0.918720083 ## 1978-08-01 -1.10550032 -1.135734515 ## 1978-09-01 -0.96639507 -1.214229097 ## 1978-10-01 -0.72713404 -0.821756187 ## 1978-11-01 -0.75495509 -0.891016113 ## 1978-12-01 0.65835423 0.729666139 ## 1979-01-01 1.44847204 1.770873682 ## 1979-02-01 1.24816048 0.748135452 ## 1979-03-01 0.92543630 0.808160720 ## 1979-04-01 0.28555216 0.080931505 ## 1979-05-01 -0.46005197 -0.648606374 ## 1979-06-01 -0.73269825 -0.971819359 ## 1979-07-01 -0.86623929 -1.015683978 ## 1979-08-01 -1.01090875 -1.202685776 ## 1979-09-01 -0.93300981 -1.283489022 ## 1979-10-01 -0.83285403 -0.957967374 ## 1979-11-01 -0.40997408 -0.466221904 ## 1979-12-01 0.07411218 -0.357714688 ts_scale(x_df) ## id time value ## 1 fdeaths 1974-01-01 1.893608829 ## 2 fdeaths 1974-02-01 0.713996326 ## 3 fdeaths 1974-03-01 1.481857295 ## 4 fdeaths 1974-04-01 0.647225807 ## 5 fdeaths 1974-05-01 -0.215226731 ## 6 fdeaths 1974-06-01 -0.860675082 ## 7 fdeaths 1974-07-01 -0.665927734 ## 8 fdeaths 1974-08-01 -0.933009811 ## 9 fdeaths 1974-09-01 -0.966395070 ## 10 fdeaths 1974-10-01 0.118625864 ## 11 fdeaths 1974-11-01 0.096369025 ## 12 fdeaths 1974-12-01 0.586019497 ## 13 fdeaths 1975-01-01 1.498549924 ## 14 fdeaths 1975-02-01 1.064541551 ## 15 fdeaths 1975-03-01 1.248160478 ## 16 fdeaths 1975-04-01 0.574891078 ## 17 fdeaths 1975-05-01 -0.521258276 ## 18 fdeaths 1975-06-01 -0.682620364 ## 19 fdeaths 1975-07-01 -0.777211933 ## 20 fdeaths 1975-08-01 -0.827289822 ## 21 fdeaths 1975-09-01 -1.211220307 ## 22 fdeaths 1975-10-01 -0.671491944 ## 23 fdeaths 1975-11-01 -0.165148842 ## 24 fdeaths 1975-12-01 1.170261539 ## 25 fdeaths 1976-01-01 1.148004699 ## 26 fdeaths 1976-02-01 3.229019209 ## 27 fdeaths 1976-03-01 1.865787779 ## 28 fdeaths 1976-04-01 -0.159584632 ## 29 fdeaths 1976-05-01 -0.632542475 ## 30 fdeaths 1976-06-01 -0.782776143 ## 31 fdeaths 1976-07-01 -1.027601379 ## 32 fdeaths 1976-08-01 -1.283555035 ## 33 fdeaths 1976-09-01 -1.133321368 ## 34 fdeaths 1976-10-01 -0.643670895 ## 35 fdeaths 1976-11-01 -0.081685693 ## 36 fdeaths 1976-12-01 1.131312070 ## 37 fdeaths 1977-01-01 1.676604642 ## 38 fdeaths 1977-02-01 0.552634238 ## 39 fdeaths 1977-03-01 0.569326868 ## 40 fdeaths 1977-04-01 0.458042669 ## 41 fdeaths 1977-05-01 -0.326510929 ## 42 fdeaths 1977-06-01 -0.938574020 ## 43 fdeaths 1977-07-01 -0.832854032 ## 44 fdeaths 1977-08-01 -1.183399257 ## 45 fdeaths 1977-09-01 -0.966395070 ## 46 fdeaths 1977-10-01 -0.977523490 ## 47 fdeaths 1977-11-01 -0.832854032 ## 48 fdeaths 1977-12-01 0.430221620 ## 49 fdeaths 1978-01-01 1.309366787 ## 50 fdeaths 1978-02-01 1.626526752 ## 51 fdeaths 1978-03-01 0.981078402 ## 52 fdeaths 1978-04-01 -0.081685693 ## 53 fdeaths 1978-05-01 -0.170713052 ## 54 fdeaths 1978-06-01 -0.638106685 ## 55 fdeaths 1978-07-01 -0.721569834 ## 56 fdeaths 1978-08-01 -1.105500318 ## 57 fdeaths 1978-09-01 -0.966395070 ## 58 fdeaths 1978-10-01 -0.727134044 ## 59 fdeaths 1978-11-01 -0.754955093 ## 60 fdeaths 1978-12-01 0.658354226 ## 61 fdeaths 1979-01-01 1.448472035 ## 62 fdeaths 1979-02-01 1.248160478 ## 63 fdeaths 1979-03-01 0.925436303 ## 64 fdeaths 1979-04-01 0.285552162 ## 65 fdeaths 1979-05-01 -0.460051967 ## 66 fdeaths 1979-06-01 -0.732698253 ## 67 fdeaths 1979-07-01 -0.866239292 ## 68 fdeaths 1979-08-01 -1.010908749 ## 69 fdeaths 1979-09-01 -0.933009811 ## 70 fdeaths 1979-10-01 -0.832854032 ## 71 fdeaths 1979-11-01 -0.409974078 ## 72 fdeaths 1979-12-01 0.074112185 ## 73 mdeaths 1974-01-01 1.473056003 ## 74 mdeaths 1974-02-01 0.847408011 ## 75 mdeaths 1974-03-01 0.879729310 ## 76 mdeaths 1974-04-01 0.879729310 ## 77 mdeaths 1974-05-01 -0.009106398 ## 78 mdeaths 1974-06-01 -0.570111792 ## 79 mdeaths 1974-07-01 -0.498543203 ## 80 mdeaths 1974-08-01 -0.842534165 ## 81 mdeaths 1974-09-01 -0.662458359 ## 82 mdeaths 1974-10-01 -0.009106398 ## 83 mdeaths 1974-11-01 0.288711281 ## 84 mdeaths 1974-12-01 0.808160720 ## 85 mdeaths 1975-01-01 1.401487414 ## 86 mdeaths 1975-02-01 1.479981995 ## 87 mdeaths 1975-03-01 1.516920622 ## 88 mdeaths 1975-04-01 0.778148086 ## 89 mdeaths 1975-05-01 -0.214577509 ## 90 mdeaths 1975-06-01 -0.480073889 ## 91 mdeaths 1975-07-01 -0.715557635 ## 92 mdeaths 1975-08-01 -0.837916837 ## 93 mdeaths 1975-09-01 -1.022609971 ## 94 mdeaths 1975-10-01 -0.343862703 ## 95 mdeaths 1975-11-01 0.113252804 ## 96 mdeaths 1975-12-01 1.316066839 ## 97 mdeaths 1976-01-01 1.209868287 ## 98 mdeaths 1976-02-01 2.895193135 ## 99 mdeaths 1976-03-01 1.817046965 ## 100 mdeaths 1976-04-01 -0.039119032 ## 101 mdeaths 1976-05-01 -0.708631643 ## 102 mdeaths 1976-06-01 -0.775582904 ## 103 mdeaths 1976-07-01 -0.884090120 ## 104 mdeaths 1976-08-01 -1.214229097 ## 105 mdeaths 1976-09-01 -1.147277836 ## 106 mdeaths 1976-10-01 -0.664767023 ## 107 mdeaths 1976-11-01 -0.066823002 ## 108 mdeaths 1976-12-01 1.299906190 ## 109 mdeaths 1977-01-01 1.717774406 ## 110 mdeaths 1977-02-01 0.318723915 ## 111 mdeaths 1977-03-01 0.521886363 ## 112 mdeaths 1977-04-01 0.704270833 ## 113 mdeaths 1977-05-01 -0.577037785 ## 114 mdeaths 1977-06-01 -0.770965575 ## 115 mdeaths 1977-07-01 -0.944115389 ## 116 mdeaths 1977-08-01 -1.114956538 ## 117 mdeaths 1977-09-01 -1.239624403 ## 118 mdeaths 1977-10-01 -0.731718284 ## 119 mdeaths 1977-11-01 -0.616285076 ## 120 mdeaths 1977-12-01 0.367205863 ## 121 mdeaths 1978-01-01 1.207559623 ## 122 mdeaths 1978-02-01 1.819355629 ## 123 mdeaths 1978-03-01 1.029792481 ## 124 mdeaths 1978-04-01 -0.168404226 ## 125 mdeaths 1978-05-01 -0.360023352 ## 126 mdeaths 1978-06-01 -0.713248971 ## 127 mdeaths 1978-07-01 -0.918720083 ## 128 mdeaths 1978-08-01 -1.135734515 ## 129 mdeaths 1978-09-01 -1.214229097 ## 130 mdeaths 1978-10-01 -0.821756187 ## 131 mdeaths 1978-11-01 -0.891016113 ## 132 mdeaths 1978-12-01 0.729666139 ## 133 mdeaths 1979-01-01 1.770873682 ## 134 mdeaths 1979-02-01 0.748135452 ## 135 mdeaths 1979-03-01 0.808160720 ## 136 mdeaths 1979-04-01 0.080931505 ## 137 mdeaths 1979-05-01 -0.648606374 ## 138 mdeaths 1979-06-01 -0.971819359 ## 139 mdeaths 1979-07-01 -1.015683978 ## 140 mdeaths 1979-08-01 -1.202685776 ## 141 mdeaths 1979-09-01 -1.283489022 ## 142 mdeaths 1979-10-01 -0.957967374 ## 143 mdeaths 1979-11-01 -0.466221904 ## 144 mdeaths 1979-12-01 -0.357714688 ts_scale(x_dt) ## id time value ## 1 fdeaths 1974-01-01 1.893608829 ## 2 fdeaths 1974-02-01 0.713996326 ## 3 fdeaths 1974-03-01 1.481857295 ## 4 fdeaths 1974-04-01 0.647225807 ## 5 fdeaths 1974-05-01 -0.215226731 ## 6 fdeaths 1974-06-01 -0.860675082 ## 7 fdeaths 1974-07-01 -0.665927734 ## 8 fdeaths 1974-08-01 -0.933009811 ## 9 fdeaths 1974-09-01 -0.966395070 ## 10 fdeaths 1974-10-01 0.118625864 ## 11 fdeaths 1974-11-01 0.096369025 ## 12 fdeaths 1974-12-01 0.586019497 ## 13 fdeaths 1975-01-01 1.498549924 ## 14 fdeaths 1975-02-01 1.064541551 ## 15 fdeaths 1975-03-01 1.248160478 ## 16 fdeaths 1975-04-01 0.574891078 ## 17 fdeaths 1975-05-01 -0.521258276 ## 18 fdeaths 1975-06-01 -0.682620364 ## 19 fdeaths 1975-07-01 -0.777211933 ## 20 fdeaths 1975-08-01 -0.827289822 ## 21 fdeaths 1975-09-01 -1.211220307 ## 22 fdeaths 1975-10-01 -0.671491944 ## 23 fdeaths 1975-11-01 -0.165148842 ## 24 fdeaths 1975-12-01 1.170261539 ## 25 fdeaths 1976-01-01 1.148004699 ## 26 fdeaths 1976-02-01 3.229019209 ## 27 fdeaths 1976-03-01 1.865787779 ## 28 fdeaths 1976-04-01 -0.159584632 ## 29 fdeaths 1976-05-01 -0.632542475 ## 30 fdeaths 1976-06-01 -0.782776143 ## 31 fdeaths 1976-07-01 -1.027601379 ## 32 fdeaths 1976-08-01 -1.283555035 ## 33 fdeaths 1976-09-01 -1.133321368 ## 34 fdeaths 1976-10-01 -0.643670895 ## 35 fdeaths 1976-11-01 -0.081685693 ## 36 fdeaths 1976-12-01 1.131312070 ## 37 fdeaths 1977-01-01 1.676604642 ## 38 fdeaths 1977-02-01 0.552634238 ## 39 fdeaths 1977-03-01 0.569326868 ## 40 fdeaths 1977-04-01 0.458042669 ## 41 fdeaths 1977-05-01 -0.326510929 ## 42 fdeaths 1977-06-01 -0.938574020 ## 43 fdeaths 1977-07-01 -0.832854032 ## 44 fdeaths 1977-08-01 -1.183399257 ## 45 fdeaths 1977-09-01 -0.966395070 ## 46 fdeaths 1977-10-01 -0.977523490 ## 47 fdeaths 1977-11-01 -0.832854032 ## 48 fdeaths 1977-12-01 0.430221620 ## 49 fdeaths 1978-01-01 1.309366787 ## 50 fdeaths 1978-02-01 1.626526752 ## 51 fdeaths 1978-03-01 0.981078402 ## 52 fdeaths 1978-04-01 -0.081685693 ## 53 fdeaths 1978-05-01 -0.170713052 ## 54 fdeaths 1978-06-01 -0.638106685 ## 55 fdeaths 1978-07-01 -0.721569834 ## 56 fdeaths 1978-08-01 -1.105500318 ## 57 fdeaths 1978-09-01 -0.966395070 ## 58 fdeaths 1978-10-01 -0.727134044 ## 59 fdeaths 1978-11-01 -0.754955093 ## 60 fdeaths 1978-12-01 0.658354226 ## 61 fdeaths 1979-01-01 1.448472035 ## 62 fdeaths 1979-02-01 1.248160478 ## 63 fdeaths 1979-03-01 0.925436303 ## 64 fdeaths 1979-04-01 0.285552162 ## 65 fdeaths 1979-05-01 -0.460051967 ## 66 fdeaths 1979-06-01 -0.732698253 ## 67 fdeaths 1979-07-01 -0.866239292 ## 68 fdeaths 1979-08-01 -1.010908749 ## 69 fdeaths 1979-09-01 -0.933009811 ## 70 fdeaths 1979-10-01 -0.832854032 ## 71 fdeaths 1979-11-01 -0.409974078 ## 72 fdeaths 1979-12-01 0.074112185 ## 73 mdeaths 1974-01-01 1.473056003 ## 74 mdeaths 1974-02-01 0.847408011 ## 75 mdeaths 1974-03-01 0.879729310 ## 76 mdeaths 1974-04-01 0.879729310 ## 77 mdeaths 1974-05-01 -0.009106398 ## 78 mdeaths 1974-06-01 -0.570111792 ## 79 mdeaths 1974-07-01 -0.498543203 ## 80 mdeaths 1974-08-01 -0.842534165 ## 81 mdeaths 1974-09-01 -0.662458359 ## 82 mdeaths 1974-10-01 -0.009106398 ## 83 mdeaths 1974-11-01 0.288711281 ## 84 mdeaths 1974-12-01 0.808160720 ## 85 mdeaths 1975-01-01 1.401487414 ## 86 mdeaths 1975-02-01 1.479981995 ## 87 mdeaths 1975-03-01 1.516920622 ## 88 mdeaths 1975-04-01 0.778148086 ## 89 mdeaths 1975-05-01 -0.214577509 ## 90 mdeaths 1975-06-01 -0.480073889 ## 91 mdeaths 1975-07-01 -0.715557635 ## 92 mdeaths 1975-08-01 -0.837916837 ## 93 mdeaths 1975-09-01 -1.022609971 ## 94 mdeaths 1975-10-01 -0.343862703 ## 95 mdeaths 1975-11-01 0.113252804 ## 96 mdeaths 1975-12-01 1.316066839 ## 97 mdeaths 1976-01-01 1.209868287 ## 98 mdeaths 1976-02-01 2.895193135 ## 99 mdeaths 1976-03-01 1.817046965 ## 100 mdeaths 1976-04-01 -0.039119032 ## 101 mdeaths 1976-05-01 -0.708631643 ## 102 mdeaths 1976-06-01 -0.775582904 ## 103 mdeaths 1976-07-01 -0.884090120 ## 104 mdeaths 1976-08-01 -1.214229097 ## 105 mdeaths 1976-09-01 -1.147277836 ## 106 mdeaths 1976-10-01 -0.664767023 ## 107 mdeaths 1976-11-01 -0.066823002 ## 108 mdeaths 1976-12-01 1.299906190 ## 109 mdeaths 1977-01-01 1.717774406 ## 110 mdeaths 1977-02-01 0.318723915 ## 111 mdeaths 1977-03-01 0.521886363 ## 112 mdeaths 1977-04-01 0.704270833 ## 113 mdeaths 1977-05-01 -0.577037785 ## 114 mdeaths 1977-06-01 -0.770965575 ## 115 mdeaths 1977-07-01 -0.944115389 ## 116 mdeaths 1977-08-01 -1.114956538 ## 117 mdeaths 1977-09-01 -1.239624403 ## 118 mdeaths 1977-10-01 -0.731718284 ## 119 mdeaths 1977-11-01 -0.616285076 ## 120 mdeaths 1977-12-01 0.367205863 ## 121 mdeaths 1978-01-01 1.207559623 ## 122 mdeaths 1978-02-01 1.819355629 ## 123 mdeaths 1978-03-01 1.029792481 ## 124 mdeaths 1978-04-01 -0.168404226 ## 125 mdeaths 1978-05-01 -0.360023352 ## 126 mdeaths 1978-06-01 -0.713248971 ## 127 mdeaths 1978-07-01 -0.918720083 ## 128 mdeaths 1978-08-01 -1.135734515 ## 129 mdeaths 1978-09-01 -1.214229097 ## 130 mdeaths 1978-10-01 -0.821756187 ## 131 mdeaths 1978-11-01 -0.891016113 ## 132 mdeaths 1978-12-01 0.729666139 ## 133 mdeaths 1979-01-01 1.770873682 ## 134 mdeaths 1979-02-01 0.748135452 ## 135 mdeaths 1979-03-01 0.808160720 ## 136 mdeaths 1979-04-01 0.080931505 ## 137 mdeaths 1979-05-01 -0.648606374 ## 138 mdeaths 1979-06-01 -0.971819359 ## 139 mdeaths 1979-07-01 -1.015683978 ## 140 mdeaths 1979-08-01 -1.202685776 ## 141 mdeaths 1979-09-01 -1.283489022 ## 142 mdeaths 1979-10-01 -0.957967374 ## 143 mdeaths 1979-11-01 -0.466221904 ## 144 mdeaths 1979-12-01 -0.357714688 ts_scale(x_tbl) ## # A tibble: 144 x 3 ## id time value ## &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; ## 1 fdeaths 1974-01-01 1.89 ## 2 fdeaths 1974-02-01 0.714 ## 3 fdeaths 1974-03-01 1.48 ## 4 fdeaths 1974-04-01 0.647 ## 5 fdeaths 1974-05-01 -0.215 ## 6 fdeaths 1974-06-01 -0.861 ## 7 fdeaths 1974-07-01 -0.666 ## 8 fdeaths 1974-08-01 -0.933 ## 9 fdeaths 1974-09-01 -0.966 ## 10 fdeaths 1974-10-01 0.119 ## # … with 134 more rows There is a bunch of other transformation functions: ts_trend, which estimates a trend; functions to calculate differences, ts_pc, ts_pcy, ts_diff, ts_diffy; a function to shift series, ts_lag; functions to construct indices, both from levels and percentage change rates: ts_index and ts_compound. For a full list of functions, check out the reference. ts_plot(AirPassengers) ts_plot(ts_trend(AirPassengers)) ts_plot(ts_pc(AirPassengers)) ts_plot(ts_pcy(AirPassengers)) ts_plot(ts_index(AirPassengers)) 10.8.3 Combine multiple time series A set of helper functions makes it easy to combine multiple time series, even if their classes are different. The basic workhorse is ts_c, which simply collect time series. Again, this works with single or multiple series of any ts-boxable class: ts_c(ts_tbl(EuStockMarkets), AirPassengers, ts_xts(mdeaths)) ## # A tibble: 7,656 x 3 ## id time value ## &lt;chr&gt; &lt;dttm&gt; &lt;dbl&gt; ## 1 DAX 1991-07-01 03:18:27 1629. ## 2 DAX 1991-07-02 13:01:32 1614. ## 3 DAX 1991-07-03 22:44:38 1607. ## 4 DAX 1991-07-05 08:27:43 1621. ## 5 DAX 1991-07-06 18:10:48 1618. ## 6 DAX 1991-07-08 03:53:53 1611. ## 7 DAX 1991-07-09 13:36:59 1631. ## 8 DAX 1991-07-10 23:20:04 1640. ## 9 DAX 1991-07-12 09:03:09 1635. ## 10 DAX 1991-07-13 18:46:15 1646. ## # … with 7,646 more rows If you want to choose a different name for single series, name the arguments: ts_c(ts_tbl(EuStockMarkets), `Airline Passengers` = AirPassengers) ## # A tibble: 7,584 x 3 ## id time value ## &lt;chr&gt; &lt;dttm&gt; &lt;dbl&gt; ## 1 DAX 1991-07-01 03:18:27 1629. ## 2 DAX 1991-07-02 13:01:32 1614. ## 3 DAX 1991-07-03 22:44:38 1607. ## 4 DAX 1991-07-05 08:27:43 1621. ## 5 DAX 1991-07-06 18:10:48 1618. ## 6 DAX 1991-07-08 03:53:53 1611. ## 7 DAX 1991-07-09 13:36:59 1631. ## 8 DAX 1991-07-10 23:20:04 1640. ## 9 DAX 1991-07-12 09:03:09 1635. ## 10 DAX 1991-07-13 18:46:15 1646. ## # … with 7,574 more rows Multiple series can be also combined to a single series: ts_bind(ts_tbl(mdeaths), AirPassengers) ## # A tibble: 216 x 2 ## time value ## &lt;date&gt; &lt;dbl&gt; ## 1 1949-01-01 112 ## 2 1949-02-01 118 ## 3 1949-03-01 132 ## 4 1949-04-01 129 ## 5 1949-05-01 121 ## 6 1949-06-01 135 ## 7 1949-07-01 148 ## 8 1949-08-01 148 ## 9 1949-09-01 136 ## 10 1949-10-01 119 ## # … with 206 more rows ts_chain offers an alternative way to combine time series, by chain- linking them. The following prolongs a short time series with percentage change rates of a longer one: mdeaths_short &lt;- ts_span(mdeaths, end = &quot;1976-12-01&quot;) mdeaths_extrapol &lt;- ts_chain(mdeaths_short, fdeaths) 10.8.4 Frequency conversion and alignment There are functions to convert the frequency of time series and to regularize irregular time series. The following changes the frequency of two series to annual: ts_frequency(ts_c(AirPassengers, austres), &quot;year&quot;, sum) %&gt;% head() ## Time Series: ## Start = 1949 ## End = 1954 ## Frequency = 1 ## AirPassengers austres ## 1949 1520 NA ## 1950 1676 NA ## 1951 2042 NA ## 1952 2364 NA ## 1953 2700 NA ## 1954 2867 NA We already met ts_span, which can be used to limit the time span of a series. ts_regular makes irregular time series regular, by turning implicit missing values into explicit NAs. ts_span(AirPassengers, -1) ## Dec ## 1960 432 ts_span(AirPassengers, start = &quot;-3 months&quot;) ## Oct Nov Dec ## 1960 461 390 432 ts_span(AirPassengers, start = &quot;-200 days&quot;) ## Jul Aug Sep Oct Nov Dec ## 1960 622 606 508 461 390 432 10.8.5 Shortcuts for pivot_wider and pivot_longer x_ts &lt;- ts_c(mdeaths, fdeaths) x_tbl &lt;- ts_tbl(x_ts) x_wide &lt;- ts_wide(x_tbl) ts_long(x_wide) ## # A tibble: 144 x 3 ## id time value ## &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; ## 1 mdeaths 1974-01-01 2134 ## 2 mdeaths 1974-02-01 1863 ## 3 mdeaths 1974-03-01 1877 ## 4 mdeaths 1974-04-01 1877 ## 5 mdeaths 1974-05-01 1492 ## 6 mdeaths 1974-06-01 1249 ## 7 mdeaths 1974-07-01 1280 ## 8 mdeaths 1974-08-01 1131 ## 9 mdeaths 1974-09-01 1209 ## 10 mdeaths 1974-10-01 1492 ## # … with 134 more rows 10.8.6 And plot just about everything Of course, this works for plotting, too. The basic function is ts_plot, which can be used with any ts-boxable time series, single or multiple, of any frequency: ts_plot(AirPassengers, ts_df(lynx), ts_xts(fdeaths)) If you want to use different names than the object names, just name the arguments (and optionally set a title): ts_plot( `Airline Passengers` = AirPassengers, `Lynx trappings` = ts_df(lynx), `Deaths from Lung Diseases` = ts_xts(fdeaths), title = &quot;Airlines, trappings, and deaths&quot;, subtitle = &quot;Monthly passengers, annual trappings, monthly deaths&quot; ) There is also a version that uses ggplot2 that uses the same syntax. With theme_tsbox() and scale_color_tsbox(), the output of ts_ggplot is very similar to ts_plot. ts_ggplot( ts_scale( ts_c( mdeaths, austres, AirPassengers, DAX = EuStockMarkets[ ,&#39;DAX&#39;] ) ) ) + theme_tsbox() + scale_color_tsbox() 10.8.7 Using tsbox in a dplyr / pipe workflow tsbox works well with tibbles and with %&gt;%, so it can be nicely integrated into a tidyverse workflow: library(tidyverse) library(nycflights13) dta &lt;- weather %&gt;% select(origin, time = time_hour, temp, humid, precip) %&gt;% ts_long() ## [id] (columns left of [time] column): &#39;origin&#39; dta %&gt;% filter(id == &quot;temp&quot;) %&gt;% ts_plot() 10.9 Exercises Combine fdeaths and mdeaths to a multiple time series. Call it tdeaths Convert them first to a long tibble, then to a wide tibble. Plot fdeaths and mdeaths, using ts_plot(). Make sure to use some nice labels for the series in the legend. Use ts_trend() to estimate a trend line of the two series. Include it in the same graph. "],
["data-transformation-4.html", "11 Data Transformation 4", " 11 Data Transformation 4 Intro Slides "],
["shiny.html", "12 Shiny 12.1 Introduction 12.2 A First Shiny App 12.3 Excercises 12.4 Where to Go From Here 12.5 Getting Help", " 12 Shiny Download as R script Intro Slides 12.1 Introduction Shiny is an R package that makes it easy to build interactive web apps straight from R. Shiny Showcase https://gallery.shinyapps.io/simple_regression/ https://gallery.shinyapps.io/premierLeague/ http://www.dataseries.org 12.2 A First Shiny App Because Shiny is written by the same company as RStudio, it is closely integrated into the IDE. To start a fresh Shiny app, simply click on the new document icon and select ‘Shiny Web App’. You will be greeted by a dialog that allows you to specify a name and a location. The ‘Application name’ is the name of the directory that will be created within the specified location. You can also choose if you want to use multiple files or a single file. Choose ‘Multiple Files’. RStudio will add a small template application to the specified folder. Click ‘run App’ to see what it does: It plots a histogram of faithful$eruptions, and allows the user to control the number of bins in the histogram. 12.2.1 User Interface ui.R Let’s see how it works. The Shiny app consists of two R files, ui.R and server.R. ui.R describes the user interface. Basically, it contains R functions that write HTML. You can verify by running the code of ui.R in the console: It writes the HTML that describes the user interface. Let’s focus on the only input element, sliderInput. It describes the slider you saw in the app. The first argument, inputId, is the most important one. Through this ID, the shiny server recognizes the elements in the user interface, so it is important that the ID matches the corresponding ID in server.R. We will discuss that in a second. The other arguments of sliderInput are straightforward: A label, two values specifying the range of the slider, and a default value which is shown on startup. Try modifying these values to see what happens in the app. There is a single output element, plotOutput, which draws the histogram plot. Again, the first argument is an ID, outputId, which must match the corresponding value in server.R. There are a bunch of additional arguments to plotOutput, but we will not discuss them here. See ?plotOutput. 12.2.2 Server Logic server.R The second file, server.R, tells the server how inputs should be translated into R calculations, and how the result of R calculations should be translated into outputs. Check out the input$bins element, which is part of the input list, which is one of the two required objects in the server function, shinyServer. input$bins contains the value that the user has entered in the user interface, using the same ID, bins, which we have specified in ui.R. The input is used to perform some R calculations. Here, we are using it to calculate the ‘breaks’ of the bins in the histogram. As this is normal R code, you can also run it in the R console, e.g., x &lt;- faithful[, 2] bins &lt;- seq(min(x), max(x), length.out = 3 + 1) Having calculated the bin breaks, we are using these values to produce a histogram, using the breaks argument of the hist() function. In pure R: hist(x, breaks = bins, col = &#39;darkgray&#39;, border = &#39;white&#39;) The Shiny function renderPlot captures the plot and prepares it fo display in the user interface. Note that the result of renderPlot is assigned to the output$distPlot element of the output list, which is the second required argument of the shinyServer function. As for the input, the name of the element needs to match the name of the ID in ui.R. A central concept in shiny Apps is the concept of reactivity. If input$bins is changed by the user, all the calculations that depend on it are recalculated. To recap, the main steps of building the server logic are: Save objects to display to output$ Build objects to display with render*() Access input values with input$ 12.2.3 Debugging As you will see in the next exercise, debugging is one of the main challenges when we write shiny applications. There are two simple ways to figure out what is going on when things do not behave as we want. The most useful is simply to add print statements to server.R. This way, the value of variable will be printed to the R console after each evaluation. Another useful command is the browser() statement, which works as in standard R. It stops the execution of the program at the statement, so can investigate the values of the variables. As a preliminary exercise, add a print statement to server.R of the demo app that prints the value of bins to the console. Run the app and see how it reacts on changes in the input. 12.3 Excercises Modify the shiny example app to include the following scatterplot: library(tidyverse) boston &lt;- as_tibble(MASS::Boston) boston %&gt;% ggplot(aes(x = lstat, y = medv)) + geom_point() Add a selector to ui.R, where you can select the column names. Store the selected column name under id cname. names(boston) ## [1] &quot;crim&quot; &quot;zn&quot; &quot;indus&quot; &quot;chas&quot; &quot;nox&quot; &quot;rm&quot; &quot;age&quot; ## [8] &quot;dis&quot; &quot;rad&quot; &quot;tax&quot; &quot;ptratio&quot; &quot;black&quot; &quot;lstat&quot; &quot;medv&quot; Use input$cname to select the column name from the ui. myvar &lt;- &quot;medv&quot; boston %&gt;% ggplot(aes(x = lstat, y = !! sym(myvar))) + geom_point() 12.4 Where to Go From Here 12.4.1 More Input Elements There are a couple of additional input elements in Shiny, see the corresponding help pages to see how they work. actionButton checkboxGroupInput dateInput dateRangeInput fileInput numericInput passwordInput radioButtons selectInput sliderInput submitButton textAreaInput textInput 12.4.2 More Output Elements Each render function has a corresponding Output function. server.R ui.R renderDataTable dataTableOutput renderImage imageOutput renderPlot plotOutput renderPrint verbatimTextOutput renderTable tableOutput renderText textOutput renderText textOutput renderUI uiOuptut, htmlOutput 12.4.3 More on Reactivity The reactive() function allow you to define reactive expressions. These are useful if you need the result of your calculations for several outputs. The observe() function, together with ‘reactiveValues’ serves a similar purpose, but it is recommended to use reactive() unless you have good reason not to. isolate() allows you to read an input value without triggering a reaction. which is often useful with actionButton. If you have time consuming calculations, you don’t want to run them each time a user is changing inputs. So you can add an actionButton to ui.R, and put all the other inputs in an isolate(). That way, only clicking the button will trigger the calculation. 12.5 Getting Help Shiny Cheat Sheet Shiny Video Tutorials "]
]
