[
["index.html", "7784 Skills: Programming in R 1 Introduction 1.1 Using left_join to join data sets", " 7784 Skills: Programming in R Kirill Müller, Christoph Sax University of St. Gallen, HS 2019 1 Introduction Intro (Sept. 19): Slides Canvas Page GitHub Organization 1.1 Using left_join to join data sets For those who are interested in using left_join() for combining data sets, see the corresponding Section in R for Data Science. "],
["visualization-and-reporting.html", "2 Visualization and Reporting", " 2 Visualization and Reporting Visualization and Reporting (Sept. 26): Slides "],
["data-tranformation-i.html", "3 Data Tranformation I", " 3 Data Tranformation I Data Tranformation I (Oct. 3): Slides "],
["data-tranformation-ii.html", "4 Data Tranformation II", " 4 Data Tranformation II Data Tranformation II (Oct. 10): Slides "],
["basics-of-r-base.html", "5 Basics of R base 5.1 The Main Data Structures 5.2 Introduction to Functions 5.3 Higher-Order Functions", " 5 Basics of R base Download as R script Intro Slides We have spent the first lessons of this course with dplyr and ggplot2, two of the great innovations in R universe of the last few years. If you were learning R 10 years ago, R would have looked quite differently. In this lesson, we cover the basics of R base - R (mostly) as it is before you load any packages. 5.1 The Main Data Structures R has four main data structures. In dplyr, we always work on data frames, or tibbles, the equivalent to an Excel sheet - a tabular collection of data. When performing operations, e.g., by using filter(), or mutate() we are performing operations on the columns of a data frame. These columns are vectors, and they are the heart of R. Related to them, matrices are basically two dimensional vectors that are useful in some mathematical or statistical applications. Finally, lists are the most versatile data type, and they are used in many circumstances. We will cover these data structures in the following order: vectors, matrices, lists and data frames, or tibbles (which we already know well). 5.1.1 Vectors, the R Workhorse The vector type is the most basic data structure in R. It’s hard to imagine R code that doesn’t involve vectors. If you use mutate to change columns of your data frame, you are effectively operating on vectors. The elements of a vector must all have the same class, or data type. You can have a vector consisting of three character strings (of class character) or three integer elements (of class integer), but not a vector with one integer element and two character string elements. 5.1.1.1 Vector Classes In many programming languages, vector variables are considered different from scalars, which are single-number variables. However, in R, numbers are actually considered one-element vectors, and there is really no such thing as a scalar. As we have seen previously, all elements of an R vector (or data frame column) must have the same class, which can be integer, numeric (also called double), character (string), logical (boolean). Here are some examples: x_log &lt;- c(TRUE, FALSE) # same as c(T, F) x_int &lt;- c(1L, 2L, 3L) # use 1L to enforce integer, rather than numeric x_num &lt;- c(1, 2, 6.3) # also called &#39;double&#39; x_chr &lt;- c(&quot;Hello World&quot;) If you need to check the class of a variable x, you can use, e.g.: class(x_log) ## [1] &quot;logical&quot; There is a certain order in the list above: logical is the least flexible class, while character is the most flexible. If you combine vectors of different classes, the more flexible class will win: class(c(x_log, x_num)) ## [1] &quot;numeric&quot; class(c(x_int, x_chr)) ## [1] &quot;character&quot; You can change the class of a vector with the following coercion functions. as.logical(c(1, 0)) ## [1] TRUE FALSE as.integer(c(1, 0)) ## [1] 1 0 as.numeric(c(&quot;1&quot;, &quot;2&quot;)) ## [1] 1 2 as.character(c(TRUE, FALSE)) ## [1] &quot;TRUE&quot; &quot;FALSE&quot; These functions will always work if you coerce towards greater flexibility. If you want to go the other way, it may give you NAs and some warnings: as.numeric(c(&quot;hi&quot;, &quot;number&quot;, &quot;1&quot;)) ## Warning: NAs introduced by coercion ## [1] NA NA 1 5.1.1.2 Recycling When applying an operation to two vectors that requires them to be the same length, R automatically recycles, or repeats, the shorter one, until it is long enough to match the longer one. Here is an example: c(1, 2) + c(6, 0, 9, 20, 22, 11) ## [1] 7 2 10 22 23 13 The shorter vector was recycled, so the operation was taken to be as follows: c(1, 2, 1, 2, 1, 2) + c(6, 0, 9, 20, 22, 11) ## [1] 7 2 10 22 23 13 The most common recycling operation involves a vector of length 1: c(6, 0, 9, 20, 22, 11) + 3 ## [1] 9 3 12 23 25 14 In dplyr, only a vector of length 1 is allowed to recycle, the other cases will result in an error. # tibble(a = c(1, 2), b = c(6, 0, 9, 20, 22, 11)) 5.1.1.3 Arithmetic Operators In R, every operator, including + in the following example, is actually a function. 2 + 3 ## [1] 5 The + here is a function with two arguments. A more functional way of writing it is the following: &quot;+&quot;(2, 3) ## [1] 5 Remember that scalars are actually one-element vectors. So, we can add vectors, and the + operation will be applied element-wise. x &lt;- c(1, 2, 4) x + c(5, 0, -1) ## [1] 6 2 3 The same is true, e.g., for multiplication, which is done element by element as well. (We will have a look at matrix multiplication in the next section.) x * c(5, 0, -1) ## [1] 5 0 -4 5.1.1.4 Comparison Operators Similar to arithmetic operators, comparison operators are applied element wise. The following expression will return a single TRUE, as we are comparing two vectors of length 1: 2 &gt; 1 ## [1] TRUE The comparison operator for ‘is equal to’ is ==, not =: 1 + 1 == 2 ## [1] TRUE Here is how they work on longer vectors: x &lt;- c(1, 2, 4, 2) y &lt;- c(2, 2, 4, 5) x == y ## [1] FALSE TRUE TRUE FALSE The usual recycling rules apply as well: x == 2 ## [1] FALSE TRUE FALSE TRUE Here are the other comparison operators: x &lt; y: less than x &lt;= y: less or equal than x &gt;= y: greater or equal than x != y: not equal Logical vectors can be combined by &amp; (AND) or | (OR): a &lt;- x &gt;= 2 b &lt;- x &lt; 4 a &amp; b ## [1] FALSE TRUE FALSE TRUE x &gt;= 4 | x &lt; 2 ## [1] TRUE FALSE TRUE FALSE 5.1.1.5 Indexing One of the most frequently used operations in R base is that of indexing vectors, in which we form a subvector by picking elements. You can use both integer values or logical vectors for indexing. 5.1.1.5.1 Indexing Using Integers We can extract values from a vector, using an integer index: y &lt;- c(1.2, 3.9, 0.4, 0.12) y[c(1, 3)] ## [1] 1.2 0.4 y[2:3] ## [1] 3.9 0.4 v &lt;- 3:4 y[v] ## [1] 0.40 0.12 Note that duplicates are allowed: y[c(1, 1, 3)] ## [1] 1.2 1.2 0.4 Negative subscripts mean that we want to exclude the elements: z &lt;- c(5, 12, 13) z[-1] ## [1] 12 13 z[-1:-2] ## [1] 13 5.1.1.5.2 Logical Indexing Logical indexing is perhaps even more important. Building on the example from above, we could also select element 1 and 3 in the following way: y &lt;- c(1.2, 3.9, 0.4, 0.12) y[c(FALSE, TRUE, FALSE, TRUE)] ## [1] 3.90 0.12 Logical indexing picks the TRUEs but not the FALSEs. This is the main building block for filtering. Suppose you have: y[y &gt; 1] ## [1] 1.2 3.9 This will return all elements that are bigger than 1. How is this done? First, R had evaluated the comparison, y &gt; 1, which led to logical vector: y &gt; 1 ## [1] TRUE TRUE FALSE FALSE Second, using logical indexing, this vector was then used to pick those elements that evaluated to TRUE. So y[y &gt; 1] is actually the same as: y[c(TRUE, TRUE, FALSE, TRUE)] ## [1] 1.20 3.90 0.12 We have already seen that you can assign to individual elements of a vector, using integer indices: y[c(2, 4)] &lt;- 5 Of course, you can do the same with logical indices: y[c(FALSE, TRUE, FALSE, TRUE)] &lt;- 5 This is a very powerful tool. For example, if you want to truncate all negative numbers in a vector to 0, you can use: z &lt;- c(-3, 1.2, 2, -22) z[z &lt; 0] &lt;- 0 Or use it with the %in% operator you encountered before: z[z %in% y] ## [1] 1.2 5.1.1.6 Exercises Create a vector called v1 containing the numbers 2, 5, 8, 12 and 16. Extract the values at positions 2 and 5 from v1. Use x:y notation to make a second vector called v2 containing the numbers 5 to 9. Subtract v2 from v1 and look at the result. Generate a vector with 1000 standard-normally distributed random numbers (use rnorm()). Store the result as v3. Extract the numbers that are bigger than 2. 5.1.2 Matrices An R matrix corresponds to the mathematical concept of the same name: a rectangular array of numbers (most of the time), or some other type. Here is some sample matrix code: m &lt;- matrix(c(1, 4, 2, 2), nrow = 2, ncol = 2) m ## [,1] [,2] ## [1,] 1 2 ## [2,] 4 2 The main use of matrices is for matrix algebra. You can do all kind of matrix algebra operations, right out of R base: m %*% m # matrix multiplication ## [,1] [,2] ## [1,] 9 6 ## [2,] 12 12 m * m # elementwise multiplication ## [,1] [,2] ## [1,] 1 4 ## [2,] 16 4 m %*% solve(m) # inverse of a matrix ## [,1] [,2] ## [1,] 1 0 ## [2,] 0 1 m / m # elementwise division ## [,1] [,2] ## [1,] 1 1 ## [2,] 1 1 m + m ## [,1] [,2] ## [1,] 2 4 ## [2,] 8 4 m - m ## [,1] [,2] ## [1,] 0 0 ## [2,] 0 0 t(m) # matrix transpose ## [,1] [,2] ## [1,] 1 4 ## [2,] 2 2 qr(m) # QR decomposition ## $qr ## [,1] [,2] ## [1,] -4.1231056 -2.425356 ## [2,] 0.9701425 -1.455214 ## ## $rank ## [1] 2 ## ## $qraux ## [1] 1.242536 1.455214 ## ## $pivot ## [1] 1 2 ## ## attr(,&quot;class&quot;) ## [1] &quot;qr&quot; det(m) # determinant ## [1] -6 eigen(m) # eigenvalues/eigenvectors ## eigen() decomposition ## $values ## [1] 4.372281 -1.372281 ## ## $vectors ## [,1] [,2] ## [1,] -0.5101065 -0.6445673 ## [2,] -0.8601113 0.7645475 diag(m) # diagonal ## [1] 1 2 Matrices are indexed by double subscripting: m[1, 2] ## [1] 2 m[2, 2] ## [1] 2 You can extract submatrices from a matrix, much as you extract subvectors from vectors: m[1, ] # row 1 ## [1] 1 2 m[1, , drop = FALSE] # keeps being a matrix ## [,1] [,2] ## [1,] 1 2 m[, 2] # column 2 ## [1] 2 2 5.1.2.1 Exercises Create a 10 x 10 matrix that contains a sequence of numbers (use the : notation). Use the transpose function on the matrix Extract the 2. column of the matrix Extract the 5. row of the matrix Extract the 5. and the 6. row of the matrix Compare the classes of the results in 3. and 4. to each other Modify 3., so that it returns the same class as 4. 5.1.3 Lists Like an R vector, an R list is a container for values, but its contents can be items of different data types, or different length. Here’s an example: x &lt;- list(u = c(2, 3, 4), v = &quot;abc&quot;) x ## $u ## [1] 2 3 4 ## ## $v ## [1] &quot;abc&quot; x$u ## [1] 2 3 4 The expression x$u refers to the u component in the list x. x[[&#39;u&#39;]] ## [1] 2 3 4 x[[1]] ## [1] 2 3 4 We can also refer to list components by their numerical indices. However, note that in this case, we use double brackets instead of single ones. We can also use single brackets rather than double brackets to get a subset of the list. x[&#39;u&#39;] ## $u ## [1] 2 3 4 x[1] ## $u ## [1] 2 3 4 x[1:2] ## $u ## [1] 2 3 4 ## ## $v ## [1] &quot;abc&quot; Note that x[[1]] returns the component (a numeric vector), while x[1] returns a subset of the list (a list of length 1): class(x[1]) ## [1] &quot;list&quot; class(x[[1]]) ## [1] &quot;numeric&quot; Hadley Wickham’s visualization helps a lot here: https://twitter.com/hadleywickham/status/643381054758363136 Lists are not restricted to containing vectors. In fact, they can contain anything, for example, a data frames: ll &lt;- list(mtcars = mtcars, u = c(2, 3, 4)) 5.1.3.1 Exercises Generate two random vectors of length 10, a, and b. Combine them in a list, call it l1. Compare the classes of l1[2] and l1[[2]]. Can you explain the difference? 5.1.4 Data Frames As we saw in many places, a typical data set contains data of different classes. Instead of a matrix, we use a data frame, or tibble. Technically, a data frame in R is a list, with each component of the list being a vector corresponding to a column in our data. You can create a data frame using tibble from tidyverse: library(tidyverse) d &lt;- tibble(kids = c(&quot;Jack&quot;, &quot;Jill&quot;), ages = c(12, 10)) d ## # A tibble: 2 x 2 ## kids ages ## &lt;chr&gt; &lt;dbl&gt; ## 1 Jack 12 ## 2 Jill 10 Here, I am using the tibble() function from tidyverse, rather than the R base equivalent, data.frame(). The reason for this is that data.frame() has some undesirable features, such as converting character vectors into factors. Because data frames are technically a list, we can access its vectors the same way as we access components of a list. d$ages ## [1] 12 10 d[[&#39;ages&#39;]] ## [1] 12 10 d[[2]] # usually not recommended ## [1] 12 10 Typically, though, data frames are created by reading in a data set from a file or a database, as we saw in the previous days of the workshop. Contrary to lists, data frames require their columns to be of the same length. If they are not, values will be recycled. That is why the following works: tibble(kids = c(&quot;Jack&quot;, &quot;Jill&quot;), ages = c(12, 10), type = &quot;kid&quot;) ## # A tibble: 2 x 3 ## kids ages type ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Jack 12 kid ## 2 Jill 10 kid 5.2 Introduction to Functions As in most programming languages, the heart of R programming consists of writing functions. A function is a group of instructions that takes inputs, uses them to compute other values, and returns a result. Let’s write a function that divides all elements of a vector by 2: half &lt;- function(x) { x / 2 } This is a function named half, whose purpose is to divides every element of a vector by 2. It’s a pretty pointless function, as the operation itself is so simple. Arguments to a function are enclosed by parentheses (()); the body of the function is enclosed by braces ({}). Let’s see how it works: half(c(3, 2, 1)) ## [1] 1.5 1.0 0.5 half(AirPassengers) ## Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec ## 1949 56.0 59.0 66.0 64.5 60.5 67.5 74.0 74.0 68.0 59.5 52.0 59.0 ## 1950 57.5 63.0 70.5 67.5 62.5 74.5 85.0 85.0 79.0 66.5 57.0 70.0 ## 1951 72.5 75.0 89.0 81.5 86.0 89.0 99.5 99.5 92.0 81.0 73.0 83.0 ## 1952 85.5 90.0 96.5 90.5 91.5 109.0 115.0 121.0 104.5 95.5 86.0 97.0 ## 1953 98.0 98.0 118.0 117.5 114.5 121.5 132.0 136.0 118.5 105.5 90.0 100.5 ## 1954 102.0 94.0 117.5 113.5 117.0 132.0 151.0 146.5 129.5 114.5 101.5 114.5 ## 1955 121.0 116.5 133.5 134.5 135.0 157.5 182.0 173.5 156.0 137.0 118.5 139.0 ## 1956 142.0 138.5 158.5 156.5 159.0 187.0 206.5 202.5 177.5 153.0 135.5 153.0 ## 1957 157.5 150.5 178.0 174.0 177.5 211.0 232.5 233.5 202.0 173.5 152.5 168.0 ## 1958 170.0 159.0 181.0 174.0 181.5 217.5 245.5 252.5 202.0 179.5 155.0 168.5 ## 1959 180.0 171.0 203.0 198.0 210.0 236.0 274.0 279.5 231.5 203.5 181.0 202.5 ## 1960 208.5 195.5 209.5 230.5 236.0 267.5 311.0 303.0 254.0 230.5 195.0 216.0 here we are saving the output in a variable inside the function: half &lt;- function(x) { z &lt;- x / 2 z } An R functions will return the last value computed if there is no explicit return() call. We could have been more explicit, but it is usually not necessary: half &lt;- function(x) { z &lt;- x / 2 return(z) } Let’s make it a bit more complex, by adding an additional argument: fraction &lt;- function(x, denominator){ x / denominator } fraction(c(2, 3, 4), 4) ## [1] 0.50 0.75 1.00 if you have more than one argument, it is a good practice to name the arguments fraction(x = c(2, 3, 4), denominator = 4) ## [1] 0.50 0.75 1.00 that way, they become independent of the order, which is a very useful if the number of argument becomes large. fraction(denominator = 4, x = c(2, 3, 4)) ## [1] 0.50 0.75 1.00 5.2.1 Variable Scope A variable that is visible only within a function body is said to be local to that function. In square(), x is a local variable. In fraction(), x and p are local variables. They disappear after the function returns: fraction(x = c(2, 3, 4), denominator = 4) ## [1] 0.50 0.75 1.00 # denominator # Error: object &#39;denominator&#39; not found x ## $u ## [1] 2 3 4 ## ## $v ## [1] &quot;abc&quot; Note that the x is not the x we used in the fraction function, but rather the x defined earlier. x here is called a global variable, while the x in the function is a local variable. Global variables are shown in RStudio in the environment pane. A global variable can be written to from within a function by using R’s superassignment operator, &lt;&lt;-, but this is rarely recommended. 5.2.2 Default Arguments A nice feature of R functions is that you can set defaults to arguments. Let’s modify the fraction function from above: fraction &lt;- function(x, denominator = 2) { x / denominator } Here denominator will be initialized to 2 if the user does not specify p in the call. So we can use fraction() the same way as half(): fraction(c(2, 2)) ## [1] 1 1 half(c(2, 2)) ## [1] 1 1 or use its extended capabilities: fraction(c(2, 2), 3) ## [1] 0.6666667 0.6666667 5.2.3 Exercises Write a function add_constant that adds a constant to a vector, and set the default value of the constant to 10. Apply it to the AirPassengers series. You can use your function within dplyr. Using the mpg dataset, use add_constant within mutate to add a constant value (100) to the number of cyl. (evil) We saw that &quot;+&quot; is actually a function. In R, it is easily possible to change the working of such a fundamental function (this is, in fact, what ggplot does). In order to do so, let’s write a new function with the same name, &quot;+&quot;, and two arguments, a, and b. But instead of summing the values, let’s subtract them (or figure out something more evil). Verify the result of 1 + 1. Cool, isn’t it? (rm(&quot;+&quot;) will restore sanity.) 5.3 Higher-Order Functions Because functions in R are objects like any other objects, it is easy to write functions that return functions or take functions as arguments. We saw some examples above. The most prominent of these functions are higher-order functions, which are very central to R. They are called apply or lapply, or similar, and there are many of them. To avoid too much confusion, we will restrict ourself to map, the tidyverse equivalent of lapply, 5.3.1 map The map function ‘maps’ a function to each component of a list. Because lists are such a useful container for objects in R, very often, you want to map a function to each component of a list. Here is an example: ll &lt;- list(a = c(2, 3, 4), b = c(1, 2, 3), c = c(5, 2, 1)) This is a list with three vectors, of which we want to calculate the means. Here’s an expression that does what we want (we will cover loops later on): z &lt;- NULL for (vi in ll){ z &lt;- c(z, mean(vi)) } We loop through each component of the list, calculate the mean and add it to an output vector, z. With the map function, this can be written much more concise: library(tidyverse) map(ll, mean) ## $a ## [1] 3 ## ## $b ## [1] 2 ## ## $c ## [1] 2.666667 So map returns a list of the same length as the first argument (a list), each element of which is the result of applying the second argument (a function) to the corresponding component of the list argument. If the input list is named (as in the example), so will be the output. Additional arguments to the mean() function can be included as well: map(ll, mean, na.rm = TRUE) ## $a ## [1] 3 ## ## $b ## [1] 2 ## ## $c ## [1] 2.666667 If you want to convert the list to a vector (as in the loop example), use unlist on the result: unlist(map(ll, mean)) ## a b c ## 3.000000 2.000000 2.666667 Of course, you can use map with your own functions. Here we want to pick the second element of each vector in the list: pick_second &lt;- function(x){ x[2] } map(ll, pick_second) ## $a ## [1] 3 ## ## $b ## [1] 2 ## ## $c ## [1] 2 In cases like this, we may want to use the possibility of having anonymous functions, i.e. functions without a name. So we just substitute pick_second by its definition: map(ll, function(x) x[2]) ## $a ## [1] 3 ## ## $b ## [1] 2 ## ## $c ## [1] 2 For simple one line functions like this, it may be justified to omit the curly braces. ### Exercises Use map to calculate the mean of each variable in the swiss dataset. Convert the resulting list to a vector. Use map to coerce the variables in the swiss dataset to character. Using map, generate a list containing 10 random vectors of random length between 1 and 10. Use the help to see what the colSums() function does. Using apply, try writing your own version, colSums2(). 5.3.2 Loops In many programming languages, one of first things you learn are loops. There is a reason that we didn’t cover them until now, and also only for completeness. Loops in R are slow, and they are – most of the time – unnecessary. This is because many operations in R are vectorized anyway, so there is no need to loop over each element. Also the group_by() operation in dplyr offers a much more elegant way of applying a function repeatedly to a group of data. Third, there are higher order functions like map (or the tidyverse equivalent: purrr::map) that save you from loops most of the time. Anyway, here is the loop: for (i in 1:10) { print(i) } ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 ## [1] 6 ## [1] 7 ## [1] 8 ## [1] 9 ## [1] 10 There will be one iteration of the loop for each component of the vector 1:10, with i taking on the values of those components – in the first iteration, i = (1:10)[1]; in the second iteration, i = (1:10)[2]; and so on. And we are not restricted to integer vectors, but can loop over any vector, even over lists: z &lt;- NULL for (i in list(swiss, mtcars)) { z &lt;- c(z, colnames(i)) } z ## [1] &quot;Fertility&quot; &quot;Agriculture&quot; &quot;Examination&quot; &quot;Education&quot; ## [5] &quot;Catholic&quot; &quot;Infant.Mortality&quot; &quot;mpg&quot; &quot;cyl&quot; ## [9] &quot;disp&quot; &quot;hp&quot; &quot;drat&quot; &quot;wt&quot; ## [13] &quot;qsec&quot; &quot;vs&quot; &quot;am&quot; &quot;gear&quot; ## [17] &quot;carb&quot; But we saw a much clearer way of doing this above: unlist(map(list(swiss, mtcars), colnames)) ## [1] &quot;Fertility&quot; &quot;Agriculture&quot; &quot;Examination&quot; &quot;Education&quot; ## [5] &quot;Catholic&quot; &quot;Infant.Mortality&quot; &quot;mpg&quot; &quot;cyl&quot; ## [9] &quot;disp&quot; &quot;hp&quot; &quot;drat&quot; &quot;wt&quot; ## [13] &quot;qsec&quot; &quot;vs&quot; &quot;am&quot; &quot;gear&quot; ## [17] &quot;carb&quot; "],
["math-and-statistics.html", "6 Math and Statistics 6.1 Math 6.2 Linear Regression 6.3 Numerical Optimization (optional) 6.4 Extended Exercises", " 6 Math and Statistics Download as R script Intro Slides R calls itself a software environment for statistical computing. Math and statistics are at the heart of R. This Lesson provides an overview of the many build-in functions for math and statistics. We will also discuss some basic statistical modeling techniques. 6.1 Math 6.1.1 Basic Math Functions R includes an extensive set of built-in math functions. Here is a partial list: log(): Natural logarithm exp(): Exponential function, base e sqrt(): Square root abs(): Absolute value sin(), cos(), and so on: Trigonometric functions min() and max(): Minimum value and maximum value within a vector which.min() and which.max(): Index of the minimal element and maximal element of a vector pmin() and pmax(): Element-wise minima and maxima of several vectors sum() and prod(): Sum and product of the elements of a vector cumsum() and cumprod(): Cumulative sum and product of the elements of a vector round(), floor(), and ceiling(): Round to the closest integer, to the closest integer below, and to the closest integer above Most of these functions are self explaining. Some remarks: The functions cumsum() and cumprod() return cumulative sums and products. x &lt;- c(12, 5, 13) cumsum(x) ## [1] 12 17 30 cumprod(x) ## [1] 12 60 780 In x, the sum of the first element is 12, the sum of the first two elements is 17, and the sum of the first three elements is 30. The function cumprod() works the same way as cumsum(), but with the product instead of the sum. We briefly mentioned the difference between min() and pmin() in the last chapter. The former simply combines all its arguments into one long vector and returns the minimum value in that vector. In contrast, if pmin() is applied to two or more vectors, it returns a vector of the pair-wise minima, hence the name pmin. Here’s an example: x &lt;- c(1, 5, 6, 2, 3, 1) y &lt;- c(3, 5, 2, 3, 2, 2) min(x) ## [1] 1 pmin(x, y) ## [1] 1 5 2 2 2 1 In the first case, min() computed the smallest value in c(1, 5, 6, 2, 3, 1). But the call to pmin() computed the smaller of 1 and 3, yielding 1; then the smaller of 5 and 5, which is 5; then the minimum of 6 and 2, giving 2, and so on. 6.1.2 Linear Algebra Operations on Vectors and Matrices (optional) Multiplying a vector by a scalar works directly, as you saw earlier. Here’s another example: y &lt;- c(1, 3, 4, 10) 2 * y ## [1] 2 6 8 20 For matrix multiplication in the mathematical sense, the operator to use is %*%, not * . For instance, here we compute the matrix product: Here’s the code: a &lt;- matrix(c(1, 3, 2, 4), nrow = 2, ncol = 2) x &lt;- matrix(c(1, 0, -1, 1), nrow = 2, ncol = 2) b &lt;- a %*% x; b ## [,1] [,2] ## [1,] 1 1 ## [2,] 3 1 The function solve() will solve systems of linear equations and find matrix inverses. For example, to solve this equation for x: a %*% x = b Here’s the code: solve(a, b) ## [,1] [,2] ## [1,] 1 -1 ## [2,] 0 1 And for the inverse: solve(a) ## [,1] [,2] ## [1,] -2.0 1.0 ## [2,] 1.5 -0.5 In that second call to solve(), the lack of a second argument signifies that we simply wish to compute the inverse of the matrix. Here are a few other linear algebra functions: t(): Matrix transpose qr(): QR decomposition chol(): Cholesky decomposition det(): Determinant eigen(): Eigenvalues/eigenvectors diag(): Extracts the diagonal of a square matrix (useful for obtaining variances from a covariance matrix and for constructing a diagonal matrix). 6.1.3 Statistical Distributions (optional) R has functions available for most of the standard statistical distributions. Prefix the name as follows: d for the density function p for the cumulative distribution function q for quantile function r for random number generation The rest of the name indicates the distribution. Let’s start with a very simple distribution, the uniform distribution, which describes an experiment where each value on a continuous scale is equally likely. The runif(n) function returns a vector of n uniformly distributed random numbers. We will visualize the outcome, using ggplot: library(tidyverse) tibble(uniform = runif(100000)) %&gt;% ggplot(mapping = aes(x = uniform)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. These functions also have arguments specific to the given distribution. The uniform distribution has a min and max argument that allows you to control the range on the random numbers. The density function of a uniform distribution looks as follows. Values below 0 and above 1 have 0 probability, while all values between 0 and 1 are equally likely: x &lt;- seq(-1, 2, 0.1) # evaluation from -1 to 2 tibble(x, density_at_x = dunif(x)) %&gt;% ggplot(mapping = aes(x = x, y = density_at_x)) + geom_point() The cumulative distribution function describes the probability that a realization occurs below a certain value. As before, the probability of a realization below 0 is 0. The cumulative distribution function then increases linearly up to one; the probability is 1 (it is certain) that a realization is smaller than or equal to 1: tibble(x, cumulative_distribution_at_x = punif(x)) %&gt;% ggplot(mapping = aes(x = x, y = cumulative_distribution_at_x)) + geom_point() The quantile function is the inverse function of the cumulative distribution function. It returns the value at which the probability of a realization is equal to the one specified in the argument. It is useful to calculate critical values. For the uniform distribution, this is quite boring: qunif(0.025) ## [1] 0.025 Performing the same calculations for the normal distribution is left as an exercise. 6.1.4 Set Operations (optional) R includes some handy set operations, including these: union(x, y): Union of the sets x and y intersect(x, y): Intersection of the sets x and y setdiff(x, y): Set difference between x and y, consisting of all elements of x that are not in y setequal(x, y): Test for equality between x and y c %in% y: Membership, testing whether c is an element of the set y Here are some examples of using these functions: x &lt;- c(1, 2, 5) y &lt;- c(5, 1, 8, 9) union(x, y) ## [1] 1 2 5 8 9 intersect(x, y) ## [1] 1 5 setdiff(x, y) ## [1] 2 setdiff(y, x) ## [1] 8 9 2 %in% x ## [1] TRUE 2 %in% y ## [1] FALSE x %in% y ## [1] TRUE FALSE TRUE The set operators are frequently used with character vectors, for example: intersect(c(&quot;Sara&quot;, &quot;Leo&quot;, &quot;Max&quot;), c(&quot;Mia&quot;, &quot;Leo&quot;)) ## [1] &quot;Leo&quot; 6.1.5 Exercises Create a vector v containing 10 uniformly distributed random numbers. Calculate log(v) and store it as logv Calculate exp(logv) and compare it to v. What do you observe? In the vector c(1, 2, 5, -2, -1, NA), what is the minimum? Hint: You may have a look at the help, the relevant function has an argument that we discussed in the previous lesson. Plot the density function and the cumulative distribution function for a normal distribution. Run an experiment with 10000 draws from a normal distribution and plot its histogram. Look up the critical value where the probability of a realization being lower than that value is 0.025. Consider the vectors: c(2, 5, 1) and c(2, 1, 7, 3). Which elements are in the first but not in the second vector? Which elements are in the second but not in the first vector? Which elements are in both vectors? 6.2 Linear Regression Let’s perform a simple linerar regression, using two vector in the anscombe data set: lm(anscombe$y1 ~ anscombe$x1) ## ## Call: ## lm(formula = anscombe$y1 ~ anscombe$x1) ## ## Coefficients: ## (Intercept) anscombe$x1 ## 3.0001 0.5001 If a data argument is provided, the formula is evaluated within the data frame, similar to the working of mutate(). lm(y1 ~ x1, data = anscombe) ## ## Call: ## lm(formula = y1 ~ x1, data = anscombe) ## ## Coefficients: ## (Intercept) x1 ## 3.0001 0.5001 As we have seen, the summary() function gives a more detailed overview of a regression, so we will usually wrap the function around: summary(lm(y1 ~ x1, data = anscombe)) ## ## Call: ## lm(formula = y1 ~ x1, data = anscombe) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.92127 -0.45577 -0.04136 0.70941 1.83882 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.0001 1.1247 2.667 0.02573 * ## x1 0.5001 0.1179 4.241 0.00217 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.237 on 9 degrees of freedom ## Multiple R-squared: 0.6665, Adjusted R-squared: 0.6295 ## F-statistic: 17.99 on 1 and 9 DF, p-value: 0.00217 Let’s try a more complex example, with more than a single independent variable. In the last lesson, we have investigated the swiss data set. Let’s use it to perform linear regression. To start with, we may want to regress the Fertility variable on all other variables. The formula interface offers a convenient shortcut for this task, the ., which stands for ‘all variables in the data set’: summary(lm(Fertility ~ ., data = swiss)) ## ## Call: ## lm(formula = Fertility ~ ., data = swiss) ## ## Residuals: ## Min 1Q Median 3Q Max ## -15.2743 -5.2617 0.5032 4.1198 15.3213 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 66.91518 10.70604 6.250 1.91e-07 *** ## Agriculture -0.17211 0.07030 -2.448 0.01873 * ## Examination -0.25801 0.25388 -1.016 0.31546 ## Education -0.87094 0.18303 -4.758 2.43e-05 *** ## Catholic 0.10412 0.03526 2.953 0.00519 ** ## Infant.Mortality 1.07705 0.38172 2.822 0.00734 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 7.165 on 41 degrees of freedom ## Multiple R-squared: 0.7067, Adjusted R-squared: 0.671 ## F-statistic: 19.76 on 5 and 41 DF, p-value: 5.594e-10 This is a standard Ordinary Least Square (OLS) regression summary, as it exists in many statistical software applications: According to the output, Education and Agriculture has a significant negative relationship on Fertility, while Catholic and Infant.Mortality has a positive impact. The coefficient on Examination is not significantly different from 0. Note also that the R2 is quite high (0.71), meaning the 5 variables are explaining 71% of the variation in Fertility. If we want to be more specific about which variable we want to include, we can use the + operator in the formula interface. summary(lm(Fertility ~ Education + Catholic + Infant.Mortality, data = swiss)) ## ## Call: ## lm(formula = Fertility ~ Education + Catholic + Infant.Mortality, ## data = swiss) ## ## Residuals: ## Min 1Q Median 3Q Max ## -14.4781 -5.4403 -0.5143 4.1568 15.1187 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 48.67707 7.91908 6.147 2.24e-07 *** ## Education -0.75925 0.11680 -6.501 6.83e-08 *** ## Catholic 0.09607 0.02722 3.530 0.00101 ** ## Infant.Mortality 1.29615 0.38699 3.349 0.00169 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 7.505 on 43 degrees of freedom ## Multiple R-squared: 0.6625, Adjusted R-squared: 0.639 ## F-statistic: 28.14 on 3 and 43 DF, p-value: 3.15e-10 The + here combines variables in the formula. If you want to use the + in the usual way – to add elements of two vectors – you can use the I() function. The following will add Eduction and Catholic element-wise and use the sum as a single regressor variable. summary(lm(Fertility ~ I(Education + Catholic), data = swiss)) ## ## Call: ## lm(formula = Fertility ~ I(Education + Catholic), data = swiss) ## ## Residuals: ## Min 1Q Median 3Q Max ## -39.237 -5.044 0.913 7.511 17.974 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 65.20494 2.83427 23.006 &lt;2e-16 *** ## I(Education + Catholic) 0.09473 0.04278 2.214 0.0319 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 11.99 on 45 degrees of freedom ## Multiple R-squared: 0.09825, Adjusted R-squared: 0.07821 ## F-statistic: 4.903 on 1 and 45 DF, p-value: 0.03192 By default, the model contains an intercept. If you want to turn it off, you can add a 0 at the beginning: summary(lm(Fertility ~ 0 + Education + Catholic, data = swiss)) ## ## Call: ## lm(formula = Fertility ~ 0 + Education + Catholic, data = swiss) ## ## Residuals: ## Min 1Q Median 3Q Max ## -100.272 1.952 22.146 46.403 68.845 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## Education 2.0020 0.4536 4.413 6.29e-05 *** ## Catholic 0.6889 0.1131 6.092 2.28e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 40.06 on 45 degrees of freedom ## Multiple R-squared: 0.6972, Adjusted R-squared: 0.6837 ## F-statistic: 51.8 on 2 and 45 DF, p-value: 2.125e-12 There is a bunch of helper functions for linear models: m &lt;- lm(Fertility ~ Education, data = swiss) coef(m) # the OLS coefficients ## (Intercept) Education ## 79.6100585 -0.8623503 confint(m, level = 0.95) # confidence intervals of the coefficients ## 2.5 % 97.5 % ## (Intercept) 75.372189 83.8479277 ## Education -1.154083 -0.5706181 fitted(m) # fitted values of the regression ## Courtelary Delemont Franches-Mnt Moutier Neuveville Porrentruy ## 69.26186 71.84891 75.29831 73.57361 66.67480 73.57361 ## Broye Glane Gruyere Sarine Veveyse Aigle ## 73.57361 72.71126 73.57361 68.39950 74.43596 69.26186 ## Aubonne Avenches Cossonay Echallens Grandson Lausanne ## 73.57361 69.26186 75.29831 77.88536 72.71126 55.46425 ## La Vallee Lavaux Morges Moudon Nyone Orbe ## 62.36305 71.84891 70.98656 77.02301 69.26186 74.43596 ## Oron Payerne Paysd&#39;enhaut Rolle Vevey Yverdon ## 78.74771 72.71126 77.02301 70.98656 63.22540 72.71126 ## Conthey Entremont Herens Martigwy Monthey St Maurice ## 77.88536 74.43596 77.88536 74.43596 77.02301 71.84891 ## Sierre Sion Boudry La Chauxdfnd Le Locle Neuchatel ## 77.02301 68.39950 69.26186 70.12421 68.39950 52.01485 ## Val de Ruz ValdeTravers V. De Geneve Rive Droite Rive Gauche ## 73.57361 73.57361 33.90549 54.60190 54.60190 resid(m) # residuals ## Courtelary Delemont Franches-Mnt Moutier Neuveville Porrentruy ## 10.9381450 11.2510941 17.2016929 12.2263935 10.2251959 2.5263935 ## Broye Glane Gruyere Sarine Veveyse Aigle ## 10.2263935 19.6887438 8.8263935 14.5004953 12.6640432 -5.1618550 ## Aubonne Avenches Cossonay Echallens Grandson Lausanne ## -6.6736065 -0.3618550 -13.5983071 -9.5853579 -1.0112562 0.2357497 ## La Vallee Lavaux Morges Moudon Nyone Orbe ## -8.0630527 -6.7489059 -5.4865556 -12.0230077 -12.6618550 -17.0359568 ## Oron Payerne Paysd&#39;enhaut Rolle Vevey Yverdon ## -6.2477082 1.4887438 -5.0230077 -10.4865556 -4.9254030 -7.3112562 ## Conthey Entremont Herens Martigwy Monthey St Maurice ## -2.3853579 -5.1359568 -0.5853579 -3.9359568 2.3769923 -6.8489059 ## Sierre Sion Boudry La Chauxdfnd Le Locle Neuchatel ## 15.1769923 10.9004953 1.1381450 -4.4242053 4.3004953 12.3851508 ## Val de Ruz ValdeTravers V. De Geneve Rive Droite Rive Gauche ## 4.0263935 -5.9736065 1.0945070 -9.9019000 -11.8019000 6.2.1 Example: Bootstrapping standard errors (optional) We want to use our simple linear model to explore another, very powerful statistical technique: the bootstrap. The bootstrap can be used to assess the variability of almost any statistical estimation, whether we understand it or not. Fortunately, we understand linear models, and the accuracy of the coefficients can be easily assessed by looking at the analytically derived standard errors: m &lt;- lm(Fertility ~ Education, data = swiss) summary(m) ## ## Call: ## lm(formula = Fertility ~ Education, data = swiss) ## ## Residuals: ## Min 1Q Median 3Q Max ## -17.036 -6.711 -1.011 9.526 19.689 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 79.6101 2.1041 37.836 &lt; 2e-16 *** ## Education -0.8624 0.1448 -5.954 3.66e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.446 on 45 degrees of freedom ## Multiple R-squared: 0.4406, Adjusted R-squared: 0.4282 ## F-statistic: 35.45 on 1 and 45 DF, p-value: 3.659e-07 A standard error of 0.1448 tells us that there is a 68.3% (pnorm(1) - pnorm(-1)) probability that the true coefficient is between -0.8624 - 0.1448 and -0.8624 + 0.1448. What if we wouldn’t have these standard errors? How much confidence should we have in our estimate of -0.8624? Let’s figuring out by using the bootstrap. First, we create a simple function, boot.fn(), that takes the swiss data set as an argument, plus and index argument, which is used to pick rows in the data. The function returns the coefficients of the model the linear regression model. To keep it simple, we just focus on the second coefficient: boot.fn &lt;- function(data, index){ unname(coef(lm(Fertility ~ Education, data = data[index, ]))[2]) } To verify that it works we are running with the dataset in the original order. boot.fn(swiss, 1:nrow(swiss)) ## [1] -0.8623503 The idea of the bootstrap is to re-estimate our model with mutated data, were the mutated data is a random sample of the original data. An estimation on a mutated dataset can be generated as such: boot.fn(swiss, sample(1:nrow(swiss), replace = TRUE)) ## [1] -0.763152 The replace = TRUE argument ensures that we are not ending up with the original dataset. If replace is TRUE, some rows will appear multiple times in the mutated dataset. Each time we run this function, we get a different result. The final step of the bootstrap is now to this many times, and analyze the results: res &lt;- numeric(1000) for (i in 1:1000){ res[i] &lt;- boot.fn(swiss, sample(1:nrow(swiss), replace = TRUE)) } Happily, both the mean and the standard deviation is very close to what we got from our the analytic result: mean(res) ## [1] -0.8740296 sd(res) ## [1] 0.1457853 The boot function from the boot package gives some more powerful tools to perform bootstrap analysis. For example, it can run simulations on several cores, which may speed up the simulation. library(boot) system.time(boot(swiss, boot.fn, R = 1000, parallel = &quot;multicore&quot;, ncpus = 4)) ## user system elapsed ## 0.006 0.011 0.275 system.time(boot(swiss, boot.fn, R = 1000)) ## user system elapsed ## 0.571 0.010 0.581 6.2.2 Factors in regression models A factor is simply as an integer vector with a bit of extra information. That extra information consists of a record of the distinct values in that vector, called levels: x &lt;- c(&quot;female&quot;, &quot;male&quot;, &quot;male&quot;, &quot;female&quot;) xf &lt;- factor(x) class(xf) ## [1] &quot;factor&quot; To see whats’s in xf, let’s ‘unclass’ it: unclass(xf) ## [1] 1 2 2 1 ## attr(,&quot;levels&quot;) ## [1] &quot;female&quot; &quot;male&quot; The core of xf is not c(&quot;female&quot;, &quot;male&quot;, &quot;male&quot;, &quot;female&quot;), but an integer vector c(1, 2, 2, 1). The level attribute maps the integer to their original meaning: 1 means &quot;female&quot; and 2 means &quot;male&quot;. From a memory perspective, this is appealing, since you just have to save an integer value, rather than the whole string. However, today, character vectors are cached in a hash table, and are already memory efficient. So, the two ways of storing data are equivalent from a memory perspective. That’s why we want to get rid of factors whenever we can! In a linear regression, however, factors are cool. To see how they work, let’s add a new variable, Conf, to our dataset, which is &quot;protestant&quot; if the share of Catholics is below 50, and &quot;catholic&quot; otherwise. We then transform it to a factor. (We can omit the last step, as R will convert all character variable to factors when used in a regression.) library(tidyverse) swiss_plus &lt;- swiss %&gt;% as_tibble() %&gt;% mutate(Conf = if_else(Catholic &gt;= 50, &quot;catholic&quot;, &quot;protestant&quot;)) %&gt;% mutate(Conf = as.factor(Conf)) We have now a categorical variable in our dataset. The usual way to deal with this variable would be to build a dummy variable, which contains 0 for catholic and 1 for protestant locations. Thanks to factors, R will do this automatically for you: summary(lm(Fertility ~ Education + Conf, data = swiss_plus)) ## ## Call: ## lm(formula = Fertility ~ Education + Conf, data = swiss_plus) ## ## Residuals: ## Min 1Q Median 3Q Max ## -17.739 -5.832 -1.953 6.251 15.466 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 83.7551 2.3994 34.907 &lt; 2e-16 *** ## Education -0.8006 0.1355 -5.909 4.59e-07 *** ## Confprotestant -7.8173 2.6512 -2.949 0.0051 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 8.729 on 44 degrees of freedom ## Multiple R-squared: 0.5329, Adjusted R-squared: 0.5117 ## F-statistic: 25.1 on 2 and 44 DF, p-value: 5.332e-08 6.2.3 Exercises Plot the bivariate relationship between Agriculture and Fertility. Perform a bivariate linear regression. Plot both the data and the regression line in a single graph. Also add Education to the regression. How has the coefficient of Agriculture changed? Do you have an explanation for it? Add a dummy variable DCATH that contains 0 if the share of Catholics is below 50, 1 otherwise. Verify that including a factor really leads to the same result. 6.3 Numerical Optimization (optional) R offers a large collection of tools for numerical optimization. The built-in standard function is optim(), which is sufficient in most situations. The syntax of optim() is different from the functions we met so far, in that the second argument of the function is a function itself! Let’s see how it works: Let’s define a simple quadratic function as our objective function: quadratic_function &lt;- function(x){ x^2 } To see how it looks, let’s evaluate it for a sequence of numbers: x &lt;- seq(from = -5, to = 5, by = 0.1) tibble(x, quadratic_function_at_x = quadratic_function(x)) %&gt;% ggplot(mapping = aes(x = x, y = quadratic_function_at_x)) + geom_point() Suppose you want to find the minimum value of this function. Visual inspection already told us that the minimum is at x = 0, where the function evaluates to 0. If you know calculus, you could have derived this result analytically. Often, however, the problem is too complicated, or there is no analytical solution at all. So you want to use numerical optimization instead. A simple way to perform numerical optimization is to evaluate the function over a relevant range of values, and simply pick the lowest. In our example, picking the minimum of y would have given you the correct result. x[which.min(y)] ## [1] -4.9 This was grid-search minimization, which works fine for small problems like this. A problem with grid search is that is quite inefficient. If you know the function is increasing at x = 1 what is the point of checking for a minimum at x = 1.1? The optim function includes several methods that handle this problem in a more efficient way. Usually, for numerical optimization, you need to give the computer a hint where to start, by providing an initial value for each parameter. 0, would be an obvious starting point, but we don’t want to make it too boring, so let’s try something different. op &lt;- optim(4, quadratic_function, method = &quot;Brent&quot;, lower = -100, upper = 100) Here, we were using the “Brent” method, which is suited for the single parameter optimization that we have here. If you have more than one parameter, the default “Nelder-Mead” usually works fine. As we have seen before, complex functions in R often return a complex object. optim returns a list with 5 components, where the first, par, is the one we are mostly interested in: op$par ## [1] -3.552714e-15 For a more complex example, let’s find the solution to the linear regression problem numerically. This is the result we want to replicate: m &lt;- lm(Fertility ~ Education, data = swiss) coef(m) ## (Intercept) Education ## 79.6100585 -0.8623503 The OLS estimator is minimizing the sum of squared residuals. Let us write down our objective function: sum_of_squared_residuals &lt;- function(b){ b0 &lt;- b[1] b1 &lt;- b[2] fitted.values &lt;- b0 + b1 * swiss$Education residuals &lt;- swiss$Fertility - fitted.values squared_residuals &lt;- residuals^2 sum(squared_residuals) } The argument b is a vector of length 2, containing the coefficient for the intercept and for Education. Using the two coefficients and the data, we calculate fitted.values, the value predicted by our model. The difference between the actual values (swiss$Fertility) are the residuals. In order to calculate the sum of squared residuals, we square the residual vector element- wise and sum up the elements, using the sum() function. For every two coefficients, the function returns the sum of squared residuals. E.g., an intercept of 0 and a slope of 1 lead to a pretty high sum of squared residuals: sum_of_squared_residuals(c(0, 1)) ## [1] 183282.9 Using the optim function, we can easily find the coefficients that minimize the function (the initial values are chosen arbitrarily): optim(c(0, 0), sum_of_squared_residuals) ## $par ## [1] 79.6077096 -0.8618092 ## ## $value ## [1] 4015.238 ## ## $counts ## function gradient ## 97 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL Indeed: At a value of 4015.238, The OLS estimator c(79.6077096, -0.8618092) is minimizing the sum of squared residuals! 6.4 Extended Exercises It can be shown analytically that the OLS estimator can be calculated as follows, using matrix algebra: b = (X’X)^(-1)X’y See, e.g., here (the formula is on page 4): http://www.stat.purdue.edu/~jennings/stat514/stat512notes/topic3.pdf where X is the data matrix, possibly including an intercept, and y is a column vector containing the left-hand variable. Let’s consider our simple linear model from above: m &lt;- lm(Fertility ~ Education, data = swiss) coef(m) ## (Intercept) Education ## 79.6100585 -0.8623503 Can you replicate the result of coef(m), using pure matrix algebra? Hints: Construct X first, adding a vector of 1s as an intercept. Make sure X is of class “matrix”, not “data.frame”. It should look like this: # Intercept Education # [1,] 1 12 # [2,] 1 9 # [3,] 1 5 # [4,] 1 7 # [5,] 1 15 This is matrix algebra, so you will need to use matrix multiplication, not the usual element-wise multiplication. Also remember the inverse and the transpose function from above. "],
["regression-and-visualization.html", "7 Regression and Visualization 7.1 The broom package 7.2 texreg and friends 7.3 From data frame to output: kable 7.4 More on linear regression 7.5 Exercises", " 7 Regression and Visualization Download as R script Intro Slides We had a first look at linear regression in the last lesson. Today, we are build on it in two ways. First, we discuss some possibilities to turn regression output into beautiful tables in your RMarkdown report. Second, we explore extensions to the linear regression model, to make it a more powerful and statistically accurate tool to capture linear relationships in data. 7.1 The broom package In the first few lessons of this course, through dplyr and ggplot, we explored powerful tools to manipulate data frames. Because we are getting good at it, we want to use this knowledge to work with regression output (or any model output) as well. The broom package tries to help, by turning any model output into a familiar data frame. For more information on the broom package, check out the vignette broom is an attempt to bridge the gap from untidy outputs of predictions and estimations to the tidy data we want to work with. It centers around three methods, each of which take common objects produced by R statistical functions (lm(), t.test(), nls(), etc) and convert them into a data frame. broom is particularly designed to work with the dplyr package. broom provides three functions that do three distinct kinds of tidying. tidy: constructs a data frame that summarizes the model’s statistical findings. This includes coefficients and p-values for each term in a regression, per-cluster information in clustering applications, or per-test information for multtest functions. augment: add columns to the original data that was modeled. This includes predictions, residuals, and cluster assignments. glance: construct a concise one-row summary of the model. This typically contains values such as R^2, adjusted R^2, and residual standard error that are computed once for the entire model. To see it in action, consider our simple linear regression from last lesson: m &lt;- lm(Fertility ~ Education, data = swiss) We saw that the summary() function prints an overview of the regression: summary(m) ## ## Call: ## lm(formula = Fertility ~ Education, data = swiss) ## ## Residuals: ## Min 1Q Median 3Q Max ## -17.036 -6.711 -1.011 9.526 19.689 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 79.6101 2.1041 37.836 &lt; 2e-16 *** ## Education -0.8624 0.1448 -5.954 3.66e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.446 on 45 degrees of freedom ## Multiple R-squared: 0.4406, Adjusted R-squared: 0.4282 ## F-statistic: 35.45 on 1 and 45 DF, p-value: 3.659e-07 But this is tedious to work with. It is much easier to have the regression output in a data frame. To extract the information on the coefficients, use tidy(): library(broom) tidy(m) ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 79.6 2.10 37.8 9.30e-36 ## 2 Education -0.862 0.145 -5.95 3.66e- 7 augment() returns statistical information on the observations, such as resiudals and predicted values: augment(m) ## # A tibble: 47 x 10 ## .rownames Fertility Education .fitted .se.fit .resid .hat .sigma .cooksd ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Courtela… 80.2 12 69.3 1.39 10.9 0.0215 9.41 1.51e-2 ## 2 Delemont 83.1 9 71.8 1.41 11.3 0.0222 9.40 1.65e-2 ## 3 Franches… 92.5 5 75.3 1.63 17.2 0.0297 9.18 5.23e-2 ## 4 Moutier 85.8 7 73.6 1.49 12.2 0.0250 9.37 2.20e-2 ## 5 Neuvevil… 76.9 15 66.7 1.50 10.2 0.0251 9.42 1.55e-2 ## 6 Porrentr… 76.1 7 73.6 1.49 2.53 0.0250 9.54 9.41e-4 ## 7 Broye 83.8 7 73.6 1.49 10.2 0.0250 9.42 1.54e-2 ## 8 Glane 92.4 8 72.7 1.44 19.7 0.0234 9.07 5.32e-2 ## 9 Gruyere 82.4 7 73.6 1.49 8.83 0.0250 9.46 1.15e-2 ## 10 Sarine 82.9 13 68.4 1.41 14.5 0.0222 9.29 2.74e-2 ## # … with 37 more rows, and 1 more variable: .std.resid &lt;dbl&gt; Finally glance() returns summary statistics that are valid for the regression model: glance(m) ## # A tibble: 1 x 11 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.441 0.428 9.45 35.4 3.66e-7 2 -171. 348. 354. ## # … with 2 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt; 7.2 texreg and friends Publications in Economics usually require regression outputs to be in a specific form. The texreg package gives you professional regression output for publications out-of-the box. (There is an alternative solution, stargazer, but its output has some flaws, especially when used in the PDF mode. E.g., it does not support the LaTeX booktab package.) For a quick demonstration, consider two competing models: m1 &lt;- lm(Fertility ~ Education, data = swiss) m2 &lt;- lm(Fertility ~ Education + Agriculture + Examination, data = swiss) m3 &lt;- lm(Fertility ~ Agriculture + Examination, data = swiss) Wrap the model in a list() and pass them to texreg() (if you want to produces a PDF) or to htmlreg() (if you want to produces a HTML document). library(texreg) htmlreg(list(m1, m2, m3), doctype = FALSE, star.symbol = &quot;\\\\*&quot;) Statistical models Model 1 Model 2 Model 3 (Intercept) 79.61*** 99.80*** 94.61*** (2.10) (7.16) (7.83) Education -0.86*** -0.67** (0.14) (0.19) Agriculture -0.18* -0.09 (0.08) (0.09) Examination -0.80** -1.20*** (0.25) (0.24) R2 0.44 0.56 0.43 Adj. R2 0.43 0.53 0.41 Num. obs. 47 47 47 RMSE 9.45 8.60 9.62 ***p &lt; 0.001, **p &lt; 0.01, *p &lt; 0.05 Like with any table output, make sure you add the chunk option results = 'asis'. This ensures that the code produced by your function (HTML or TeX) is interpreted as it is, not not simply printed. doctype = FALSE and star.symbol = &quot;\\\\*&quot; are two fixes needed if HTML content is used within RMarkdown. There is also a version than produces a text version of the output, which is useful for quick interactive exploration of the table: screenreg(list(m1, m2, m3)) ## ## ============================================ ## Model 1 Model 2 Model 3 ## -------------------------------------------- ## (Intercept) 79.61 *** 99.80 *** 94.61 *** ## (2.10) (7.16) (7.83) ## Education -0.86 *** -0.67 ** ## (0.14) (0.19) ## Agriculture -0.18 * -0.09 ## (0.08) (0.09) ## Examination -0.80 ** -1.20 *** ## (0.25) (0.24) ## -------------------------------------------- ## R^2 0.44 0.56 0.43 ## Adj. R^2 0.43 0.53 0.41 ## Num. obs. 47 47 47 ## RMSE 9.45 8.60 9.62 ## ============================================ ## *** p &lt; 0.001, ** p &lt; 0.01, * p &lt; 0.05 You can give the models a customized name: htmlreg( list(`(1)` = m1, `(2)` = m2, `(3)` = m3), doctype = FALSE, star.symbol = &quot;\\\\*&quot; ) Statistical models (1) (2) (3) (Intercept) 79.61*** 99.80*** 94.61*** (2.10) (7.16) (7.83) Education -0.86*** -0.67** (0.14) (0.19) Agriculture -0.18* -0.09 (0.08) (0.09) Examination -0.80** -1.20*** (0.25) (0.24) R2 0.44 0.56 0.43 Adj. R2 0.43 0.53 0.41 Num. obs. 47 47 47 RMSE 9.45 8.60 9.62 ***p &lt; 0.001, **p &lt; 0.01, *p &lt; 0.05 texreg shines when it comes to LaTeX tables, which are often used in publications. The following line will produce the LaTeX table below texreg(list(`(1)` = m1, `(2)` = m2, `(3)` = m3), booktabs = TRUE, dcolumn = TRUE) However, because we are using the LaTeX packages booktabs (for nicer lines) and dcolumn (for aligned numbers), we need to load these first and adjust the YAML header as follows: --- output: pdf_document header-includes: - \\usepackage{booktabs} - \\usepackage{dcolumn} --- 7.3 From data frame to output: kable Including any kind of table in a RMarkdown document can be done with the kable package. The kable package is super simple but sufficient in most cases, so there is rearely a need to switch to more extensive package like gt or xtable. Contrary to texreg, kable is smart enough to see if we want to produce a PDF or a HTML document. To produce a simple table, just write kable() around a data frame: library(knitr) kable(iris[1:5,]) Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.0 3.6 1.4 0.2 setosa In a PDF, the output looks like the following: Customization can be done through the kableExtra package but the details differ for HTML and PDF. We won’t cover them here, but both are useful sources for documentation to style your tables according to your needs. 7.4 More on linear regression We had a quick look at the lm() function and linear regressions in R. We will deepen our knowledge by having another look at these models. This time, we put more emphasis on the econometric side of the problem. The following relies heavily on a new a new online book, Econometrics in R. This chapter covers the content of chapters 4 to 7. Contrary to the textbook, we will rely on the tools of the tidyverse, to unfiy the data analysis. We will not cover most of the statistical topics in the book. The interested reader is refered to the book, or to the underlying Econometrics textbook, Introduction to Econometrics (which is not free). 7.4.1 Relationship between class size and test score The book takes a careful look on the impact of class sizes on school test scores. If, for example, a school cuts its class sizes by hiring new teachers, that is, the school lowers the student-teacher ratios of its classes, how would this affect the performance of the students involved in a standardized test? With linear regression we can not only examine whether the student-teacher ratio does have an impact on the test results but we can also learn about the direction and the strength of this effect. The dataset is included in the AER package and can be loaded as follows: library(AER) data(CASchools) The dataset contains data on test performance, school characteristics and student demographic backgrounds for school districts in California. We will enhance the dataset by defining two new variables, student_teacher_ratio, the student-teacher ratio, and test_score, an average of two underlying test scores: library(tidyverse) caschools &lt;- CASchools %&gt;% as_tibble() %&gt;% mutate(student_teacher_ratio = students / teachers) %&gt;% mutate(test_score = (read + math) / 2) caschools ## # A tibble: 420 x 16 ## district school county grades students teachers calworks lunch computer ## &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 75119 Sunol… Alame… KK-08 195 10.9 0.510 2.04 67 ## 2 61499 Manza… Butte KK-08 240 11.1 15.4 47.9 101 ## 3 61549 Therm… Butte KK-08 1550 82.9 55.0 76.3 169 ## 4 61457 Golde… Butte KK-08 243 14 36.5 77.0 85 ## 5 61523 Paler… Butte KK-08 1335 71.5 33.1 78.4 171 ## 6 62042 Burre… Fresno KK-08 137 6.40 12.3 87.0 25 ## 7 68536 Holt … San J… KK-08 195 10 12.9 94.6 28 ## 8 63834 Vinel… Kern KK-08 888 42.5 18.8 100 66 ## 9 62331 Orang… Fresno KK-08 379 19 32.2 93.1 35 ## 10 67306 Del P… Sacra… KK-06 2247 108 79.0 87.3 0 ## # … with 410 more rows, and 7 more variables: expenditure &lt;dbl&gt;, income &lt;dbl&gt;, ## # english &lt;dbl&gt;, read &lt;dbl&gt;, math &lt;dbl&gt;, student_teacher_ratio &lt;dbl&gt;, ## # test_score &lt;dbl&gt; It is always a good idea to start with a plot. Let us refresh our knowledge of ggplot: caschools %&gt;% ggplot(aes(x = student_teacher_ratio, y = test_score)) + geom_point() + ggtitle(&quot;Test scores are higher in small classes&quot;) The plot shows the scatterplot of all observations on the student-teacher ratio and test score. We see that the points are strongly scattered, and that the variables are negatively correlated. That is, we expect to observe lower test scores in bigger classes. Let’s use lm() to estimate a linear regression model: m &lt;- lm(test_score ~ student_teacher_ratio, data = caschools) summary(m) ## ## Call: ## lm(formula = test_score ~ student_teacher_ratio, data = caschools) ## ## Residuals: ## Min 1Q Median 3Q Max ## -47.727 -14.251 0.483 12.822 48.540 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 698.9329 9.4675 73.825 &lt; 2e-16 *** ## student_teacher_ratio -2.2798 0.4798 -4.751 2.78e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 18.58 on 418 degrees of freedom ## Multiple R-squared: 0.05124, Adjusted R-squared: 0.04897 ## F-statistic: 22.58 on 1 and 418 DF, p-value: 2.783e-06 Thus, the coefficient on the student_teacher_ratio is about -2.3, i.e., a reduced class size by one is associated with a test score increased by 2.3. geom_smooth() allows us to plot a bivariate regression line directly into a ggplot: caschools %&gt;% ggplot(aes(x = student_teacher_ratio, y = test_score)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + ggtitle(&quot;Test scores are higher in small classes&quot;, &quot;with regression line&quot;) 7.4.2 Heteroskedasticity-consistent standard errors In economic contexts, some of the assumptions of the classical regression model are usually violated, and it recommended and sometimes required to use robust standard errors. The textbook discusses the case of heteroskedasticity-consistent standard errors. The quickest way to compute heteroskedasticity-consistent standard errors is to use robust_lm, from the estimatr package. library(estimatr) m_robust &lt;- lm_robust( test_score ~ student_teacher_ratio, se_type = &quot;HC1&quot;, data = caschools ) summary(m_robust) ## ## Call: ## lm_robust(formula = test_score ~ student_teacher_ratio, data = caschools, ## se_type = &quot;HC1&quot;) ## ## Standard error type: HC1 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) CI Lower CI Upper ## (Intercept) 698.93 10.3644 67.436 9.487e-227 678.560 719.306 ## student_teacher_ratio -2.28 0.5195 -4.389 1.447e-05 -3.301 -1.259 ## DF ## (Intercept) 418 ## student_teacher_ratio 418 ## ## Multiple R-squared: 0.05124 , Adjusted R-squared: 0.04897 ## F-statistic: 19.26 on 1 and 418 DF, p-value: 1.447e-05 We can use htmlreg (or screenreg, for interactive use; or texreg) from above to get a clean overview of the two estimations (include.ci = FALSE is needed for a peculiarity of lm_robust): library(texreg) htmlreg( list( `OLS s.e.` = m, `heteroskedasticity-consistent s.e.` = m_robust ), include.ci = FALSE, star.symbol = &quot;\\\\*&quot;, doctype = FALSE, caption = &quot;Robust standard errors are slightly wider.&quot; ) Robust standard errors are slightly wider. OLS s.e. heteroskedasticity-consistent s.e. (Intercept) 698.93*** 698.93*** (9.47) (10.36) student_teacher_ratio -2.28*** -2.28*** (0.48) (0.52) R2 0.05 0.05 Adj. R2 0.05 0.05 Num. obs. 420 420 RMSE 18.58 18.58 ***p &lt; 0.001, **p &lt; 0.01, *p &lt; 0.05 Robust standard errors are often wider, making it less likely that an effect is statistically significant. Whenever possible, use heteroskedasticity-consistent standard errors. 7.4.3 Omitted variable bias The previous analysis of the relationship between test score and class size has a major flaw: we ignored other determinants of the dependent variable (test score) that correlate with the regressor (class size). This might induce an estimation bias. In our example we therefore wrongly estimate the causal effect on test scores of a unit change in the student-teacher ratio, on average. This issue is called omitted variable bias (OVB). Let’s have a look at the following relationship between the percentage of English learners (english) and class size: caschools %&gt;% ggplot(aes(x = student_teacher_ratio, y = english)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + ggtitle( &quot;Larger classes have a larger share of English learners &quot;, &quot;with regression line&quot; ) Since a high share of English learners is likely to have lowering impact on test scores, this suggests that the effect of small classes is overestimated as it captures the effect of having fewer English learners, too. Multiple regression allows us to disentangle the two effects: m_multiple &lt;- lm_robust(test_score ~ student_teacher_ratio + english, se_type = &quot;HC1&quot;, data = caschools) summary(m_robust) ## ## Call: ## lm_robust(formula = test_score ~ student_teacher_ratio, data = caschools, ## se_type = &quot;HC1&quot;) ## ## Standard error type: HC1 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) CI Lower CI Upper ## (Intercept) 698.93 10.3644 67.436 9.487e-227 678.560 719.306 ## student_teacher_ratio -2.28 0.5195 -4.389 1.447e-05 -3.301 -1.259 ## DF ## (Intercept) 418 ## student_teacher_ratio 418 ## ## Multiple R-squared: 0.05124 , Adjusted R-squared: 0.04897 ## F-statistic: 19.26 on 1 and 418 DF, p-value: 1.447e-05 htmlreg( list( m_robust, m_multiple ), include.ci = FALSE, star.symbol = &quot;\\\\*&quot;, doctype = FALSE, caption = &quot;Including the share of English learners decreases the coefficient on class size.&quot; ) Including the share of English learners decreases the coefficient on class size. Model 1 Model 2 (Intercept) 698.93*** 686.03*** (10.36) (8.73) student_teacher_ratio -2.28*** -1.10* (0.52) (0.43) english -0.65*** (0.03) R2 0.05 0.43 Adj. R2 0.05 0.42 Num. obs. 420 420 RMSE 18.58 14.46 ***p &lt; 0.001, **p &lt; 0.01, *p &lt; 0.05 We find that the negative coefficient on student_teacher_ratio is still significant but only half the size. 7.5 Exercises For this exercise, you will work with Boston, which is part of the MASS package. Load the MASS package, check the documentation to get an overview of the variables in Boston. Convert Boston from a data frame to a tibble, called boston. Plot a scatter-plot between the percent of households with low socioeconomic status, lstat, (x-axis) and the median house value of districts medv, (y-axis). Describe your observations. How appropriate is the assumption of linearity? Estimate a simple linear regression model that explains medv by lstat and a constant. Save the model to m_uni. Use heteroskedasticity-consistent standard errors. Regress the median housing value in a district, medv, on the average age of the buildings, age, the per-capita crime rate, crim, the percentage of individuals with low socioeconomic status, lstat, and a constant. Store it as m_multi. Regress medv on all available regressors. Use the formula medv ~ . as a short cut. Store the model as m_full. Use htmlreg() (or screereg(), or texreg() to produce a regression output that is ready for publication. Include m_uni, m_multi and m_full. Check whether the augmented model yields a higher adjusted R2. Why is not meaningful to compare the unadjusted R2? Can we improve the model (in terms of adjusted R2), by dropping a variable? Hint: the glance function makes it easy to directly access the adjusted R2. "],
["transformation-3.html", "8 Transformation 3", " 8 Transformation 3 Intro Slides "],
["random-forests.html", "9 Random Forests 9.1 Slang 9.2 The Boston housing data set, again 9.3 Prediction with OLS 9.4 Decision Trees 9.5 Random Forest 9.6 Classification", " 9 Random Forests Download as R script Intro Slides 9.1 Slang Econometrics ML sample data, to estimate the model training sample estimating a model training a model regression parameters weights regressor, predictor, independent variable, RHS feature estimating relationship between dependent var and regressors supervised learning clustering unsupervised learning discrete response problems classification problems 9.2 The Boston housing data set, again As in a previous exercise, we will work with Boston, which is part of the MASS package. The data set was used in an 1978 article on hedonic price estimation and the measurement for the demand for clean air. For an explanation of variable names, check out the documentation: # `?MASS::Boston` Again, we want to keep the data in a tibble, for convenience: library(tidyverse) boston &lt;- as_tibble(MASS::Boston) %&gt;% mutate(chas = as.logical(chas)) Most predictive modeling is subject to the danger of overfitting. We will discuss the concept later on. This means that a model can perform well in within the sample it was estimated, but terrible, if it is applied to new data. A popular way to control the problem is to estimate (train) the model on a part of the original data only (e.g. 70%), and use the rest to test the model afterwards. To separate the dataset, we first add an ID column: boston_with_id &lt;- boston %&gt;% mutate(id = row_number()) The sample_frac() function allow as to pick a percentage of all rows from the original dataset: boston_train_with_id &lt;- boston_with_id %&gt;% sample_frac(0.7) boston_train &lt;- boston_train_with_id %&gt;% select(-id) This is the datset we will use for estimating our models. anti_join() can be used to retrieve all rows that are not within boston_train, based on the id: boston_test &lt;- boston_with_id %&gt;% anti_join(boston_train_with_id, by = &quot;id&quot;) %&gt;% select(-id) This is the datset we will use for evaluating our models. 9.3 Prediction with OLS We can use the OLS model to perform predictions of the median house value. The OLS curve covers the predicted values of the model. Let’s recapitulate and see, how the median value of owner-occupied homes, medv, can pe predicted by the neighborhood, as measured by the percentage of lower status population, lstat. m_ols &lt;- lm(medv ~ lstat, data = boston_train) boston_test %&gt;% mutate(predict = predict(m_ols, newdata = boston_test)) %&gt;% ggplot(aes(x = lstat, y = medv)) + geom_point() + geom_point(mapping = aes(y = predict), shape = 21) + geom_segment(mapping = aes(xend = lstat, yend = predict)) 9.4 Decision Trees A decision tree is tree-like structure of decisions and their possible consequences. In a decision tree model, the prediction depends on particular value of a variable. For example: library(rpart) m_tree_1 &lt;- rpart(medv ~ lstat, data = boston_train, method = &quot;anova&quot;, maxdepth = 1) m_tree_1 ## n= 354 ## ## node), split, n, deviance, yval ## * denotes terminal node ## ## 1) root 354 31907.440 22.72684 ## 2) lstat&gt;=9.615 214 4907.116 17.42850 * ## 3) lstat&lt; 9.615 140 11809.990 30.82571 * plot(m_tree_1) text(m_tree_1) boston_test %&gt;% mutate(predict = predict(m_tree_1, newdata = boston_test)) %&gt;% ggplot(aes(x = lstat, y = medv)) + geom_point() + geom_point(mapping = aes(y = predict), shape = 21) + geom_segment(mapping = aes(xend = lstat, yend = predict), alpha = 0.2) + geom_vline(aes(xintercept = 9.95)) More interesting trees can be grown if we increase the depth of a tree. Here, we let the tree grow to a depths of 2, which allows for four possible prediction values: m_tree_2 &lt;- rpart(medv ~ lstat, data = boston_train, method = &quot;anova&quot;, maxdepth = 2) m_tree_2 ## n= 354 ## ## node), split, n, deviance, yval ## * denotes terminal node ## ## 1) root 354 31907.4400 22.72684 ## 2) lstat&gt;=9.615 214 4907.1160 17.42850 ## 4) lstat&gt;=15 118 2192.4100 14.83051 * ## 5) lstat&lt; 15 96 939.2841 20.62188 * ## 3) lstat&lt; 9.615 140 11809.9900 30.82571 ## 6) lstat&gt;=4.475 107 5141.7700 27.63738 * ## 7) lstat&lt; 4.475 33 2053.7160 41.16364 * plot(m_tree_2) text(m_tree_2) boston_test %&gt;% mutate(predict = predict(m_tree_2, newdata = boston_test)) %&gt;% ggplot(aes(x = lstat, y = medv)) + geom_point() + geom_point(mapping = aes(y = predict), shape = 21) + geom_segment(mapping = aes(xend = lstat, yend = predict), alpha = 0.2) + geom_vline(aes(xintercept = 4.295)) + geom_vline(aes(xintercept = 4.295)) + geom_vline(aes(xintercept = 15)) Analogous to multiple regression, we also can grow trees that depend on more than variable. Here, our prediction depends both on the average number of rooms per dwelling, rm and the percentage of lower status population lstat. m_tree_all &lt;- rpart(medv ~ ., data = boston_train, method = &quot;anova&quot;, maxdepth = 2) m_tree_all ## n= 354 ## ## node), split, n, deviance, yval ## * denotes terminal node ## ## 1) root 354 31907.4400 22.72684 ## 2) lstat&gt;=9.615 214 4907.1160 17.42850 ## 4) lstat&gt;=15 118 2192.4100 14.83051 * ## 5) lstat&lt; 15 96 939.2841 20.62187 * ## 3) lstat&lt; 9.615 140 11809.9900 30.82571 ## 6) rm&lt; 7.437 117 5158.7180 27.87521 * ## 7) rm&gt;=7.437 23 451.4722 45.83478 * plot(m_tree_all) text(m_tree_all) boston_test %&gt;% mutate(predict = predict(m_tree_all, newdata = boston_test)) %&gt;% ggplot(aes(x = lstat, y = medv, color = rm)) + geom_point() + geom_point(mapping = aes(y = predict), shape = 21) + geom_segment(mapping = aes(xend = lstat, yend = predict), alpha = 0.2) 9.5 Random Forest Random forest takes random subsets of data and train trees on each subset. That is the reason behind the name for this method: random (subsets) forest (trees). When predicting new data from the test set each tree produces its own prediction. Then the predictions from all the trees are combined together (averaged). This is why random forest can approximate linear patterns, even when the single trees, as we saw above, only produce mean values for different ranges of values. The randomForest package offers basic random forest estimation in R: library(randomForest) Because random forests are based on random sampling, setting the seed is needed to make the estimation reproducible: set.seed(0) Let’s start with a simple example: m_forest &lt;- randomForest(medv ~ lstat, data = boston_train) plot(m_forest) boston_test %&gt;% mutate(predict = predict(m_forest, newdata = boston_test)) %&gt;% ggplot(aes(x = lstat, y = medv)) + geom_point() + geom_point(mapping = aes(y = predict), shape = 21) + geom_segment(mapping = aes(xend = lstat, yend = predict), alpha = 0.2) We now have a bunch of possible forecasts. Which one should you use. The Root Mean Squared Error, while not the only possible measure, is usually a good starting point. It’s main characteristics are the following: Squared - so negative values become positive (deviation is deviation) Squared - so large deviations are extra-penalized Mean - influence of all errors are summarized with one number Root - to go back to original scale after squaring. boston_test %&gt;% summarize( rmse_forest = sqrt(mean((medv - predict(m_forest, newdata = boston_test))^2)), rmse_tree_1 = sqrt(mean((medv - predict(m_tree_1, newdata = boston_test))^2)), rmse_ols = sqrt(mean((medv - predict(m_ols, newdata = boston_test))^2)) ) ## # A tibble: 1 x 3 ## rmse_forest rmse_tree_1 rmse_ols ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 5.33 6.94 5.75 9.5.1 Using more than one variable library(randomForest) m_forest_all &lt;- randomForest(medv ~ ., data = boston_train) boston_test %&gt;% mutate(predict = predict(m_forest_all, newdata = boston_test)) %&gt;% ggplot(aes(x = lstat, y = medv)) + geom_point() + geom_point(mapping = aes(y = predict), shape = 21) + geom_segment(mapping = aes(xend = lstat, yend = predict), alpha = 0.2) m_ols_all &lt;- lm(medv ~ ., data = boston_train) summary(m_ols_all) ## ## Call: ## lm(formula = medv ~ ., data = boston_train) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.0156 -2.9634 -0.5869 2.1585 25.2777 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 44.590000 6.526979 6.832 3.89e-11 *** ## crim -0.149666 0.039656 -3.774 0.000189 *** ## zn 0.059000 0.016785 3.515 0.000499 *** ## indus 0.008600 0.073542 0.117 0.906976 ## chasTRUE 4.457064 1.206543 3.694 0.000257 *** ## nox -20.069362 4.775555 -4.203 3.38e-05 *** ## rm 3.435511 0.544755 6.307 8.87e-10 *** ## age -0.000949 0.016257 -0.058 0.953487 ## dis -1.756968 0.245438 -7.159 5.06e-12 *** ## rad 0.337495 0.077762 4.340 1.88e-05 *** ## tax -0.013745 0.004314 -3.186 0.001574 ** ## ptratio -1.034316 0.159799 -6.473 3.37e-10 *** ## black 0.006549 0.003247 2.017 0.044507 * ## lstat -0.544976 0.062931 -8.660 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.855 on 340 degrees of freedom ## Multiple R-squared: 0.7489, Adjusted R-squared: 0.7392 ## F-statistic: 77.98 on 13 and 340 DF, p-value: &lt; 2.2e-16 boston_test %&gt;% mutate(predict = predict(m_ols_all, newdata = boston_test)) %&gt;% ggplot(aes(x = lstat, y = medv)) + geom_point() + geom_point(mapping = aes(y = predict), shape = 21) + geom_segment(mapping = aes(xend = lstat, yend = predict), alpha = 0.2) boston_test %&gt;% summarize( rmse_forest = sqrt(sum((medv - predict(m_forest, newdata = boston_test))^2)), rmse_tree_1 = sqrt(sum((medv - predict(m_tree_1, newdata = boston_test))^2)), rmse_ols = sqrt(sum((medv - predict(m_ols, newdata = boston_test))^2)), rmse_ols_all = sqrt(sum((medv - predict(m_ols_all, newdata = boston_test))^2)), rmse_forest_all = sqrt(sum((medv - predict(m_forest_all, newdata = boston_test))^2)) ) ## # A tibble: 1 x 5 ## rmse_forest rmse_tree_1 rmse_ols rmse_ols_all rmse_forest_all ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 65.8 85.5 70.9 58.0 34.8 If the interactions between features are mostly linear (as modeled by linear regression) then random forest will not beat linear regression no matter how much data it will have. However if there are interactions in the data the linear model will require for you to list them in the formula (y ~ x1 * x2) and random forest would be able to find them on its own. 9.5.2 Variable importance So we have used OLS, decision trees and random forest to explain the values of medv. But which independent variables are the most important? We have many variables, and would like to rank them by importance. How can we do that? For OLS, the importance can be obtained by the p-value. According to this, lstat and rm are the most important variables: broom::tidy(m_ols_all) %&gt;% arrange(p.value) ## # A tibble: 14 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 lstat -0.545 0.0629 -8.66 1.93e-16 ## 2 dis -1.76 0.245 -7.16 5.06e-12 ## 3 (Intercept) 44.6 6.53 6.83 3.89e-11 ## 4 ptratio -1.03 0.160 -6.47 3.37e-10 ## 5 rm 3.44 0.545 6.31 8.87e-10 ## 6 rad 0.337 0.0778 4.34 1.88e- 5 ## 7 nox -20.1 4.78 -4.20 3.38e- 5 ## 8 crim -0.150 0.0397 -3.77 1.89e- 4 ## 9 chasTRUE 4.46 1.21 3.69 2.57e- 4 ## 10 zn 0.0590 0.0168 3.52 4.99e- 4 ## 11 tax -0.0137 0.00431 -3.19 1.57e- 3 ## 12 black 0.00655 0.00325 2.02 4.45e- 2 ## 13 indus 0.00860 0.0735 0.117 9.07e- 1 ## 14 age -0.000949 0.0163 -0.0584 9.53e- 1 For single trees, the values that split the data at the highest level should be more important. Again, lstat and rm are the most important variables: m_tree_all ## n= 354 ## ## node), split, n, deviance, yval ## * denotes terminal node ## ## 1) root 354 31907.4400 22.72684 ## 2) lstat&gt;=9.615 214 4907.1160 17.42850 ## 4) lstat&gt;=15 118 2192.4100 14.83051 * ## 5) lstat&lt; 15 96 939.2841 20.62187 * ## 3) lstat&lt; 9.615 140 11809.9900 30.82571 ## 6) rm&lt; 7.437 117 5158.7180 27.87521 * ## 7) rm&gt;=7.437 23 451.4722 45.83478 * For random forest, there are several different measures of variable importance. We will focus on one of them, the decrease in accuracy. It measures the percentage of accuracy decrease if one variable is omitted. To calculate this measure of importance - we need to add importance = TRUE to our model call: m_forest_all &lt;- randomForest(medv ~ ., data = boston_train, importance = TRUE) varImpPlot() gives a convenient overview of variable importance, type = 1 selects the measure of importance, here, the mean decrease in accuracy after permutation: varImpPlot(m_forest_all, type = 1) Again lstat and rm are the most important variables, but rm seems to be substantially more important than lstat. 9.5.3 Exercises Back to the CASchools dataset on test performance, school characteristics and student demographic backgrounds for school districts in California. As before, we will enhance the dataset by defining two new variables, student_teacher_ratio, the student-teacher ratio, and test_score, an average of two underlying test scores: library(AER) data(CASchools) caschools &lt;- CASchools %&gt;% as_tibble() %&gt;% mutate(student_teacher_ratio = students / teachers) %&gt;% mutate(test_score = (read + math) / 2) %&gt;% select(-read, -math, -students, -teachers, -district, -school, -county, -grades) caschools ## # A tibble: 420 x 8 ## calworks lunch computer expenditure income english student_teacher… ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.510 2.04 67 6385. 22.7 0 17.9 ## 2 15.4 47.9 101 5099. 9.82 4.58 21.5 ## 3 55.0 76.3 169 5502. 8.98 30.0 18.7 ## 4 36.5 77.0 85 7102. 8.98 0 17.4 ## 5 33.1 78.4 171 5236. 9.08 13.9 18.7 ## 6 12.3 87.0 25 5580. 10.4 12.4 21.4 ## 7 12.9 94.6 28 5253. 6.58 68.7 19.5 ## 8 18.8 100 66 4566. 8.17 47.0 20.9 ## 9 32.2 93.1 35 5356. 7.39 30.1 19.9 ## 10 79.0 87.3 0 5036. 11.6 40.3 20.8 ## # … with 410 more rows, and 1 more variable: test_score &lt;dbl&gt; Separate the data set into a training and a test set. Make sure the training set contains 75% of the available observations. Build a decision tree to predict student_teacher_ratio, using all variables in caschools_train. Use the defaults of rpart. Draw the resulting decision tree. Store the model as m_tree. From the documentation, ?rpart, can you figure out how the depth of a tree is determined? Which one is the most important variable? Estimate an OLS model to predict student_teacher_ratio, using all variables in caschools_train. Which one is the most important variable? Store the model as m_ols. grow a random forest to predict student_teacher_ratio, using all variables in caschools_train. Use the defaults of randomForest. Store the model as m_forest. Plot the variable importance for m_forest. Which one is the most imporant? Using the test data, can you compute RMSE measures for m_ols, m_tree, and m_forest? Which performs best? 9.6 Classification This data set provides information on the fate of passengers on the fatal maiden voyage of the ocean liner Titanic, summarized according to economic status (class), sex, age and survival. Titanic dataset is classic example in machine learning, and can be found in the titanic package. # install.packages(&quot;titanic&quot;) # make sure it is installed. head(titanic::titanic_train) ## PassengerId Survived Pclass ## 1 1 0 3 ## 2 2 1 1 ## 3 3 1 3 ## 4 4 1 1 ## 5 5 0 3 ## 6 6 0 3 ## Name Sex Age SibSp Parch ## 1 Braund, Mr. Owen Harris male 22 1 0 ## 2 Cumings, Mrs. John Bradley (Florence Briggs Thayer) female 38 1 0 ## 3 Heikkinen, Miss. Laina female 26 0 0 ## 4 Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35 1 0 ## 5 Allen, Mr. William Henry male 35 0 0 ## 6 Moran, Mr. James male NA 0 0 ## Ticket Fare Cabin Embarked ## 1 A/5 21171 7.2500 S ## 2 PC 17599 71.2833 C85 C ## 3 STON/O2. 3101282 7.9250 S ## 4 113803 53.1000 C123 S ## 5 373450 8.0500 S ## 6 330877 8.4583 Q The goal of the exercise it to predict survival. The datasets come divided into training and testing set. However, the testing part does not have survival information and is not useful that way. In the following, we will limit ourself to the titanic::titanic_train. Categorical variables will be a problem in random forest, so we will convert them into factors. We also clean up the data and remove missing values. The cleaned dataset looks as follows: titanic &lt;- as_tibble(na.omit(titanic::titanic_train)) %&gt;% select(-Name, -PassengerId, -Cabin, -Ticket, -Embarked) %&gt;% mutate(Sex = as_factor(Sex)) %&gt;% mutate(Survived = as_factor(Survived)) Survived Passenger Survival Indicator Pclass Passenger Class Sex Sex Age Age SibSp Number of Siblings/Spouses Aboard Parch Number of Parents/Children Aboard Again, the usual separation in test and training data set. set.seed(0) titanic_with_id &lt;- titanic %&gt;% mutate(id = row_number()) titanic_train_with_id &lt;- titanic_with_id %&gt;% sample_frac(0.70) titanic_train &lt;- titanic_train_with_id %&gt;% select(-id) titanic_test &lt;- titanic_with_id %&gt;% anti_join(titanic_train_with_id, by = &quot;id&quot;) %&gt;% select(-id) To start, let’s build a decision tree: m_titanic_tree &lt;- rpart(Survived ~ ., data = titanic_train) plot(m_titanic_tree) text(m_titanic_tree, all = TRUE) m_titanic_tree ## n= 500 ## ## node), split, n, loss, yval, (yprob) ## * denotes terminal node ## ## 1) root 500 203 0 (0.59400000 0.40600000) ## 2) Sex=male 319 62 0 (0.80564263 0.19435737) ## 4) Pclass&gt;=1.5 248 35 0 (0.85887097 0.14112903) ## 8) Age&gt;=9.5 227 25 0 (0.88986784 0.11013216) * ## 9) Age&lt; 9.5 21 10 0 (0.52380952 0.47619048) ## 18) SibSp&gt;=2.5 11 0 0 (1.00000000 0.00000000) * ## 19) SibSp&lt; 2.5 10 0 1 (0.00000000 1.00000000) * ## 5) Pclass&lt; 1.5 71 27 0 (0.61971831 0.38028169) ## 10) Age&gt;=36.5 45 12 0 (0.73333333 0.26666667) * ## 11) Age&lt; 36.5 26 11 1 (0.42307692 0.57692308) ## 22) Fare&gt;=61.8 11 4 0 (0.63636364 0.36363636) * ## 23) Fare&lt; 61.8 15 4 1 (0.26666667 0.73333333) * ## 3) Sex=female 181 40 1 (0.22099448 0.77900552) ## 6) Pclass&gt;=2.5 65 31 0 (0.52307692 0.47692308) ## 12) Fare&gt;=20.8 15 2 0 (0.86666667 0.13333333) * ## 13) Fare&lt; 20.8 50 21 1 (0.42000000 0.58000000) ## 26) Age&gt;=17.5 36 17 0 (0.52777778 0.47222222) ## 52) Fare&lt; 15 29 12 0 (0.58620690 0.41379310) ## 104) Fare&gt;=8.29375 16 4 0 (0.75000000 0.25000000) * ## 105) Fare&lt; 8.29375 13 5 1 (0.38461538 0.61538462) * ## 53) Fare&gt;=15 7 2 1 (0.28571429 0.71428571) * ## 27) Age&lt; 17.5 14 2 1 (0.14285714 0.85714286) * ## 7) Pclass&lt; 2.5 116 6 1 (0.05172414 0.94827586) * This has a very simple interpretation: If you are on the Titanic, the best guess is that you will die. If you are male, things look bad, unless you are a child. If you are female, things look better, especially if you travel first class. So it is not very hard to guess the end of the movie Titanic. Switching to random forest, the output now includes the a confusion matrix. The confusion matrix shows, based on out-of-bag evaluation, which classification has been done correctly and which has not. 270 deaths has been predicted correctly, 151 survivals have been predicted correctly. On the other hand, 27 dying passengers have been incorrectly predicted to survive, while 52 surviving passengers have been incorrectly predicted to to die. set.seed(0) m_titanic_forest &lt;- randomForest(Survived ~ ., data = titanic_train, importance = TRUE) m_titanic_forest ## ## Call: ## randomForest(formula = Survived ~ ., data = titanic_train, importance = TRUE) ## Type of random forest: classification ## Number of trees: 500 ## No. of variables tried at each split: 2 ## ## OOB estimate of error rate: 15.8% ## Confusion matrix: ## 0 1 class.error ## 0 270 27 0.09090909 ## 1 52 151 0.25615764 The out-of-bag forecast error are usually a good way to quickly assess the forecast accuracy of a random forest model. However, if we want to run the model on our test data, compute the confusion table as follows: tibble( predicted = predict(m_titanic_forest, newdata = titanic_test), actual = titanic_test$Survived ) %&gt;% group_by(predicted, actual) %&gt;% summarize(count = n()) ## # A tibble: 4 x 3 ## # Groups: predicted [2] ## predicted actual count ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; ## 1 0 0 107 ## 2 0 1 26 ## 3 1 0 20 ## 4 1 1 61 The corresponding out-of-bag confusion table as is as follows: tibble( predicted = predict(m_titanic_forest), actual = titanic_train$Survived ) %&gt;% group_by(predicted, actual) %&gt;% summarize(count = n()) ## # A tibble: 4 x 3 ## # Groups: predicted [2] ## predicted actual count ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; ## 1 0 0 270 ## 2 0 1 52 ## 3 1 0 27 ## 4 1 1 151 Finally, which variables are the important one? varImpPlot(m_titanic_forest, type = 1) "]
]
